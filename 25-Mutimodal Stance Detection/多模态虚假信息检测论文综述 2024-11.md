### [MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs](https://arxiv.org/pdf/2406.08772)
包含文本真实性失真、视觉真实性失真和跨模态一致性失真，以及12个子类别的虚假信息伪造类型的混合源 MMD 基准测试。

数据集被分为10000、1000个图文对的测试集和验证集，按照30%文本真实性失真、10%视觉真实性失真、30%跨模态一致性失真和30%真实数据进行划分。

MMFakeBench 数据集组成：
- 文本真实性失真：文本谣言-AI 生成文本谣言对应的图像，文本谣言-高度相关性的真实图像
- 视觉真实性失真：文本真实-PS编辑真实图像，文本真实-AI生成冲突的图像
- 跨模态一致性失真：文本图像都真实-但不匹配，真实的文本和图像-其中一方被编辑
- 真实数据：文本图像均真实

MMD-Agent 框架：在上述数据集上指导 LVLM 生成多角度的推理，并整合模型行动以促进决策。
GPT-4V 模型在该测试上达到了 51.0% 的 F1 分数。


### [VERITE: a Robust benchmark for multimodal misinformation detection accounting for unimodal bias](https://link.springer.com/article/10.1007/s13735-023-00312-6)
解决现有数据集单模态偏差的评价准则。

数据来源：来自 Snopes 和 Reuters 的已核实文章。

VERITE 数据集组成： 
- 真实 (True): 图像和文本真实地描述了相同的场景或事件。
- OOC (Out-of-Context): 真实的文本与不相关的图像配对，或真实的图像与不相关的文本配对。
- MC (MisCaptioned): 真实的图像与具有误导性的文本配对，文本错误地描述了图像的来源、内容或上下文

模态平衡： 
- 每个图像和文本在数据集中出现两次，一次与真实文本配对，一次与错误文本或 OOC 图像配对。


### [M3A: A multimodal misinformation dataset for media authenticity analysis](https://www.sciencedirect.com/science/article/pii/S1077314224002868?ref=pdf_download&fr=RR-2&rr=8e35e7c6df571099)
解决现有虚假新闻数据集模态有限、来源单一、主题范围狭窄等问题的大规模虚假新闻数据集。

数据来源：来自 60 家知名新闻机构的新闻内容，涵盖文本、图像、音频和视频等多种模态，并覆盖了政治、科技、娱乐等广泛的新闻主题。

数据包含约 510 万的文本-图像对（其中 50 万真实样本）和 214 万的文本-图像-视频-音频对（其中 18 万真实样本）。

M3A 数据集的组成：
- 其他模态真实，文本中的命名实体被替换
- 将匹配的文本、图像、音频或视频进行互换，创造模态不匹配的样本
- 文本虚假，由 AI 根据虚假文本生成匹配的其他模态
- 其他模态真实，由 AI 生成虚假文本


### [MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media](https://dl.acm.org/doi/10.1145/3539618.3591896)
MR2-E 英文数据集，来源于推特；MR2-C 中文数据集，来源于微博。
每个数据集约 7000 个样本，分为谣言、非谣言、未验证三种标注。

MR2 数据集组成：
- 文本真实-图像真实且匹配
- 文本真实-图像虚假
- 文本真实-图像经过修改
- 文本真实-图像真实但不匹配
- 文本虚假-图像虚假
- 文本虚假-图像真实


### [Multimodal Misinformation Detection using Large Vision-Language Models](https://dl.acm.org/doi/pdf/10.1145/3627673.3679826)
用于多模态虚假信息检测的管道，在零样本训练的基础上，充分利用证据进行判断。

证据检索：
- 文本检索：使用 MOCHEG 中预训练的 SBERT 模型，获取与查询的前 N 个最相关的句子，作为初始检索。再将查询和初始检索到的句子输入 Mistral 模型，并使用不同的提示模板来评估句子与查询之间的相关性。
- 图像检索：使用 CLIP 模型的文本编码器和图像编码器，获取与查询的前 N 个最相关的图像，同样作为初始检索。将查询和初始检索到的图像输入 InstructBLIP 模型，并使用不同的提示模板来评估图像与查询之间的相关性。

根据模型的回答和初始检索分数，对句子或者图像进行重排序。根据重排序后的分数，选择 Top-K 个最相关的文本和图像证据作为最终检索结果，用于后续的事实核查步骤。

事实验证：
- 构建提示模板，将声明和多模态证据对（文本-图像对）输入模型，模型需要判断证据是否支持或反驳声明。
- 对于文本证据，使用单层提示；对于多模态证据采用两级提示，首先判断证据是否足够，然后对于足够信息的证据，进一步判断是支持还是反驳。

最终模型在 MOCHEG 和 Factify 两个数据集上进行实验。数据集经过手动注释，证据样本更完整。


### [LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation](https://arxiv.org/abs/2402.11943)
提出的框架结合了 LVLM 的直觉、推理能力和外部知识来增强虚假信息检测的准确性。

初始推理阶段：
- LVLM 接受图像-文本对，生成预测和推理
- LVLM 根据推理判断是否需要外部知识、上下文信息是否足够判断、是否有内容需要验证
- 如果不需要外部知识，直接输出预测
- 如果需要外部知识，将提取外部证据进一步判断
- 如果外部知识仍然不足以判断，则标记“未验证”，输出预测

多模态检索：
- LVLM 接受图像-文本对、推理，对文本生成一个简短的标题，根据推理中的信息，生成问题进行验证，最后，将标题与问题组合为键通过 DuckDuckGo 搜索 API 搜索相关的文档集
- 使用 Google 搜索追踪图像来源，寻找包含与图像高度类似图像的网页，使用该网页的标题作为证据用于评估图片的上下文

资源精炼：
- 从搜索所得文档集选取前 k 个相关资源，使用 LVLM 评估其中每个文档的相关性，筛选出高度相关的文档
- 对于筛选出的文档，使用 LVLM 提取主要内容中的关键片段、文档网页标题、发布日期，作为证据三元组条目
- 将图像证据与文本条目结合为汇总证据

细化预测：
- 将图像-文本对和汇总证据输入 LVLM 重新进行评估

最终模型在 Twitter 和 Fakeddit 数据集上进行测试，初始阶段推理和视觉检索对模型的性能至关重要。



### [FKA-Owl: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs](https://dl.acm.org/doi/abs/10.1145/3664647.3681089)
 基于 LVLM 的多模态虚假新闻检测框架，通过引入伪造知识增强 LVLM 的推理能力，有效解决跨域检测难题。


文本图像编码器：
- 使用 ImageBind-Huge 将图像编码为多层次视觉特征，将文本编码为语义特征

跨模态推理：
- 从图像编码器的不同层级提取多层次视觉特征
- 分别以图像和文本特征作为查询和键值，进行交叉注意力计算，提取跨模态语义相关性
- 将图像与文本的特征进行连接，通过线性层投影到 LVLM 的词嵌入空间，生成语义嵌入

视觉伪造痕迹定位：
- 使用文本编码器对两种类别提示（“自然”和“不自然”）进行编码，并将低分辨率的特征通过连续的反卷积层转换为高分辨率的特征，以恢复局部空间细节
- 将文本和视觉特征投影到共享空间，计算相似度分数，用于建立操作分割图，实现每个像素的预测
- 使用可学习的标记作为查询，而视觉特征作为键和值，通过多头注意力聚合与伪造痕迹相关的局部信息作为聚合标记
- 利用双分支特征（操作分割图和聚合标记）中的伪造痕迹嵌入知识，分别设计多个卷积层和简单的线性层作为投影器，将这些特征转换为连续伪造痕迹嵌入

知识融合：
- 将语义嵌入和伪造痕迹嵌入映射到 LVLM 的语言空间，并与图像特征和人工提示进行融合，用于深度推理

提示策略：
- 将问题和答案选项呈现给 LVLM，使其预测与选定答案相关的符号，促进模型进行多角度思考
- 引入可学习的连续向量，与语义嵌入结合，提取额外的语义信息，并减轻 LVLM 学习伪造知识对齐的负担

使用 Vicuna-7B 作为推理 LVLM，并结合图像特征、伪造知识嵌入和人工提示进行深度推理。
损失函数使用交叉熵损失，并设计了像素级定位损失（focal 和 dice）和补丁级定位损失（L1 和 GloU）


最终模型在 DGM4 数据集上进行训练，在 NewsCLIPpings 数据集上进行测试，以验证模型的跨域性能。



### [Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection](https://aclanthology.org/2024.acl-long.316/)
基于多视图学习的虚假新闻检测框架，可以有效地捕捉新闻事件中的多模态不一致性。

事件不一致性编码器：
- 对每个图像-文本对进行解码，提取其特征，将两者特征连接，构成一个多模态图，其中节点表示文本和图像，边表示节点之间的相似度
- 使用命名实体识别识别文本中的主体、对象和地点，将其与文本向量连接，生成文本事件子图；将文本实体与图像中相似度最高的物体节点连接，生成图像事件子图
- 对多模态图和事件子图使用 GCN 进行卷积，对节点进行表示，并调整边权重
- 使用比较函数计算文本事件子图和图像事件子图的不一致性，获得事件不一致性特征

情感和模式编码器：
- 对文本进行情感分析，提取情感特征向量
- 对图像进行离散余弦变换（DCT），再使用多头自注意力编码变换后的图像特征，获得模式特征向量

单视图可信度计算：
- 将每个视图的分类器输出视为“证据”，估计 Beta 分布参数
- 根据 Beta 分布参数和主观逻辑理论框架，计算每个视图的可信度
- 将三个视图（事件不一致性、情感、模式）的可信度连接作为可信度向量

多视图融合：
- 使用多头自注意力，将每个视图的特征向量与其可信度相乘，得到加权可信度特征
- 计算不同视图之间的结构差异，作为结构差异特征
- 将加权可信度特征与结构差异特征连接，作为融合特征
- 将融合特征与结构差异特征连接，再使用线性层获得预测结果

最终模型在 Twitter，Weibo 和 Pheme 数据集上进行训练评估。



### [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2403.03170)
基于 InstructBLIP 的 LVLM，通过指令微调和外部知识增强来增强对 OOC 虚假信息的识别能力。

底层模型：InstructBLIP
- 使用 ViT-G/14 进行图像特征提取；使用 Vicuna-13B 进行文本理解和生成
- Q-Former：将图像特征转换为软提示，并与文本指令进行交互，引导语言模型进行推理

微调：
- 阶段一： 采集 NewsCLIPpings 数据集中的新闻图像和标题，通过 ChatGPT-4 生成包含图像描述的指令数据。微调 Q-Former，使模型能够将新闻图像中的视觉概念与对应的细粒度实体名称进行对齐
- 阶段二：使用用 GPT-4 生成包含判断和解释的 OOC 指令数据。微调 Q-Former，使模型能够识别文本和图像之间的不一致性，并进行解释

推理：
- 使用 Google Entity Detection API 识别图像中的视觉实体，将识别出的实体信息作为补充信息输入到指令中，引导模型进行内部一致性检查，生成内部检查结果
- 通过图像搜索获取相关网页文本，将网页文本与新闻标题输入到模型中，判断两者之间的相关性，生成外部检查结果
- 将内部和外部检查结果输入到模型中，据所有信息进行综合推理，并输出最终的判断和解释

最终模型在 NewsCLIPpings 数据集上进行训练和评估。



### [MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)
用于多模态虚假信息解释和蒸馏推理的框架。

数据增强：
- 使用 OCR 识别图像中的文本内容，转换为可编辑的文本形式；使用图像描述技术为图像内容生成描述性文本
- 将输入的图像作为查询，使用 Google 的反向图像搜索检索文本证据；将输入的文本内容作为查询，使用 Google 可编程搜索引擎检索视觉证据，对检索到的视觉证据进行 OCR 和图像描述处理

解释提取：
- 将处理后的内容输入教师 LLM
- 使用标签提示模板指导教师 LLM 解释其标记的原因
- 教师 LLM 输出包含解释的真实标签，作为训练学生 LLM 的数据

知识蒸馏：
- LoRA 微调学生 LLM，教师 LLM 的输出与学生 LLM 的输入结合作为训练数据

模型最终在 MR2 数据集上训练评估。


### [FMC: Multimodal fake news detection based on multi-granularity feature fusion and contrastive learning](https://www.sciencedirect.com/science/article/pii/S1110016824010019)
基于多粒度特征融合和对比学习的多模态虚假新闻检测框架。

多模态特征提取：
- 使用 CLIP 提取文本和图像的语义特征，将文本和图像编码到同一特征空间中，得到粗粒度的文本特征和图像特征
- 使用 BERT 提取文本的上下文特征，得到细粒度的文本特征；使用 ResNet50 提取图像的层次特征，得到细粒度的图像特征

多粒度特征融合：
- 将不同粒度的文本特征和图像特征拼接成多粒度的文本特征和图像特征
- 将多粒度的文本特征和图像特征转换到同一特征空间，使用均方误差损失对齐不同模态特征，增强特征一致性
- 计算查询矩阵、键矩阵和价值矩阵，通过注意力机制分别得到增强的多粒度文本特征和多粒度图像特征
- 将多粒度文本特征和多粒度图像特征拼接成多粒度多模态特征

联合学习：
- 对多粒度多模态特征使用多头自注意力，获得最终的多模态特征
- 选择与输入新闻具有相同标签的 k 个正样本和具有不同标签的 k 个负样本，并计算对比损失 ，以此减小相同标签新闻表示之间的距离，增大不同标签新闻表示之间的距离
- 使用池化层和 dropout 层对多模态特征进行处理，获得预测标签

最终模型在 ReCOvery，GossipCop 和 MR2 数据集上进行训练和评估。


