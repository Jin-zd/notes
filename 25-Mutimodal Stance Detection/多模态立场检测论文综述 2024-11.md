### [Advancing Stance Detection of Political Fan Pages: A Multimodal Approach](https://dl.acm.org/doi/abs/10.1145/3589335.3651467)
基于无监督学习的多模态立场检测模型，它结合了文本内容、视觉图像和用户互动信息来识别政治立场。

数据来源于2021年台湾公投相关的22个政治议题为主要内容的页面的476篇帖子和403张图片，以及用户互动信息（点赞和评论）。

政治立场中存在着多种不同的观点，社交媒体数据存在噪声、虚假信息和网络水军等，都会影响立场检测的准确性。

数据嵌入：
- 文本：去除 URL、标签和表情
- 图像：使用 BEiT 转换为向量嵌入
- 用户互动：使用无向图表示，节点代表帖子，边代表用户之间的互动

特征提取：
- Sentence-BERT 提取文本向量特征
- BEiT 的输出作为图像向量特征
- 对文本进行情感分析，获得情感特征
- 帖子发布时间作为时间特征
- 使用 GraphSAGE 进行图表示学习，获得图嵌入特征

特征融合与立场检测：
- 将上述所有特征拼接为融合特征
- 使用 Mean-Shift 进行聚类，获得不同的政治立场结果


### [Multi-modal Stance Detection: New Datasets and Model](https://arxiv.org/abs/2402.14298)
用于多模态立场检测的数据集和多模态框架。

数据集：
- 使用Twitter Streaming API收集包含特定关键词的推文，并保留包含文本和至少一张图片或视频/GIF的帖子。对于视频/GIF，保留其第一帧；对于包含多张图片的帖子，将文本与每张图片组合成多个样本。
- 对每个样本进行了仔细的标注。
- 构建了 5 个数据集：MTSE (Multi-modal Twitter Stance Election 2020)、MCCQ (Multi-modal COVID-CQ)、MWTWT (Multi-modal Will-They-Won’t-They)、MRUC (Multi-modal Russo-Ukrainian Conflict)、MTWQ (Multi-modal Taiwan Question)，包含 5 个领域，总计17544个样本，每个样本由一个目标、一个文本和一个关联的图像组成。
- 每个数据集按照7:1:2的比例划分为训练集、验证集和测试集。

数据集并没有考虑音频模态和视频信息。

TMPT：
- 文本提示微调：针对每个目标构建一个提示，使用预训练模型编码包含提示和文本内容的输入，获得文本模态的特征表示。
- 视觉提示微调：将图像分割成固定大小的图像块，并使用线性嵌入投影将每个图像块映射到一个固定维度的向量；为每个目标设计一个视觉提示；使用预训练的视觉模型对包含视觉嵌入和视觉提示的输入进行编码，获得视觉模态的特征表示。
- 多模态融合：使用两个全连接层分别对文本和视觉模态的特征向量进行处理，获得隐藏表示，将文本和视觉模态的隐藏表示进行拼接，获得最终的多模态立场特征。
- 将多模态立场特征输入到一个全连接层，并使用 softmax 函数输出立场标签的概率分布。

模型需要针对每个目标设计特定的提示，并且没有集成外部目标相关的知识。


### [Few-Shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings](https://ieeexplore.ieee.org/abstract/document/10098547)
结合了文本和网络特征的立场检测模型，通过聚合多模态嵌入来提高立场分类的准确性。

文本分类器：
- 输入推文的文本数据和目标信息，使用 RoBERTa 将文本编码为特征向量
- 将特征输入进线程层中获得文本分类结果

网络分类器：
- 输入网络信息：关注者、朋友和点赞
- 将用户 ID 作为节点，用户之间的关系作为边，构建用户网络图。使用 Node2Vec 对网络图进行编码，获得特征向量
- 同样，将特征输入进线程层中获得网络信息分类结果

输出聚合器：
- 输入文本分类器和网络分类器的结果
- 采用多数投票策略，整合分类结果，选择票数最多的类别作为最终输出

模型在 P-stance 数据集上训练，该数据集包含三位政治人物的推文，并标注了立场标签。

该模型目前只适用于二类立场分类，采用多数投票策略进行简单的特征融合，达到相对较高的预测准确率需要较多的训练数据支撑。


