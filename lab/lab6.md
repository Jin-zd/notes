### 1 LLama2 运行结果
![[屏幕截图 2024-12-12 190555.png]]
模型越大，运行速度越慢。

### 2 LLama2 优化
采用 AVX 进行优化，
优化过程中使用了AVX 指令对矩阵乘法过程中的内积部分进行了向量化处理。
通过使用 AVX 提供的 mm256_loadu_ps 和 mm256_add_ps 等指令，将每次计算从单一标量扩展为 8 个浮点数的并行操作。利用 AVX 指令加载、计算、存储数据，每次处理 8 个浮点数； 在最后的部分，针对剩余元素进行了逐元素计算。

![[Pasted image 20241213204408.png|400]]
优化之后，进行测试：
![[屏幕截图 2024-12-12 192457.png]]
可见，相比之前的运行速度有了很大的提升。

多次测试，得到

|                 | 1      | 2      | 3      | avg    |
| --------------- | ------ | ------ | ------ | ------ |
| stories15M.bin  | 242.84 | 241.99 | 242.51 | 242.45 |
| stories42M.bin  | 93.95  | 93.88  | 93.90  | 93.91  |
| stories110M.bin | 35.82  | 35.88  | 35.67  | 35.79  |
相较于优化之前，stories15M.bin 速度提升为原来的 187.6 %，stories42M.bin 提升为原来的 221.5%，stories110M.bin 提升为原来的 227.7%。

