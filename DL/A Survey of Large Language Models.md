# 1 总述（Overview）
## 1.1 LLM 背景知识
- **Scaling Laws**：对于 Transformer 架构的模型，增大模型参数规模、数据规模和计算规模能显著提高模型能力，但近期面临边际效益递减的问题。
- **Emergent Abilities**：当模型的规模达到一定水平时，其性能显著高于随机水平，并且会出现一些小规模模型不具有的能力。典型的涌现能力有上下文学习、指令遵循和逐步推理。
## 1.2 GPT 系列模型的发展
- GPT 系列模型成功的关键：采用 Decoder-Only 的 Transformer 架构、扩大模型的规模。
- 能力增强方式：使用代码数据训练、与人类行为进行对齐（RLHF）。

# 2 预训练（Pre-Training）
预训练建立了 LLM 的基本能力。
## 2.1 数据收集和准备
- **De-duplication**：语料库中的重复数据会降低语言模型的多样性，可能会使训练过程变得不稳定，进而影响模型性能。
- **Tokenization**：
    - Byte-Pair Encoding (BPE) tokenization：从一组基本符号（如字母表和边界字符）开始，并迭代地将语料库中连续出现的两个标记对合并为新标记）。每次合并的选择标准基于两个连续标记的共现频率：会选择最频繁出现的对。合并过程会持续进行，直到达到预定义的大小。
    - WordPiece tokenization：WordPiece 与 BPE 的思想相似，都通过迭代合并连续的标记，但在合并的选择标准上略有不同。WordPiece 中需要训练一个语言模型，并利用该模型为所有可能的标记对打分。在每次合并时，会选择使训练数据的可能性增加最多的标记对。
    - Unigram tokenization：使用 expectation–maximization (EM) 算法。从语料库中足够大的可能子字符串或子标记集合开始，并逐步移除当前词汇表中的标记，直到达到预期的词汇表大小。
- **Data Scheduling**：
    - Data Mixture：混合不同的数据来源的数据作为预训练数据集。增加数据源的异质性（如纳入多样化的数据源）对于提升 LLMs 的下游性能至关重要。
    - Data Curriculum：在某些情况下，按照技能序列（例如：基础技能 → 目标技能）进行学习，比直接从仅专注于目标技能的语料库中学习效果更好 。
总的来说，数据准备的过程是：Data  collection -> Data cleaning -> Data scheduling。

## 2.2 架构
### 2.2.1 Transformer 典型架构
Transformer 模型的原始架构：
- **Encoder-decoder**：编码器-解码器架构中，编码器采用堆叠的多头自注意力层对输入序列进行编码，以生成其潜在表示，而解码器则对这些表示进行交叉注意力操作，并自回归地生成目标序列。
- **Casual Decoder**：因果解码器架构采用单向注意力掩码，以确保每个输入标记只能关注过去的标记和自身。输入标记和输出标记在解码器中以相同的方式进行处理。
- **Prefix Decoder**：前缀解码器架构（也称为非因果解码器）修改了因果解码器的掩码机制，以实现对前缀标记进行双向注意力计算，而对生成的标记仅进行单向注意力计算。通过这种方式，与编码器-解码器架构类似，前缀解码器可以对前缀序列进行双向编码，并自回归地逐个预测输出标记，其中编码和解码过程中共享相同的参数。
- **Mixture-of-Experts**：混合专家模型是一种稀疏门控制的深度学习模型，它主要由一组专家模型和一个门控模型组成。MoE 的基本理念是将输入数据根据任务类型分割成多个区域，并将每个区域的数据分配一个或多个专家模型。每个专家模型可以专注于处理输入这部分数据，从而提高模型的整体性能。MoE 是一种在保持计算成本不变的情况下扩展模型参数的灵活方法。

传统的 Transformer 架构通常在序列长度方面具有二次方的计算复杂度，这使得处理长输入时成本较高。目前新兴的架构主要基于参数化的状态空间模型（SSM），可以将其视为循环神经网络（RNN）和卷积神经网络（CNN）的结合
主要代表有：
- **Mamba**：该架构在状态更新时有选择性地过滤或保留信息。使用输入的函数替换了 SSM 层的原始固定参数，根据当前输入有选择性地过滤掉之前状态和当前输入的信息。
- **RWKV**：该架构采用了时间混合模块，即带有门控的循环神经网络（RNN），以及特殊的前馈神经网络构成的通道混合模块。在这些模块中，使用了 token 移位（即当前 token 和前一个 token 的线性组合）代替 token 表示作为输入。
- **RetNet**：该架构提出了一种多尺度保留（MSR）模块，以替代 Transformer 中的注意力模块。在 MSR 模块中，输入首先被映射为查询、键和值，然后利用键和值的乘积来更新状态。之后，利用查询将状态投影到输出。与传统的 SSM 类似，RetNet 同时保持了并行和递归计算的能力。
- **Hyena**：该架构采用长卷积来取代注意力模块。在长卷积模块中，基于相对位置的滤波器用于将不同位置的信息聚合到中间表示中，并且使用门控函数将中间表示进一步投影到最终输出。由于使用了长卷积，其无法像循环神经网络（RNN）那样进行推理，而必须显式地访问所有先前的状态。
### 2.2.2 Transformer 架构细节
归一化（Normalization）是一种广泛用于稳定神经网络的训练，典型归一化方法有：
- **LayerNorm**：计算每一层中所有激活值的均值和方差，以重新中心化和重新缩放激活值。
- **RMSNorm**：为了提高 LayerNorm 的训练速度，仅通过激活值的平方和的均方根（RMS）对激活值进行重新缩放，而不是使用均值和方差。
- **DeepNorm**：用于稳定深度 Transformer 模型的训练。

归一化方法应用于模型架构的位置也十分重要，位置上有以下几种选择：
- **Post-LN**：放置于残差块之间，由于输出网络的巨大梯度，该方式会使训练不稳定。
- **Pre-LN**：在每个子层之前应用了pre-LN，并且在最终预测之前放置了一个额外的 LN，该方式会带来性能下降。
- **Sandwich-LN**：基于Pre-LN，在残差连接之前增加了额外的 LN，以避免 Transformer 层输出中的数值爆炸问题。

激活函数的选择上，主流的 LLM 使用 GeLU，一些变体，如 SwiGLU 和 GeGLU 也被用于最新的 LLM 上。

由于 Transformer 中的自注意力模块具有置换等变性，因此使用位置嵌入（PE）来注入绝对或相对位置信息以对序列进行建模。常用的位置嵌入方式有：
- **Absolute position embedding**：两种绝对位置嵌入的变体：正弦位置嵌入和可学习位置嵌入，后者通常用于现有的预训练语言模型。
- **Relative position embedding**：根据键和查询之间的偏移量生成。
- **Rotary position embedding**：据每个键或查询的绝对位置设置特定的旋转矩阵。将查询向量和键向量中每对连续的元素组合成一个维度，因此对于原始长度为 d 的嵌入，就有 d/2 个维度。
- **ALiBi**：与相对位置嵌入类似的是，根据键和查询之间的距离对注意力分数施加基于距离的惩罚。与相对位置嵌入不同的是，ALiBi 中的惩罚分数是预先定义的，没有任何可训练参数。

注意力机制是 Transformer 架构成功的关键，其允许序列中的标记相互交互，并计算输入和输出序列的表示。常用的注意力方式有：
- **Full attention**：以成对的方式进行，考虑序列中所有标记对之间的关系。
- **Sparse attention**：与整个序列不同，每个查询只能根据位置关注一部分标记。
- **Multi-query/grouped-query attention**：不同的头在键和值上共享相同的线性变换矩阵。以微小的模型质量损失为代价，实现了更高的推理速度。
- **FlashAttention**：将输入划分为块，并引入必要的重新计算，以更好地利用快速存储器 SRAM。
- **PagedAttention**：将每个序列划分为子序列，这些子序列对应的 KV 缓存被分配到不连续的物理块中。参考了操作系统中的分页思想。

为了增强泛化能力和训练稳定性，建议选择 RMSNorm 进行层归一化，并使用 SwiGLU 或 GeGLU 作为激活函数。此外，嵌入层后不应立即使用 LN，这可能会导致性能下降。对于位置嵌入，RoPE 或 ALiBi 是更好的选择，因为它们在长序列上表现更好。
### 2.2.3 预训练任务
预训练将大规模语料库中的通用知识编码到庞大的模型参数。常见的预训练任务有：
- **Language Modeling**：给定一个标记序列，该任务的目标是基于序列中前面的标记自回归地预测目标标记。
- **Denoising Autoencoding**：该任务的输入是带有随机替换片段的损坏文本，然后语言模型被训练以恢复被替换的标记。
- **Mixture-of-Denoisers**：MoD将语言模型（LM）和去噪自编码器（DAE）目标视为不同类型的去噪任务，即S去噪器（LM）、R去噪器（DAE，短跨度且低程度的损坏）和X去噪器（DAE，长跨度或高程度的损坏）。
### 2.2.4 解码策略
在 LLMs 完成预训练后，必须采用特定的解码策略来生成适当的输出：
- **Greedy search**：在文本生成任务（例如机器翻译和文本摘要）中能够取得令人满意的结果，其中输出高度依赖于输入。
- **Beam search**：在解码过程中每一步都保留具有最高n（集束大小）概率的句子，并最终选择具有最高概率的生成回应。
- **Length penalty**：根据句子长度（除以长度的指数α）对句子概率进行归一化，从而解决搜索策略偏向长度较短的句子的问题。
- **Temperature sampling**：为了调节采样的随机性，可以调整用于计算词汇表中第 j 个标记的概率的 softmax 函数的温度系数。降低温度 t 会增加选择高概率词汇的可能性，同时减少选择低概率词汇的可能性。
- **Top-k sampling**：直接截断概率较低的标记，仅从概率最高的 k 个标记中进行采样。
- **Top-p sampling**：通过从累积概率高于（或等于）p的最小集合中进行采样。
## 2.3 模型训练
### 2.3.1 优化设置
- **Batch Training**：动态调整批量大小能够有效稳定 LLM 的训练过程。
- **Learning Rate**：现有的 LLMs 通常在预训练阶段采用类似的包含预热和衰减策略的学习率计划。
- **Optimizer**：Adam 和 AdamW 被广泛用于训练 LLMs。
### 2.3.2 大规模训练技术
- **3D Parallelism**
    - Data parallelism：在多个 GPU 上复制模型参数和优化器状态，然后将整个训练语料库分配到这些 GPU 中。这样，每个 GPU 只需处理分配给它的数据，并执行前向和反向传播以获得梯度。不同 GPU 上计算出的梯度将进一步聚合，以获得整个批次的梯度，用于更新所有 GPU 中的模型。
    - Pipeline parallelism：将 LLM 的不同层分布到多个 GPU 上。可以采用多批次数据和异步梯度更新的技术来提高流水线效率。
    - Tensor parallelism：专注于分解 LLM 中的张量（参数矩阵）。
- **Mixed Precision Training**：FP16 可能会导致计算精度的损失，从而影响最终模型性能。BF16 被广泛用于训练，其分配的指数位比 FP16 更多，而尾数位则更少。

# 3 后训练（Post-Training）
后训练包含指令微调（instruction tuning）和对齐微调（alignment tuning），前者主要旨在增强（或解锁）LLM 的能力，后者旨在使 LLM 的行为与人类价值观或偏好保持一致。
## 3.1 指令微调（Instruction Tuning）
- 指令微调是一种对预训练 LLM 进行微调的方法，通过在自然语言形式的格式化实例集合上进行训练
- 一个指令格式的实例包括一个任务描述（称为指令）、一个可选的输入、相应的输出以及少量的示例（可选）。
- 增加任务数量可以显著增强 LLM 的泛化能力；自然语言格式的设计也对 LLM 的泛化性能产生高度影响；指令微调数据依赖于精心设计的提示引导 LLMs 对给定的指令进行细化或重写。
- 随着 LLMs 的能力不断提高，数据合成方法已成为生成大量指令数据的主流方法。
- 指令微调策略：
    - 平衡数据分布：按比例混合示例数据。
    - 整合指令微调和预训练：在指令微调过程中引入了预训练数据，可以被视为模型微调的一种正则化手段。
    - 多阶段指令微调：例如先对 LLMs 进行大规模任务格式指令的微调，然后再针对日常聊天指令进行微调。
- 不同规模的模型都可以从指令微调中受益，随着参数规模的增加，性能得到提升。经过指令微调训练的 LLMs 可以跨语言泛化到相关任务。经过指令微调，现有的通用 LLMs 可以被适应为特定领域专家。
## 3.2 对齐微调（Alignment Tuning）
对齐可能会在一定程度上损害 LLMs 的通用能力，相关文献中将其称为 alignment tax。
生成人类反馈数据的主要方法是人工标注。
### 3.2.1 Reinforcement Learning from Human Feedback（RLHF）
RLHF 系统主要由三个关键组件构成：一个待对齐的预训练语言模型（LM）、一个从人类反馈中学习的奖励模型以及一个用于训练语言模型的强化学习算法。
奖励模型（RM）提供反映人类对语言模型生成文本偏好的指导信号，通常以标量值的形式呈现。
近端策略优化（PPO）是一种在现有工作中被广泛用于对齐的强化学习算法。

RLHF 的关键步骤：
- **Supervised fine-tuning**：为了使语言模型最初能够执行期望的行为，通常需要收集一个包含输入提示（指令）和期望输出的监督数据集以用于微调该语言模型。
- **Reward model training**：使用 LM 根据采样的提示（来自监督数据集或人类生成的提示）作为输入生成一定数量的输出文本。然后，人类标注者对这些文本对进行偏好标注。最后，RM 被训练以预测人类偏好的输出。
- **RL fine-tuning**：对语言模型（LM）进行对齐（即微调）被形式化为一个强化学习（RL）问题。在这种设置中，预训练的语言模型充当策略，它以提示作为输入并返回输出文本，其动作空间是词汇表，状态是当前生成的标记序列，而奖励由奖励模型（RM）提供。

RLHF 策略：
- **Effective reward model training**：使用大型奖励模型（例如，与原始模型大小相当或更大）往往更为有效，因为大型奖励模型通常在判断 LLMs 生成输出的质量方面表现更好。
- **Effective RL training**：在进行强化学习之前，通过在对齐数据集上对 LLMs 的最佳输出结果（称为 rejection sampling 或 best-of-N）进行微调，直到收敛。
- **Efficient RL training**：由于RLHF要求LLM生成多个候选输出，与其多次调用采样解码程序，不如使用束搜索解码算法更为高效。其只需要进行一次解码以生成回应，同时这种策略也能增强生成的候选回应的多样性。

过程监督的 RLHF（Process-Supervised RLHF）：
过程监督的强化人类反馈（RLHF）对生成内容中的每个单独组成部分（例如句子、单词或推理步骤）进行评估，利用细粒度的监督信号来指导训练，帮助 LLMs 优化不理想的生成内容。
- 使用细粒度监督信号训练过程监督奖励模型（PRM），这些模型可以在 RLHF 过程中产生分步奖励（例如基于句子或基于标记的奖励）。
- 为了有效地利用 PRMs 中的流程监督信号，主要使用这些细粒度的信号来评估 LLM 响应中的各个部分，然后引导 LLMs 调整其生成行为，以最大化响应所获得的奖励。
### 3.2.2 无需 RLHF 的对齐微调
主要是使用高质量对齐数据集通过监督学习微调 LLMs。
- 数据收集方式主要有：利用现有的奖励模型；利用 LLM 生成；利用 LLM 对话。
- 监督对齐微调下，主要优化目标仍然是传统的交叉熵损失。由于每条指令的响应都可以通过奖励模型进行评分，因此可以使用排序损失作为辅助优化目标来训练模型，以保留这些响应的排序顺序。
- Direct preference optimization（DPO）提出使用策略模型重新参数化响应奖励，然后原始奖励建模目标可以仅基于策略模型重新表述。通过这种方式，DPO 去除了显式的奖励建模步骤，而仅涉及策略模型的新的学习目标的优化等同于奖励的优化。DPO在平衡正实例和负实例的学习方面可能存在困难，一个较弱的参考模型也会影响对齐性能。
- 为了增强响应与指令之间的相关性，一些工作采用对比学习来提高正确指令-响应对的概率，同时降低错误指令-响应对的概率。
### 3.2.3 STF 和 RLHF 的利弊
STF（Supervised Fine-Tuning）采用教师强制方法，直接优化演示输出的可能性：
- 当演示数据超出 LLMs 的知识或能力范围时，可能会潜在地引发幻觉行为。
- 将更优的模型提炼出来训练能力较弱的模型可能会增加生成幻觉文本的可能性，从而可能影响 LLM 的事实准确性。

RLHF（Reinforcement Learning from Human Feedback） 首先学习奖励模型，然后利用它通过强化学习训练（例如 PPO）来改进 LLM：
- RLHF 在减少有害回答和增强模型能力方面十分有效。 RLHF 可以同时提高有用性和无害性的得分。
- RLHF 继承了经典强化学习算法的缺点，如采样效率低下和训练不稳定。
## 3.3 参数高效的模型适配
- **Adapter Tuning**：该方法将小型神经网络模块（称为适配器）整合到 Transformer 模型中。适配器模块将被集成到每个 Transformer 层中，通常在 Transformer 层的两个核心部分（即注意力层和前馈层）之后以串联的方式插入。再微调过程中，适配器模块根据具体任务目标进行优化，同时在这个过程中保持原始语言模型的参数不变。
- **Prefix Tuning**：在语言模型的每一层 Transformer 中添加一系列前缀，这些前缀是一组可训练的连续向量。这些前缀向量是特定于任务的，可以被视为虚拟的标记嵌入。为了优化这些前缀向量，可以通过学习一个 MLP 函数，将一个较小的矩阵映射到前缀的参数矩阵，而不是直接优化前缀。优化完成后，映射函数将被丢弃，只保留导出的前缀向量以增强特定于任务的性能。
- **Prompt Tuning**：该方法主要集中在输入层嵌入可训练的提示向量。在实现过程中，特定任务的提示嵌入与输入文本嵌入相结合，随后输入到语言模型中。在训练过程中，只有提示嵌入会根据特定任务的监督进行学习。由于这种方法仅在输入层包含少量可训练参数，因此其性能高度依赖于底层语言模型的模型容量。
- **Low-Rank Adaptation (LoRA)**：该方法通过对每个全连接层的更新矩阵施加低秩约束，从而减少可训练参数，以适应下游任务。基本思想是在冻结原始矩阵 $W \in \mathbb{R}^{m \times n}$ 的同时，通过低秩分解矩阵来近似参数更新 $\Delta W$ ，即 $\Delta W = A \cdot B^{\top}$ ，其中 $A \in \mathbb{R}^{m \times k}$ 和 $B \in \mathbb{R}^{n \times k}$ 是用于任务适应的可训练参数，且 $k \ll \min(m, n)$ 表示降低的秩。

# 4 利用（Utilization）
## 4.1 提示（Prompting）
- 有四个关键要素可以描绘出用于激发 LLM 完成任务能力的提示的功能，包括任务描述、输入数据、上下文信息和提示风格。
- 少量示范可以帮助 LLMs 在无需参数调整的情况下学习输入和输出之间的语义映射。
- 离散提示通常由一系列自然语言标记组成。尽管其形式简单且灵活，但由于组合爆炸式的巨大搜索空间，优化离散空间中的提示是一个具有挑战性的问题，有如下几种方式：
    - 基于梯度的方法。这种方法旨在通过梯度更新最大化输出概率，从而优化提示搜索过程。这种搜索过程可能极其昂贵，因为它需要对提示的每个位置的每个候选标记进行评估，从而导致了额外的多次前向传播。
    - 基于强化学习的方法。多数研究将离散提示优化表述为强化学习（RL）问题，并利用强化学习算法进行优化。
    - 基于编辑的方法。即为直接根据任务表现编辑现有提示。
    - 基于 LLM 的方式。由于 LLMs 的卓越能力，越来越多的研究直接利用其作为提示生成器。
- 与离散提示不同，连续提示由一组连续的嵌入向量组成，可以通过基于下游任务损失的梯度更新直接进行优化。主要缺陷在于缺少大规模的高质量的标记数据：
    - 使用足够数据的提示学习（Prompt learning with sufficient data）。在这种方法中，大多数现有方法将连续提示视为可训练的模型参数，然后利用监督学习通过基于足够的下游任务数据来最小化交叉熵损失，从而优化连续提示。
    - 在数据稀缺的情况下进行提示转移（Prompt transferring with scarce data）。如有一种基于提示的迁移学习方法，首先为几个具有代表性的源任务学习一个单一的连续提示，然后使用该提示来初始化目标任务的提示。
## 4.2 上下文学习（In-Context Learning）
- 上下文学习使用一种格式化的自然语言提示，包含任务描述或几个任务示例作为演示。
- 首先，从任务数据集中选择几个示例作为演示。然后，按照特定顺序将它们组合成具有专门设计的模板的自然语言提示。最后，将测试实例附加到演示中，作为 LLM 生成输出的输入。基于任务演示，LLM 可以在没有明确梯度更新的情况下识别并执行新任务。
- 示例选择：启发式算法和基于 LLM 选择的方法。
- 示例格式：一种直接的方法是用相应的输入-输出对实例化一个预定义的模板。为了构建更具信息量的模板，近期的研究考虑增加任务描述或通过链式思考提示增强LLM 的推理能力。
- LLM 有时会受到近期偏差的影响，即它们倾向于重复示例末尾附近的答案。因此，合理安排示例的顺序是十分重要的。
- 预训练是怎么影响上下文学习的？
    - 上下文学习最早是在 GPT-3 中提出的，并且已经证明，模型规模越大，上下文学习能力就越显著。
    - 训练任务的设计是影响 LLM 上下文学习能力的重要因素。
    - 上下文学习可以从理论上被解释为在具有长程连贯性的文档上进行预训练的产物。
- LLMs 是怎么表现出上下文学习能力的呢？
    - LLMs 通过示例识别任务，并利用预训练中获得的先验知识来解决新的测试任务。
    - 有观察发现，将示例中的输入或标签替换为从输入或标签空间中随机采样的内容，并不会严重影响 LLM 的性能，这表明 LLM 主要是从演示中识别目标任务，而不是从演示中学习。
    - LLM 仅通过演示来学习在预训练阶段未见过的新任务。
- LLMs 在上下文学习中表现出任务识别和任务学习的能力，但这两项能力似乎需要不同的模型规模，其中任务识别能力更容易获得。
- 小型语言模型倾向于忽略标签，主要依赖其先验知识来完成任务，而 LLMs 则能够超越其先验知识，并从示范中获取新知识，从而获得更好的结果。
## 4.3 思维链提示（Chain-of-Thought Prompting）
- 思维链是连接输入和输出的一系列中间推理步骤。
- 使用多样化的思维链（即每个问题的多种推理路径）可以有效提升性能。
- 具有更复杂推理路径的提示更有可能激发 LLMs 的推理能力，从而可能导致生成正确答案的准确率更高。
- 增强思维链的方式：
    - 采用样本多条推理路径，而不是使用贪婪解码；
    - 验证生成的推理步骤的正确性，可以使用经过训练的验证器或语言模型本身。
- 推理结构扩展：
    - 树状结构：将推理过程表述为层次化的树形结构，其中中间思想是节点。LLM 能够并行探索多条推理路径，并进一步支持向前看和回溯操作，以促进更全面的决策。
    - 图状结构：将推理过程概念化为任意图，其中顶点表示中间思维，边表示这些思维之间的相互依赖关系。与树状结构相比，它在生成新想法时可以进一步利用其他推理路径中的想法。
- 思维链推理只对足够大的模型有积极影响，而对小模型则没有影响。此外，由于思维链提示通过增加中间推理步骤来增强标准提示，因此它主要对需要逐步推理的任务有效。
- 关于思维链推理能力的来源，普遍认为可以归因于对代码的训练，因为经过此类训练的模型表现出强大的推理能力。
- 一项研究确定了思维链提示中的三个关键组成部分，即符号（例如，算术推理中的数字量）、模式（例如，算术推理中的方程式）和文本（即既非符号也非模式的其余标记）。
## 4.4 规划（Planning）
- 任务规划包含三个部分：
    - 任务规划器，由 LLMs 扮演，旨在生成解决目标任务的完整计划。基于LLM的任务规划器可以通过记忆机制进行增强，用于计划的存储和检索，这有助于处理长期任务。
    - 计划执行器负责执行计划中的行动。
    - 环境是指计划执行者实施行动的场所，可根据具体任务设置不同，它向任务规划器提供关于行动执行结果的反馈。
- LLMs 可以被提示生成一系列动作，供计划执行者执行以解决复杂任务；基于代码的方法被提出用于生成以编程语言形式的可执行代码的更具可验证性的计划，通过这种方式，LLMs 首先被提示生成程序，然后利用确定性求解器来执行它。
- LLM 本身可以被用作反馈提供者，一种直接的方式是通过提示直接评估生成计划的质量。或者可以通过外部的工具提供反馈。
- 规划增强：
    - 一些工作通过添加明确的推理过程来提取反馈中的关键信息。
    - 树状思维链允许利用搜索算法进行回溯，从而实现全局规划。还可以利用反馈信号来修订整个计划。
    - 为了处理长期任务，可以利用长期记忆辅助计划细化，以及通过上下文学习利用 LLMs 的短期记忆。

# 5 能力和评估（Capcity and Evaluation）
## 5.1 能力
### 5.1.1 语言生成
- 语言建模旨在根据之前的标记预测下一个标记，主要侧重于基本语言理解和生成的能力。
- 现有的 LLMs 也展现出生成正式语言的强能力，尤其是在满足特定条件的情况下生成计算机程序（即代码），这被称为代码合成。
### 5.1.2 知识利用
- 知识利用是智能系统完成知识密集型任务（例如，常识问答和事实补充）的重要能力，这些任务基于支持性事实证据。
- LLMs 在某种程度上可以被视为一个知识库，可以利用其来填补或预测知识单元（例如知识三元组）中缺失的部分。
- 一个具有挑战性的问题是幻觉生成，其中生成的信息要么与现有来源相冲突（内在幻觉），要么无法通过现有来源进行验证（外在幻觉）。
    - 为缓解这一问题，现有的研究广泛采用了对齐调整策略，这些策略依赖于在高质量数据上对 LLMs 进行调整，或利用人类反馈。
    - 整合外部工具以提供可信的信息来源，有助于缓解幻觉问题。
- LLMs 在处理需要超出训练数据范围的最新知识的任务时会遇到困难。
### 5.1.3 复杂推理
- 知识推理任务依赖于逻辑关系和关于事实知识的证据来回答所给问题。
- 符号推理任务主要集中在正式规则设定下操纵符号以实现特定目标，其中的操作和规则可能在预训练期间从未被 LLM 见过。
- 数学推理任务需要综合运用数学知识、逻辑和计算来解决问题或生成证明陈述。
- LLMs 可能在推理路径无效的情况下得出正确答案，或者在推理过程正确的情况下产生错误答案，从而导致得出的答案与推理过程之间存在不一致性。
- 对于复杂的推理任务，LLMs 在涉及的数值计算方面仍然面临困难，尤其是对于在预训练过程中很少遇到的符号，例如大数字的算术运算。
### 5.1.4 人类对齐
LLMs 完全有可能符合人类的价值观和需求，即实现人类对齐，这是 LLMs 在现实世界广泛应用的关键能力。
### 5.1.5 和外部环境交流
- LLMs 能够接收来自外部环境的反馈，并根据行为指令执行动作。
- 这种能力也在能够生成详细且高度逼真的行动计划的 LLMs 中出现。
### 5.1.6 工具使用
- 在解决复杂问题时，LLMs 可以使用外部工具，如果它们认为有必要的话。
- LLMs 具备自主为特定任务创建工具的能力。
## 5.2 基准与评估方法
- 经过指令微调的模型通常比基础模型表现更好。
- 小型开源模型在数学推理、与环境交互以及工具操作任务方面表现不佳。
- 在不同的与人类对齐的任务中，表现最佳的模型各不相同。
- 扩大开源模型的规模可以持续提升性能。

# 6 应用（Application）

## 6.1 自然语言处理（NLP Tasks）
- 尽管具有出色的一般能力，但 LLMs 仍然无法有效处理低资源领域的自然语言处理任务。
- 此外，LLMs 在处理经典自然语言处理任务中的复杂语义关系时仍然面临挑战。
## 6.2 信息检索（Information Retrieval）
- 大多数研究集中于将 LLMs 用作重排序器，旨在优化检索到的候选结果的排名。
- LLMs 可以被指示对给定查询的正面或负面文档进行注释，或者根据语料库中的一组文档生成相应的查询，只需参考几个示例。
## 6.3 推荐系统（Recommender Systems）
- 一些方法在零样本范式下提示 LLMs 完成推荐任务。
- 一些方法使用 LLMs 对项目和用户的辅助信息进行编码。
- 几项研究采用类似蒸馏的方法，将 LLMs 的能力（例如语义编码）转移到传统推荐系统中（即小型模型），以提升其性能。
- LLMs 也被用于开发推荐模拟器，在模拟推荐系统中用户真实行为方面展现出巨大潜力。
## 6.4 多模态大模型（Multimodal Large Language Model）
- 视觉语言对齐预训练（即第一阶段训练）的目标是通过在大规模图像-文本对上进行端到端训练，对齐视觉编码器和 LLM。
- 跨模态对齐：
    - 如果图像 - 文本对的数量不够大通常建议仅更新连接模块；
    - 如果训练数据包含高质量的文本语料库或具有细粒度注释的图像 - 文本对，可以对 LLM 进行微调以提升性能；
    - 如果图像 - 文本对的数量非常庞大，对视觉编码器进行微调也是可行的，但其效果仍需进一步验证。
- 经过视觉语言预训练后，第二阶段训练，即视觉指令微调，旨在提升多模态语言模型的指令遵循和任务解决能力。
- 现有的多模态语言模型旨在尽可能保留 LLMs 的固有能力与参数知识，同时通过利用预训练的 LLMs 和视觉编码器，有效适应多模态任务。为实现上述两个目标，视觉指令微调通常采用两种典型的训练策略，即仅优化连接模块，或者同时对连接模块和 LLMs 组件进行微调。
## 6.5 知识图谱增强
- 现有工作通常采用检索模型，首先从知识图谱中获取一个相对较小的子图，然后利用该子图通过丰富相关知识来增强 LLMs 。
- 为了解决复杂任务，通常需要 LLMs 多次查询知识图谱，按照一个系统的解决方案进行。最近的研究建议将复杂的任务分解为多个子目标，并通过利用知识图谱中的必要知识逐一迭代解决每一个。

# 7 前沿主题（Advanced Topics）
## 7.1 长文本建模（Long Context Modeling）
与预训练阶段需要大量数据不同，少量长文本数据就足以用于上下文窗口扩展的持续预训练。
为增强长上下文建模能力，通常采用两种主流方法：扩展位置嵌入和调整上下文窗口。
### 7.1.1 扩展位置嵌入（Scaling Position Embeddings）
- **Direct model fine-tuning**：为了使 LLMs 适应长上下文窗口，一种直接的方法是在目标长度的长文本上对模型进行微调。
- **Position interpolation**：该方法通过缩小原始上下文窗口内的位置索引，避免预训练期间出现超出分布范围的旋转角度。
- **Position truncation**：将较长的相对位置截断以满足最大训练长度的要求。
- **Base modification**：由于 LLMs 通常是在预设的最大训练长度下进行训练的，Position truncation 在某些维度上的波长可能会超出训练长度，对于更长的文本，语言模型可能没有得到充分的训练。通过增加基底的值可以实现基底的减小。此外，减小基底还可以帮助重新调整训练长度以下所有维度的波长，但这通常需要持续的预训练以使 LLMs 适应长上下文窗口。
- **Basis truncation**：与基向量修改类似，基向量的截断处理同样聚焦于解决波长超出训练长度的奇异维度问题。通过基向量截断，可以在较大位置索引处避免出现分布外的旋转角度。然而，这种方法在长上下文任务中表现欠佳。
### 7.1.2 调整上下文窗口
- **Parallel context window**：并行上下文窗口方法采用分治策略处理输入文本：先将文本分割为多个独立编码的片段（共享位置嵌入），在生成阶段通过修改注意力掩码使后续词元能访问同片段的前驱词元。但该方法无法区分不同片段的顺序关系。
- **Λ-shaped context window**：采用"Λ形"注意力掩码机制，该设计选择性地保留初始词元及各查询最近邻的可关注词元，同时舍弃超出此范围的所有词元。
- **Token selection**：从词元级或块级记忆单元中选取最相关的词元用于生成。
    - 词元级选择方法将历史键值存储于外部记忆库，并采用 k 近邻搜索算法检索出 k 个最相关词元供生成使用。
    - 块级选择方法首先将长序列分割成长度相同的块，并将每个块表示为几个关键向量以便检索。
## 7.2 大模型智能体（LLM-empowered Agent）
- 基于 LLM 的智能体包含三大核心组件：记忆、规划与执行。
    - 记忆组件用于存储从环境中感知的信息，并可用于辅助决策。
    - 规划组件负责根据记忆组件的信息生成行动方案。
    - 执行组件负责执行规划组件制定的计划，该计划可以通过内部的 LLM 或外部工具来完成。
- 基于 LLM 的智能体的工作流：
    - 首先，智能体接收环境信息并写入短期记忆；
    - 随后处理短期记忆中的新信息，该过程可通过长期记忆检索的信息增强；
    - 接着规划组件利用短期记忆中的加工信息生成后续计划；
    - 最终执行组件实施规划组件生成的计划，并可借助外部工具辅助完成。
- 根据智能体间的协同策略，多智能体系统可分为两类：合作型与竞争型。在合作模式下，为实现智能体间的信息共享与协同行动，研究者提出了多种通信协议，包括自由对话、结构化文档和数据嵌入。
- 在竞争模式下，辩论是促进发散性思维并获取有价值的外部反馈的流行沟通协议之一。
## 7.3 模型训练的分析与优化（Analysis and Optimization for Model Training）
### 7.3.1 模型内存占用估计
- 模型状态通常占据训练期间的大部分内存，主要包括模型参数、梯度和优化器状态。
- 激活是前向传播过程中需要存储的中间状态，用于反向传播时的梯度计算。
### 7.3.2 内存优化方式
- **Gradient Checkpointing**：梯度检查点（亦称激活重计算）是一种用于优化反向传播期间内存占用的技术。梯度检查点技术在前向传播时仅保留部分激活值，并在反向传播时重新计算这些值以节省内存，但会增加额外的计算开销。
- **ZeRO**：零冗余优化器（ZeRO）技术由 DeepSpeed 库提出，旨在缓解数据并行中的内存冗余问题。ZeRO 技术旨在每个 GPU 上仅保留部分数据，其余数据可在需要时从其他 GPU 获取。ZeRO 提供了三种策略：优化器状态分区（ZeRO-1）、梯度分区（ZeRO-2）以及参数分区（ZeRO-3）。
- **Offload**：梯度与优化器状态将被卸载至CPU内存，仅保留模型参数于 GPU。
### 7.3.3 效率优化方法
- **FlashAttention**：FlashAttention 是一种注意力机制的优化方法，可显著减少注意力计算过程中的内存传输。其核心思想是尽量减少中间结果的存储，直接获取最终结果。
- **Sequence Parallelism**：序列并行可视为预训练中的第四种并行维度，尤其适用于处理长数据序列。其核心思想是将序列分割至多个设备进行并行计算。
## 7.4 模型推理的分析和优化（Analysis and Optimization for Model Inference）
### 7.4.1 推理效率分析
LLM 的推理过程可分为两个阶段进行开销分析：
- 预填充阶段：计算输入序列状态并缓存键值张量；
- 解码阶段：计算新生成词元的状态，更新键值缓存（KV缓存），并以自回归方式持续生成词元直至完成。
### 7.4.2 系统层级优化
- **FlashAttention and Flash-Decoding**：FlashAttention 方法可以应用于预填充阶段，减少了数据传输操作并有效提高了算术强度；Flash-Decoding 将 KV 缓存分割为更小的块，使得查询向量能够与这些块并行计算，从而提升解码效率。
- **PageAttention**：专注于优化 KV 缓存和注意力计算，显著减少了这两个方面的数据传输操作。通过引入内存分页管理方法解决了这一问题，预先分配多块内存用于未来的 KV 缓存，从而大幅减少了拼接过程中的内存分配操作。使用算子融合技术，将查询向量与多个 KV 缓存块的计算并行化，从而提高了计算效率。
- **Batch Management Optimization**：批量管理优化旨在增加解码阶段的批量大小，以提高算术强度。与传统固定长度批处理不同，该技术将每个请求拆分为预填充迭代和若干单步解码迭代，并采用启发式算法动态选择请求进行预填充或单步解码迭代。
### 7.4.3 算法层级优化
- **Speculative Decoding**：使用一个相对较小但更高效的模型（如 ngram 统计模型或小型预训练模型）自回归地生成几个标记。然后，一个更大的模型验证这些标记，确定每个标记是否是每个生成步骤中的最高排名预测。小模型和大模型迭代重复这一过程，直到解码完成。
- **Cascade Inference**：级联推理通过使用不同规模的模型来处理不同难度的请求，从而优化推理效率。
- **Non-autoregressive Decoding**：基于输入一次性生成所有词元。然而，这种方法的生成质量仍然远落后于自回归方法。为了提高生成文本的质量，一些研究尝试结合这两种解码方法，提出了半自回归解码方法，在每个步骤生成一组词元，并使用这些词元作为输入来生成下一组词元。
- **Early Exit**：在模型解码过程中，当满足提前退出的条件时，模型可以直接利用某些层的中间计算结果生成标记，从而提高推理效率。为了确定退出条件，可以将下一个词元生成概率分布的预测置信度或熵作为参考指标。
## 7.5 模型压缩（Model Compression）
### 7.5.1 量化方法
- 模型量化通常有两种主要方法，即量化感知训练（QAT）（需要额外的完整模型再训练）和训练后量化（PTQ）（不需要模型再训练）。
- 在神经网络压缩中，量化通常指的是从浮点数到整数的映射过程。对于神经网络模型，通常有两种需要量化的数据，即权重（模型参数）和激活值（隐藏激活值），它们最初以浮点数表示。
- 简单的公式：$x_q = R(x / S) - Z$ ，将浮点数 $x$ 转换为量化值 $x_q$。在这个函数中，$S$ 和 $Z$ 分别表示缩放因子（包含决定裁剪范围的两个参数 $\alpha$ 和 $\beta$）和零点因子（决定对称或非对称量化），而 $R(\cdot)$ 表示将缩放后的浮点值映射到近似整数的舍入运算。
- **Post-Training Quantization (PTQ)**：
    - Mixed-precision decomposition：当模型大小达到一定参数或以上时，隐藏激活中会出现极大的值（称为离群值的出现）。这些离群值显著影响隐藏激活的数据分布范围，使得进行有效的模型量化变得具有挑战性。为了减少量化误差，一种直接的方法是分别处理离群值和其余权重值。
    - Salient weights protection：对于基于 Transformer的语言模型，通常存在一个对量化更敏感的权重值子集，这些权重值也被称为显著权重。与在推理过程中动态发生并可能需要复杂的运行时处理的激活异常值不同，权重异常值是静态的，可以在模型部署之前进行预处理。
    - Fine-grained quantization：对整个张量使用粗粒度量化参数（即，按张量量化）通常会导致不准确的重建结果。因此，提出了细粒度方法来减少量化误差。
    - Balancing the quantization difficulty：权重比激活值更容易量化。
    - Layerwise quantization：这种方法找到最优的量化权重，以最小化层级重建损失：$\arg \min_{W_c} \|WX - W_cX\|_2^2$ 。
- **Efficient fine-tuning enhanced quantization**：QLoRA 将额外的可调小适配器（16位精度）整合到量化模型中，以实现高效、高精度的模型微调。
- **Quantization-aware training (QAT)**：通过应用无数据蒸馏方法压缩权重、激活以及键值缓存，来达到 QAT 方法的效果。
- INT8权重量化通常可以在 LLMs 上产生非常好的结果，而较低精度权重量化的性能则取决于具体的方法。
- LLM 对低比特权重量化不如小型语言模型敏感。在实践中，在相同的内存成本下，建议使用具有较低量化精度的 LLM，而不是具有较高量化精度的小型语言模型。
- 上下文学习、逐步推理和指令遵循似乎都很少受到4位权重量化的影响。这表明，INT4 量化在总比特数和涌现能力的性能方面都表现出了良好的权衡。
- 激活值比权重更难以量化。
- 高效微调增强量化是提高量化 LLMs 性能的一个很好的选择。
### 7.5.2 其他模型压缩方法

- 蒸馏（Distillation）：模型蒸馏旨在将强大模型（称为教师模型）的能力转移到较弱模型（称为学生模型），从而实现对强大模型的压缩。
- 剪枝（Pruning）：模型剪枝的目标是在尽可能保持模型性能的同时，最小化模型中的参数数量。
    - 结构化剪枝旨在移除对性能影响最小的某些不太重要的模型组件（例如，神经元、通道、层）。
    - 非结构化剪枝主要集中于移除神经网络模型中的单个权重或连接，而不改变模型的主要结构。
## 7.6 检索增强生成（Retrieval-Augmented Generation）
检索增强生成（RAG）技术通过引入外部知识源以提升模型响应质量。
### 7.6.1 基本工作流
- 内容检索：检索步骤主要集中于从现有信息源中找到与当前信息需求相关的上下文信息。
    - 基于词汇的检索使用稀疏向量表示，对文档进行分词，并基于词汇表构建倒排索引，然后使用词汇匹配检索相关文档。
    - 基于语义的检索方法使用密集向量表示，将文档映射到低维密集向量，然后使用近似最近邻搜索算法构建文档向量的有效索引，并根据嵌入的相似性对候选文档进行排序。
- 提示构建：在检索阶段返回相关文档后，需要将这些文档与任务描述一起合并到 LLM 的输入提示中。
    - 由于检索到的文档通常很长，简单地将它们连接到提示中可能会因注意力偏差（如，中间丢失）而导致对提供的上下文利用不佳。
    - 现有方法通常引入重排序模型，从检索结果中选择最相关的文档。
    - 或者可以使用信息提取或文本压缩技术，仅保留文档中的高度相关信息，从而减少输入上下文的长度。
- 回复生成：构建的提示被输入到 LLM 中，使其能够利用检索到的内容更好地完成相应的任务。
    - 检索到的文档可能包含不相关的信息，甚至与正确答案相矛盾的信息，这可能会影响 LLM 生成的输出。
    - 可以进一步提示 LLM 自检生成的输出质量，并决定是否基于新输出重新执行检索。
    - 或者可以执行置信度评估，以确定当前任务是否需要检索或使用检索到的内容。
### 7.6.2 改善策略
- 检索方式改善：
    - 较粗的粒度（如文档级别）可能导致高效检索，但往往会包含大量不相关的信息，而较细的粒度（如句子级别）会增加检索结果中相关内容的比例，但可能导致更高的检索延迟。为了平衡相关性和延迟，现有的研究工作提出使用“命题”作为检索单元，对应于语义完整且相对独立的文本片段，这可以有效减少不相关信息的召回。
    - 查询扩展侧重于向原始查询添加补充信息，例如合并相关实体信息或提供查询中关键信息的详细解释，这有助于加强相关性匹配。
- 检索结果强化：
    - 检索阶段返回的文档可以根据其与输入的关联性进行重新排序，过滤掉低质量或不相关的文档，或将关联性较低的文档放置在提示中的非最佳位置。
    - 可以采用信息提取或自动摘要技术，通过从检索到的文档中提取更简洁且与查询相关的内容，来提炼检索到的内容。
- 迭代检索增强：
    - 迭代检索增强旨在基于模型生成的結果迭代地优化初始查询，以实现所需信息的全面覆盖。
    - 引入了停止机制用于检索迭代，使用 LLM 评估当前生成结果的置信度，以确定是否继续迭代过程。
    - 自适应检索增强进一步增强了 LLM 自主使用检索机制的能力。 LLM 首先需要确定何时使用检索器，然后利用预设提示来启动查询生成和检索结果处理。
- RAG 增强训练：可以设计专门的训练任务，包括指令调优和预训练任务，以进一步增强 LLMs 利用检索内容的能力。
    - 在整理指令数据时，必须考虑两个重要问题：位置偏差和输入上下文中的无关信息。
    - 相关文档可以放置在提示中的不同位置，这可以增强模型对各个位置相关内容的关注，并防止模型忽略某些位置。
    - 可以在指令数据中添加无关信息，以提高模型抵抗此类信息干扰的能力。
## 7.7 幻觉（Hallucination）
### 7.7.1 幻觉定义
- 幻觉指的是 LLMs 生成与事实信息不一致的内容的现象，已经成为一个严重的问题，极大地影响了 LLMs 的任务性能。
- 幻觉被分为内在幻觉和外在幻觉，其中内在幻觉指模型的输出与输入文本不匹配，而外在幻觉指模型的输出无法根据输入进行验证。
- 现有的工作主要集中在开放领域的客观事实幻觉上，即模型生成的内容与现有的世界知识不一致或无法通过现有世界知识验证。
### 7.7.2 幻觉来源
- 训练数据的质量显著影响模型的输出，并且是产生幻觉的主要来源。训练数据的分布在塑造 LLMs 的行为方面也起着关键作用。
- 在自回归训练方法下，模型的注意力分布往往随着序列长度的增加而衰减。这会阻止大型语言模型有效地建模长距离依赖关系，可能导致推理错误或幻觉。
- 教师强制策略在训练阶段和生成阶段之间的差异导致了“暴露偏差”，这可能会导致幻觉问题。
- LLMs 可能会为了获得更高的奖励而迎合人类的回答，这可能会导致答案与事实知识不符。
- 不恰当的提示设计可能导致模型忽略或误解重要信息，从而产生不正确或不相关的内容。不标准的表达或者抽象的概念也会导致幻觉。
- 解码阶段增加多样性也带来了生成幻觉内容的更高可能性。
### 7.7.3 幻觉检测
- 由于强大的语言能力和丰富的世界知识，现有工作广泛采用强大的 LLMs 来检测模型生成的文本中的幻觉。
- LLMs 中幻觉的发生可能与其输出的不确定性有关。
- LLM 可以通过调用外部工具来验证模型生成的内容，从而检测幻觉。
### 7.7.4 幻觉缓解
- 幻觉缓解与人类对齐标准中的诚实标准密切相关，并且可以采用诸如 RLHF 之类的各种对齐方法来缓解模型幻觉。
- 为 LLM 提供高度可靠的外部知识作为上下文，有助于减少幻觉。
- 可以利用 LLM 自身的内部状态或知识来减少幻觉。一些工作引入了外部知识源来辅助解码过程。