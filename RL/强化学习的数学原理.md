# 1 基本概念
## 1.1 概念
状态、状态空间；动作、动作空间；
状态转移：当采取一个动作，智能体从一个状态转移到另一个状态。
策略：智能体在一个状态应该采取什么动作。

奖励：智能体采取动作后获得的一个实数值，正为鼓励，负为惩罚。
轨迹：一条轨迹是一个状态-动作-奖励链。

## 1.2 马尔可夫决策过程
马尔科夫决策过程（MDP）：
- 状态空间：$S$
- 动作空间：$A(s),\; s \in S$
- 奖励：$R(s, a),\; s \in S,\; a \in A(s)$
- 状态转移概率：$p(s'\;| s,\; a),\; s \in S,\; a \in A(s)$
- 策略：$\pi (a\;|s),\; s \in S,\; a \in A(s)$
马尔可夫性质：未来状态的条件转移概率只依赖于当前的状态和动作，与过去的状态和动作是独立的。

# 2 贝尔曼方程
## 2.1 确定性问题的贝尔曼方程
确定性问题的贝尔曼公式：
$$
\boldsymbol{v} = \boldsymbol{r} + \gamma \boldsymbol{P}\boldsymbol{v}
$$
其中 $\boldsymbol{r}$ 为奖励，$\boldsymbol{P}$ 为状态转移矩阵，$\boldsymbol{v}$ 为状态价值。
表明一个状态的价值依赖于其他状态的价值。

## 2.2 随机性问题的折扣回报
对于随机性问题：
$$
S_t \xrightarrow{A_t} R_{t + 1},\; S_{t + 1}
$$
- $S_t, A_t, R_{t + 1}$ 均为随机变量。
- $S_t \to A_t$ 依赖于 $\pi (A_t = a | S_t = s)$，即策略函数。
- $S_t, A_t \to R_{t + 1}$ 依赖于 $p(R_{t + 1} = r| S_t=s, A_t = a)$。
- $S_t,A_t \to S_{t + 1}$ 依赖于 $p(S_{t + 1}=s'|S_t = s, A_t = a)$。

考虑以下多步轨迹：

$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \ldots$$
折扣回报定义为：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

其中 $\gamma \in [0, 1)$ 是折扣率，$G_t$ 也是一个随机变量，因为 $R_{t+1}, R_{t+2}, \ldots$ 都是随机变量。
折扣回报通过折扣因子 $\gamma$ 降低了未来奖励的权重，使得智能体更关注近期奖励。较小的 $\gamma$ 值会使智能体更加"近视"，而接近1的 $\gamma$ 值则使智能体更加"远视"。

## 2.3 状态价值函数
状态价值函数（或简称状态价值）是折扣回报 $G_t$ 的期望值（也称为期望价值或均值），定义为：

$$v_{\pi}(s) = \mathbb{E}[G_t|S_t = s]$$

- 状态价值是关于状态 $s$ 的函数，表示在给定起始状态 $s$ 的条件下的条件期望。
- 状态价值基于策略 $\pi$。不同的策略可能导致不同的状态价值。
- 状态价值代表一个状态的"价值"。状态价值越大，说明该策略越好，因为能获得更高的累积奖励。

## 2.4 推导贝尔曼方程

考虑一个随机轨迹：
$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \dots$$

回报 $G_t$ 可以写成：
$$
\begin{aligned}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\ 
    & = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
    & = R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$
根据状态价值函数的定义，有：
$$
\begin{aligned}
v_\pi(s) & = \mathbb{E}_\pi [G_t | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
\end{aligned}
$$

接下来，分别计算这两个期望。
首先，计算第一项 $\mathbb{E}_\pi [R_{t+1} | S_t = s]$（这是即时奖励的均值）： 
$$ \begin{aligned} \mathbb{E}_\pi [R_{t+1} | S_t = s] &= \sum_a \pi(a|s) \mathbb{E} [R_{t+1} | S_t = s, A_t = a] \\ &= \sum_a \pi(a|s) \sum_{r} p(r|s, a) r \end{aligned} $$
接下来，计算第二项 $\mathbb{E}_\pi [G_{t+1} | S_t = s]$（这是未来回报的均值）：
$$
\begin{aligned}
\mathbb{E}_\pi [G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_t = s, S_{t+1} = s'] p(s'|s) \\
&= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s'] p(s'|s) \text{（由于马尔可夫性质）}\\
&= \sum_{s'} v_\pi(s') p(s'|s) \\
&= \sum_{s'} v_\pi(s') \sum_a p(s'|s, a) \pi(a|s)
\end{aligned}
$$
因此，我们得到：
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s] \\
&= \underbrace{\sum_a \pi(a|s) \sum_r p(r|s, a) r}_{\text{即时奖励的均值}} + \gamma \underbrace{\sum_a \pi(a|s) \sum_{s'} p(s'|s, a) v_\pi(s')}_{\text{未来奖励的均值}} \\
&= \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right], \quad \forall s \in \mathcal{S}
\end{aligned}
$$
- $v_\pi(s)$ 和 $v_\pi(s')$ 是需要计算的状态价值。
- $\pi(a|s)$ 是给定的策略。求解这个方程被称为**策略评估 (policy evaluation)**。
- $p(r|s, a)$ 和 $p(s'|s, a)$ 代表环境。

上述方程被称为**贝尔曼方程**，描述了不同状态的状态价值函数之间的关系，由两项组成：即时奖励项和未来奖励项。这是一个方程组：每个状态都有一个这样的方程。

## 2.5 贝尔曼方程的向量形式
假设状态可以被索引为 $s_i$ ($i = 1, \dots, n$)。对于状态 $s_i$，贝尔曼方程是： $$v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)$$将所有状态的这些方程放在一起，并改写为矩阵向量形式： $$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$其中： 
- $\mathbf{v}_\pi = [v_\pi(s_1), \dots, v_\pi(s_n)]^T \in \mathbb{R}^n$ 是价值向量
- $\mathbf{r}_\pi = [r_\pi(s_1), \dots, r_\pi(s_n)]^T \in \mathbb{R}^n$ 是奖励向量，$r_\pi(s) = \sum_a \pi(a|s) \sum_r p(r|s, a) r$
- $\mathbf{P}_\pi \in \mathbb{R}^{n \times n}$ 是状态转移矩阵，$[\mathbf{P}_\pi]_{ij} = p_\pi(s_j|s_i) = \sum_a \pi(a|s_i) p(s_j|s_i, a)$

## 2.6 求解状态价值

贝尔曼方程的矩阵向量形式是：
$$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$

闭式解 (The closed-form solution) 是：
$$ \mathbf{v}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi $$
在实践中，我们仍然需要使用数值工具来计算矩阵的逆。

通过迭代算法可以避免矩阵求逆运算，一个迭代解 (An iterative solution) 是：
$$ \mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k $$
这个算法产生一个序列 $\{\mathbf{v}_0, \mathbf{v}_1, \mathbf{v}_2, \dots\}$。可以证明：
$$ \mathbf{v}_k \rightarrow (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi, \quad k \rightarrow \infty $$

**证明：**
定义误差为 $\delta_k = \mathbf{v}_k - \mathbf{v}_\pi$。我们只需要证明当 $k \rightarrow \infty$ 时，$\delta_k \rightarrow \mathbf{0}$。

将 $\mathbf{v}_{k+1} = \delta_{k+1} + \mathbf{v}_\pi$ 和 $\mathbf{v}_k = \delta_k + \mathbf{v}_\pi$ 代入迭代公式 $\mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k$，得到：
$$
\begin{aligned}
\delta_{k+1} + \mathbf{v}_\pi 
&= \mathbf{r}_\pi + \gamma \mathbf{P}_\pi (\delta_k + \mathbf{v}_\pi) \\
& = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \delta_k + \gamma \mathbf{P}_\pi \mathbf{v}_\pi 
\end{aligned}
$$
由于 $\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi$，上式可以改写为：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k $$
因此，我们可以得到：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k = \gamma^2 \mathbf{P}_\pi^2 \delta_{k-1} = \dots = \gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 $$
注意 $0 \le [\mathbf{P}_\pi]_{ij} \le 1$，这表示 $\mathbf{P}_\pi$ 的每一行的元素都是非负的且和为 1。这是因为 $\mathbf{P}_\pi$ 是状态转移矩阵。因此，$\mathbf{P}_\pi \mathbf{1} = \mathbf{1}$，其中 $\mathbf{1} = [1, 1, \dots, 1]^T$。
另一方面，由于 $\gamma < 1$，我们知道 $\gamma^k \rightarrow 0$，因此 $\gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 \rightarrow \mathbf{0}$ 当 $k \rightarrow \infty$ 时。
因此，$\delta_k \rightarrow \mathbf{0}$，这意味着 $\mathbf{v}_k \rightarrow \mathbf{v}_\pi$ 当 $k \rightarrow \infty$ 时。

## 2.7 动作价值函数
状态价值：从一个状态开始，智能体能够获得的平均回报。
动作价值：从一个状态开始并采取一个动作，智能体能够获得的平均回报。

定义：
$$ q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
* $q_\pi(s, a)$ 是状态-动作对 $(s, a)$ 的函数。
* $q_\pi(s, a)$ 依赖于策略 $\pi$。

根据条件期望的性质，有：
$$
\underbrace{\mathbb{E}[G_t | S_t = s]}_{v_\pi(s)} = \sum_a \underbrace{\mathbb{E}[G_t | S_t = s, A_t = a]}_{q_\pi(s,a)} \pi(a|s)
$$
因此，
$$ v_\pi(s) = \sum_a \pi(a|s) q_\pi(s, a) \quad (1) $$
回顾状态价值函数由下式给出：
$$ v_\pi(s) = \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right] \quad (2) $$

通过比较 (1) 和 (2)，我们得到**动作价值函数**为：
$$ q_\pi(s, a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \quad (3) $$
(1) 和 (3) 是同一枚硬币的两面：
* (1) 展示了如何从动作价值函数获得状态价值函数。
* (3) 展示了如何从状态价值函数获得动作价值函数。

# 3 贝尔曼最优方程
## 3.1 最优策略
状态值函数可以用来评估一个策略的好坏：如果对于所有状态 $s \in \mathcal{S}$，都有 
$$
v_{\pi_1}(s) \ge v_{\pi_2}(s),
$$
那么我们认为策略 $\pi_1$ 比策略 $\pi_2$ “更好”。 

一个策略 $\pi^*$ 被称为**最优策略**，如果对于所有状态 $s \in \mathcal{S}$ 和任何其他策略 $\pi$，都有
$$
v_{\pi^*}(s) \ge v_{\pi}(s).
$$

## 3.2 贝尔曼最优方程
贝尔曼最优方程（元素形式）：
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
也可以写成：
$$
v_*(s) = \max_{a} q_*(s, a), \quad s \in \mathcal{S}
$$
其中，
$$
q_*(s, a) = \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right)
$$
是最优状态-动作值函数。
* $p(r|s, a)$ 和 $p(s'|s, a)$ 是已知的。
* $v_*(s)$ 和 $v_*(s')$ 是未知的，需要计算。
* 在最优性方程中，我们正在寻找最优策略，通常情况下 $\pi(a|s)$ 是我们想要确定的。

贝尔曼最优性方程（矩阵-向量形式）：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
其中，对应于状态 $s$ 或 $s'$ 的元素是：
$$
[\mathbf{r}_{\boldsymbol{\pi}}]_s \triangleq \sum_a \pi(a|s) \sum_{s'} p(s'|s, a) r(s, a, s')
$$
$$
[\mathbf{P}_{\boldsymbol{\pi}}]_{s, s'} \triangleq \sum_a \pi(a|s) p(s'|s, a)
$$
这里的 $\max_{\boldsymbol{\pi}}$ 是**按元素**取最大值。

对于贝尔曼最优方程：
* **算法 (Algorithm):** 如何求解这个方程？
* **存在性 (Existence):** 这个方程是否有解？
* **唯一性 (Uniqueness):** 这个方程的解是唯一的吗？
* **最优性 (Optimality):** 它与最优策略是如何相关的？

## 3.3 贝尔曼最优方程右侧的最大化
贝尔曼最优方程: 元素形式
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
贝尔曼最优方程: 矩阵-向量形式
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
首先固定 $v'(s)$ 并求解 $\pi$：
$$
\begin{aligned}
v(s) 
&= \max_{\pi} \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v'(s')) \right) \\
&= \max_{\pi} \sum_a \pi(a|s) q(s, a), \quad \forall s \in \mathcal{S}
\end{aligned}
$$
考虑到 $\sum_a \pi(a|s) = 1$，我们有：
$$
\max_{\pi} \sum_a \pi(a|s) q(s, a) = \max_{a \in \mathcal{A}(s)} q(s, a)
$$
最优性在以下情况下取得：
$$
\pi(a|s) =
\begin{cases}
1 & \text{如果 } a = a^* \\
0 & \text{如果 } a \neq a^*
\end{cases}
$$
其中 $a^* = \arg \max_a q(s, a)$。

## 3.4 求解贝尔曼最优性方程

贝尔曼最优性方程是：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*).
$$
令：

$$
f(\mathbf{v}) := \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
那么，贝尔曼最优性方程可以写成：
$$
\mathbf{v}_* = f(\mathbf{v}_*)
$$
其中，$f(\mathbf{v})$ 的第 $s$ 个元素是：
$$
[f(\mathbf{v})]_s = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v(s')) = \max_a q(s, a)
$$
### 3.4.1 预备知识：压缩映射定理 (Contraction mapping theorem)
#### 3.4.1.1 一些概念
**不动点 (Fixed point):** $x \in \mathcal{X}$ 是函数 $f: \mathcal{X} \rightarrow \mathcal{X}$ 的一个不动点，如果：$$f(x) = x$$

**压缩映射 (Contraction mapping or contractive function):** 函数 $f$ 是一个压缩映射，如果存在一个常数 $\gamma \in [0, 1)$，使得对于所有 $x_1, x_2 \in \mathcal{X}$，都有：$$\|f(x_1) - f(x_2)\| \le \gamma \|x_1 - x_2\|$$其中：
- $\gamma \in [0, 1)$。
- $\gamma$ 必须严格小于 1，这样才能保证许多极限成立，例如当 $k \rightarrow \infty$ 时，$\gamma^k \rightarrow 0$。
- $\|\cdot\|$ 可以是任何向量范数。

#### 3.4.1.2 压缩映射定理
对于任何形式为 $x = f(x)$ 的方程，如果 $f$ 是一个压缩映射，那么：
* **存在性 (Existence):** 存在一个不动点 $x^*$ 满足 $f(x^*) = x^*$。
* **唯一性 (Uniqueness):** 这个不动点 $x^*$ 是唯一的。
* **算法 (Algorithm):** 考虑一个序列 $\{x_k\}$，其中 $x_{k+1} = f(x_k)$，那么 $x_k \rightarrow x^*$ 当 $k \rightarrow \infty$。此外，收敛速度是指数级的。

#### 3.1.4.3 压缩映射定理的证明
*第一部分：我们证明序列 $\{x_k\}_{k=1}^\infty$，其中 $x_k = f(x_{k-1})$ 是收敛的。*
这个证明依赖于[[Concepts#3 柯西序列（Cauchy sequence）|柯西序列（Cauchy sequence）]]。
一个序列 $x_1, x_2, ... \in \mathbb{R}$ 被称为柯西序列，如果对于任意小的 $\epsilon > 0$，存在一个正整数 $N$ 使得对于所有的 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。
直观的理解是，存在一个有限的整数 $N$，使得所有在 $N$ 之后的元素都充分地靠近彼此。
柯西序列很重要，因为可以保证一个柯西序列收敛到一个极限。它的收敛性质将被用于证明压缩映射定理。
注意，我们必须有 $||x_m - x_n|| < \epsilon$ 对于所有 $m, n > N$。如果我们仅仅有 $x_{n+1} - x_n \to 0$，不足以声明该序列是一个柯西序列。例如，对于 $x_n = \sqrt{n}$，有 $x_{n+1} - x_n \to 0$，但是显然 $x_n = \sqrt{n}$ 是发散的。
我们接下来证明 $\{x_k = f(x_{k-1})\}_{k=1}^\infty$ 是一个柯西序列，因此是收敛的。
首先，由于 $f$ 是一个压缩映射，我们有
$$||x_{k+1} - x_k|| = ||f(x_k) - f(x_{k-1})|| \le \gamma ||x_k - x_{k-1}||.$$
类似地，我们有 $||x_k - x_{k-1}|| \le \gamma ||x_{k-1} - x_{k-2}||, ..., ||x_2 - x_1|| \le \gamma ||x_1 - x_0||$。
因此，我们有
$$
\begin{align*} 
||x_{k+1} - x_k|| 
&\le \gamma ||x_k - x_{k-1}|| \\ 
&\le \gamma^2 ||x_{k-1} - x_{k-2}|| \\ 
&\vdots \\ 
&\le \gamma^k ||x_1 - x_0||. 
\end{align*}
$$
由于 $\gamma < 1$，我们知道 $||x_{k+1} - x_k||$ 以指数速度收敛到零，当 $k \to \infty$ 时，给定任意的 $x_1, x_0$。特别地，$\{|x_{k+1} - x_k|\}$ 的收敛性不足以推出 $\{x_k\}$ 的收敛性。因此，我们需要进一步考虑对于任意 $m > n$ 的 $||x_m - x_n||$。特别地，
$$
\begin{align*} 
||x_m - x_n|| 
&= ||x_m - x_{m-1} + x_{m-1} - ... - x_{n+1} + x_{n+1} - x_n|| \\ 
&\le ||x_m - x_{m-1}|| + ||x_{m-1} - x_{m-2}|| + ... + ||x_{n+1} - x_n|| \\ 
&\le \gamma^{m-1} ||x_1 - x_0|| + ... + \gamma^n ||x_1 - x_0|| \\ 
&= \gamma^n (\gamma^{m-n-1} + ... + 1) ||x_1 - x_0|| \\ 
&= \gamma^n (1 + \gamma + ... + \gamma^{m-n-1}) ||x_1 - x_0|| \\ 
&= \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.
\end{align*}
$$
因此，对于任意 $\epsilon$，我们总能找到一个 $N$ 使得对于所有 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。因此，这个序列是柯西序列，并且收敛到一个极限点，记为 $x^* = \lim_{k \to \infty} x_k$。

*第二部分：我们证明极限 $x^* = \lim_{k \to \infty} x_k$ 是一个不动点。*
为了做到这一点，由于
$$||f(x_k) - x_k|| = ||x_{k+1} - x_k|| \le \gamma^k ||x_1 - x_0||,$$
我们知道 $||f(x_k) - x_k||$ 以指数速度收敛到零。因此，在极限情况下，我们有 $f(x^*) = x^*$。

*第三部分：我们证明不动点是唯一的。*
假设存在另一个不动点 $x'$ 满足 $f(x') = x'$。那么，
$$||x' - x^*|| = ||f(x') - f(x^*)|| \le \gamma ||x' - x^*||.$$
由于 $\gamma < 1$，这个不等式成立当且仅当 $||x' - x^*|| = 0$。因此，$x' = x^*$。

*第四部分：我们证明 $x_k$ 以指数速度收敛到 $x^*$。*
回想一下，正如上述证明的，$||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||$。由于 $m$ 可以任意大，我们有
$$||x^* - x_n|| = \lim_{m \to \infty} ||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.$$
由于 $\gamma < 1$，当 $n \to \infty$ 时，误差以指数速度收敛到零。

### 3.4.2 贝尔曼最优性方程的压缩性质
让我们回到贝尔曼最优性方程：
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
**定理 (压缩性质 - Contraction Property)**
$f(\mathbf{v})$ 是一个压缩映射，满足：
$$
\|f(\mathbf{v}_1) - f(\mathbf{v}_2)\| \le \gamma \|\mathbf{v}_1 - \mathbf{v}_2\|
$$
其中 $\gamma$ 是折扣率（discount rate），且 $\gamma \in [0, 1)$。

**证明：**
考虑任意两个向量 $v_1, v_2 \in \mathbb{R}^{|S|}$，
假设 $\pi_1^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_1)$ 并且 $\pi_2^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_2)$。那么，
$$
\begin{aligned}
f(v_1) &= \max_{\pi} (r_\pi + \gamma P_\pi v_1) = r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 \ge r_{\pi_2^*} + \gamma P_{\pi_2^*} v_1, \\
f(v_2) &= \max_{\pi} (r_\pi + \gamma P_\pi v_2) = r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2 \ge r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2,
\end{aligned}
$$
其中 $\ge$ 是按元素比较。因此，
$$
\begin{aligned}
f(v_1) - f(v_2) &= r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2) \\
&\le r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2) \\
&= \gamma P_{\pi_1^*} (v_1 - v_2).
\end{aligned}
$$
类似地，可以证明 $f(v_2) - f(v_1) \le \gamma P_{\pi_2^*} (v_2 - v_1)$。因此，
$$
\gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2).
$$
定义
$$
z \doteq \max \{|\gamma P_{\pi_2^*} (v_1 - v_2)|, |\gamma P_{\pi_1^*} (v_1 - v_2)|\} \in \mathbb{R}^{|S|},
$$
其中 $\max\{\cdot, \cdot\}$，$|\cdot|$，以及 $\ge$ 都是按元素操作。根据定义，$z \ge 0$。一方面，很容易看出
$$
-z \le \gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2) \le z,
$$
这蕴含着
$$
|f(v_1) - f(v_2)| \le z.
$$
由此可得
$$
\|f(v_1) - f(v_2)\|_\infty \le \|z\|_\infty. \quad (*)
$$
其中 $\|\cdot\|_\infty$ 是最大范数。另一方面，假设 $z_i$ 是 $z$ 的第 $i$ 个元素，并且 $p_i^T$ 和 $q_i^T$ 分别是 $P_{\pi_1^*}$ 和 $P_{\pi_2^*}$ 的第 $i$ 行。那么，
$$
z_i = \max \{|\gamma p_i^T (v_1 - v_2)|, |\gamma q_i^T (v_1 - v_2)|\}.
$$
由于 $p_i$ 是一个所有元素非负且元素之和等于 1 的向量，因此有
$$
|p_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty.
$$
类似地，我们有 $|q_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty$。因此，$z_i \le \gamma \|v_1 - v_2\|_\infty$，从而
$$
\|z\|_\infty = \max_i |z_i| \le \gamma \|v_1 - v_2\|_\infty.
$$
将此不等式代入 (\*) 得到
$$
\|f(v_1) - f(v_2)\|_\infty \le \gamma \|v_1 - v_2\|_\infty,
$$
这证明了 $f(v)$ 的压缩性质。

应用压缩映射定理可以得到以下结果：
**定理 (存在性、唯一性和算法 - Existence, Uniqueness, and Algorithm)**
对于贝尔曼最优性方程
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*),
$$
总是存在一个解 $\mathbf{v}_*$ 并且解是唯一的。这个解可以通过迭代的方式求解：
$$
\mathbf{v}_{k+1} = f(\mathbf{v}_k) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_k)
$$
这个序列 $\{\mathbf{v}_k\}$ 以指数级的速度收敛到 $\mathbf{v}_*$，给定任何初始猜测 $\mathbf{v}_0$。收敛速度由 $\gamma$ 决定。


## 3.5 策略最优性
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程的解。它满足：
$$
\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
假设存在一个策略 $\boldsymbol{\pi}^*$，对于所有状态 $s$，它都能达到上述最大值，即：
$$
\boldsymbol{\pi}^* = \arg \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
那么，对于这个特定的策略 $\boldsymbol{\pi}^*$，我们有：
$$
\mathbf{v}^* = \mathbf{r}_{\boldsymbol{\pi}^*} + \gamma \mathbf{P}_{\boldsymbol{\pi}^*} \mathbf{v}^*
$$
因此，$\boldsymbol{\pi}^*$ 是一个策略，且 $\mathbf{v}^*$ 是其对应的状态值函数。

**问题：$\boldsymbol{\pi}^*$ 是最优策略吗？$\mathbf{v}^*$ 是可以达到的最大的状态值吗？**

**定理 (策略最优性 - Policy Optimality Theorem)**
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程 
$$
\mathbf{v} = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
的唯一解，并且对于任意给定的策略 $\boldsymbol{\pi}$，其状态值函数 $\mathbf{v}_{\boldsymbol{\pi}}$ 满足贝尔曼期望方程 $\mathbf{v}_{\boldsymbol{\pi}} = \mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_{\boldsymbol{\pi}}$，那么：
$$
\mathbf{v}^* \ge \mathbf{v}_{\boldsymbol{\pi}}, \quad \forall \boldsymbol{\pi}
$$
**证明：**
对于任意策略 $\pi$，有
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi.
$$
由于
$$
v^* = \max_{\pi} (r_\pi + \gamma P_\pi v^*) = r_{\pi^*} + \gamma P_{\pi^*} v^* \ge r_\pi + \gamma P_\pi v^*,
$$
我们有
$$
v^* - v_\pi \ge (r_\pi + \gamma P_\pi v^*) - (r_\pi + \gamma P_\pi v_\pi) = \gamma P_\pi (v^* - v_\pi).
$$
重复应用上述不等式得到 $v^* - v_\pi \ge \gamma P_\pi (v^* - v_\pi) \ge \gamma^2 P_\pi^2 (v^* - v_\pi) \ge \cdots \ge \gamma^n P_\pi^n (v^* - v_\pi)$。由此可知
$$
v^* - v_\pi \ge \lim_{n \to \infty} \gamma^n P_\pi^n (v^* - v_\pi) = 0,
$$
其中最后一个等式成立是因为 $\gamma < 1$ 且 $P_\pi$ 是一个非负矩阵，其所有元素小于等于 1（因为 $P_\pi \mathbf{1} = \mathbf{1}$）。因此，$v^* \ge v_\pi$ 对于任意 $\pi$ 成立。


**定理 (贪婪最优策略 - Greedy Optimal Policy Theorem)**
对于任何状态 $s \in \mathcal{S}$，确定性的贪婪策略 $\boldsymbol{\pi}^*$ 定义为：
$$
\pi^*(a|s) =
\begin{cases}
1 & \text{if } a = a^*(s) \\
0 & \text{if  } a \neq a^*(s)
\end{cases} \quad (1)
$$
是求解贝尔曼最优性方程的一个最优策略。这里，$a^*(s)$ 定义为：
$$
a^*(s) = \arg \max_a q^*(s, a)
$$
其中最优动作值函数 $q^*(s, a)$ 定义为：
$$
q^*(s, a) := \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$

**证明：**
简单来说，
$$
\begin{aligned}
\pi^*(s) 
&= \arg \max_a \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s')) \right) \\
&= \arg \max_a q^*(s, a)
\end{aligned}
$$

## 3.6 分析最优策略
哪些因素决定了最优策略？
从贝尔曼最优性方程中可以清晰地看出：

$$
v^*(s) = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$
有三个关键因素：
* **奖励设计 (Reward design):** $r(s, a, s')$
* **系统模型 (System model):** $p(s'|s, a)$
* **折扣率 (Discount rate):** $\gamma$
需要计算的未知量是：$v^*(s), v^*(s'), \pi^*(a|s)$。

**定理 (最优策略不变性 - Optimal Policy Invariance)**
考虑一个马尔可夫决策过程，其最优状态值函数为 $\mathbf{v}^* \in \mathbb{R}^{|\mathcal{S}|}$，满足 $\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)$。如果每个奖励 $r$ 都经过一个仿射变换 $ar + b$，其中 $a, b \in \mathbb{R}$ 且 $a > 0$，那么对应的最优状态值函数 $\mathbf{v}'^*$ 也是 $\mathbf{v}^*$ 的一个仿射变换：
$$
\mathbf{v}'^* = a\mathbf{v}^* + \frac{b}{1 - \gamma} \mathbf{1}
$$
其中 $\gamma \in [0, 1)$ 是折扣率，$\mathbf{1} = [1, \dots, 1]^T$ 是一个所有元素都为 1 的向量。
因此，最优策略对于奖励信号的仿射变换是不变的。

**证明：**
对于任意策略 $\pi$，定义 $r_\pi = [..., r_\pi(s), ...]^T$，其中
$$
r_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s,a)r, \quad s \in \mathcal{S}.
$$
如果 $r \rightarrow \alpha r + \beta$，那么 $r_\pi(s) \rightarrow \alpha r_\pi(s) + \beta$，因此 $r_\pi \rightarrow \alpha r_\pi + \beta \mathbf{1}$，其中 $\mathbf{1} = [..., 1, ...]^T$。在这种情况下，BOE 变为
$$
v' = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi v'). \quad (*)
$$
我们通过证明 $v' = \alpha v^* + c\mathbf{1}$ 是新 BOE (\*) 的解来求解它，其中 $c = \beta/(1-\gamma)$。特别地，将 $v' = \alpha v^* + c\mathbf{1}$ 代入 (\*) 得到
$$
\alpha v^* + c\mathbf{1} = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi (\alpha v^* + c\mathbf{1})) = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \alpha \gamma P_\pi v^* + \gamma c P_\pi \mathbf{1}),
$$
其中最后一个等式成立是因为 $P_\pi \mathbf{1} = \mathbf{1}$。上述方程可以重新整理为
$$
\alpha v^* = \max_{\pi \in \Pi} (\alpha r_\pi + \alpha \gamma P_\pi v^*) + \beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1},
$$
这等价于
$$
\beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1} = 0.
$$
由于 $c = \beta/(1-\gamma)$，上述方程成立，因此 $v' = \alpha v^* + c\mathbf{1}$ 是 (\*) 的解。由于 (\*) 是 BOE，$v'$ 也是唯一的解。最后，由于 $v'$ 是 $v^*$ 的仿射变换，动作值之间的相对关系保持不变。因此，从 $v'$ 导出的贪婪最优策略与从 $v^*$ 导出的相同：$\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v')$ 与 $\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*)$ 相同。