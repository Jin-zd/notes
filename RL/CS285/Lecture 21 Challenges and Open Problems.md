## 1 Brief Review
我们首先简单回顾一下过去介绍的内容:
- 从根本上来说，我们之前介绍的是基于学习的控制方法，这包括模仿学习（从演示中学习）和强化学习（从奖励中学习）；
- 之后我们介绍了强化学习中的一 “经典”无模型强化学习算法，例如策略梯度和基于值的方法以及介于它们之间的演 -评论家算法。同时我们也介绍了这几类算法的对应例子，例如信赖域策略优化（TRPO）和近端策略优化（PPO）、深度 Q 网络（DQN）以及软演员 - 评论家（SAC）；
- 之后我们介绍了另一类可能的算法：基于模型的控制（例如线性二次调节器，LQR），这些方法未必是基于学习的方法，但它们也可和基于学习的方法结合，得到无策略的基于模型的强化学习（例如模型预测控制，MPC）。而这样的方式也可也和强化学习的策略学习结合，得到有策略的基于模型的强化学习（例如动态规划 - Q，Dyna - Q）；
- 我们剩下讨论的一些内容在某种意义上正交于我们之前讨论的内容，例如探索、无监督强化学习（如技能发现）。我们还介绍了一些工具例如概率推断，以及利用其建立的 “控制即推断” 的框架，在这基础上我们可以得到逆强化学习的算法；
- 我们同样还介绍了离线强化学习、基于序列模型的强化学习、元学习等等其他内容。
![](21-1.png)

## 2 Challenges in Deep Reinforcement Learning
核心算法面临的挑战：​ 
- 稳定性：算法是否收敛？ ​
- 效率：收敛需要多少时间 / 样本？
- 泛化性：在收敛后，其泛化性怎么样？

假设方面的挑战：
- 问题的构建是否合理，能否将该问题建模为强化学习问题？是否存在更好的假设？
- 监督的来源是什么？例如，在模仿学习中，监督来自示范；而在强化学习中，监督则来自奖励。

接下来我们从以上提到的一些角度进行讨论。
### 2.1 Stability and hyperparameter tuning
对于强化学习而言，我们需要自行获取数据，而非使用[[Concepts#1 独立同分布（i.i.d.）|独立同分布（i.i.d.）]]的数据，同时我们的目标是优化一个目标函数，而非简单地学习真实标签。这些额外的调整意味着强化学习对一系列超参数更为敏感，这给设计稳定的强化学习算法带来了挑战。

基于价值的方法：
通常无法保证基于 Q 学习估计的算法收敛，同时我们需要大量超参数来实现稳定性，例如目标网络延迟、经验回放缓冲区大小、裁剪、学习率等。当然，深度学习中的许多技巧（如更大规模的网络、归一化、数据增强，参见 Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels, Ilya Kostrikov, Denis Yarats, Rob Fergus）都可以使训练更加稳定。
一些开放性问题：尽管基于经典机器学习理论会认为深度学习会出现严重的过拟合，但实际上深度学习通常能够表现得很好，这说明存在某种潜在的正则化效应，例如随机梯度下降本身。然而，基于价值的方法事实上并非梯度下降，因此一个可能的研究方向是探究这种“魔力”是否同样存在于基于值的方法中。

策略梯度方法：
对于策略梯度相关的算法，我们通常有更深入的研究，也有一些收敛性的保证。但代价是我们有更高的方差。其他强化学习算法通常存在来自值函数的偏差，而策略则具有很大的方差但没有偏差。这意味着我们需要大量样本进行训练，同时我们需要考虑的超参数有学习率、批量大小和基线设计。

基于模型的强化学习：
这一类算法，表面上看似乎是一个稳定的选择，因为模型的学习过程属于监督学习。然而，在这类方法中，模型在训练过程中会不断变化。这里的问题在于，模型变得准确并不能直接使策略变得更好：尽管如果我们的模型是完美的，此时能够得到最优策略，但模型变好的情况并非如此。模型有可能以某种特定方式改进，例如可能在某些地方的准确性略有下降，而这会给策略带来灾难性的后果；或者说，模型的不同误差虽然在训练模型的监督学习意义上是相同的，但对策略的影响并不一致。
在[[Lecture 10 Model-Based Policy Learning]]一节中，我们介绍了使用反向传播通过时间的方法来更新策略，但这种方法效果并不好，因此我们通常还是会使用无模型的算法，将模型作为一种加速的方式。

基于模型的方法还存在一些其他问题，例如策略可能会利用模型的一些错误，从这个意义上来说，这种方法类似于一种对抗性过程。

### 2.2 The challenge with sample complexity
不同的算法在样本效率上有很大差异
- 无梯度方法（例如 NES、CMA 等）；
- 完全在线方法（例如 A3C），这类方法大约需要 $10^8$ 个步骤，实际中大约需要 $15$ 天。
- 策略梯度方法（例如 TRPO），这类方法通常需要 $10^7$ 个步骤，实际中大约需要 $1.5$ 天。
- 重放缓冲区值估计方法（Q-learning、DDPG、NAF、SAC 等），这类方法大约需要 $10^6$ 个步骤，对于一些简单任务可能只需几小时。
- 基于模型的深度强化学习方法（PETS、引导策略搜索）
- 基于模型的“浅层”强化学习方法（例如 PILCO），但使用的是无法扩展到大规模场景的方法，如高斯过程。
![](21-2.png)

直观来说，样本效率自然是越高越好，那么为什么我们还会选择使用那些效率没那么高的算法呢？

很多时候我们可以并行收集数据，例如对于机器人任务，我们可以使用多个机械臂，模拟环境中更是如此。在某些情况下，收集数据的成本可能低于训练模型的成本。但是，在真实世界学习以及一些特定问题（如 AI for Science，AI4S）中，收集数据的成本可能很高，此时算法的样本效率就变得至关重要。

尽管总体而言，目前各种算法的采样效率也在不断提升，但当我们尝试解决范围更广的问题时，这会成为一个更为严峻的问题。

### 2.3 Scaling up deep RL & generalization
我们会发现一个很有意思的现象：
- 在目前的深度学习中，我们会在大规模的数据上训练，并且通常强调任务的多样性，使用泛化能力作为评估指标；
- 在强化学习（RL）中，通常我们只会在一个小规模的数据上训练，且我们主要关注单任务上的效果，也只会用性能来评估。

为什么我们不能像一般的深度学习那样简单地扩大强化学习的规模呢？这实际上与强化学习本身的工作流程密切相关：
- 对于监督学习来说，通常只需从真实世界中获取一次数据，之后通过算法 / 模型进行训练即可。如果对结果不满意，无论是调整超参数还是修改模型和算法，只需在数据集上重新训练即可；
![](21-3.png)
- 对于典型的强化学习来说，数据来源于与环境的反复交互，通常我们每一次调整算法都需要重复这一漫长的过程，且现实中还有一个外循环是人，如果扩大强化学习的规模则会让整个过程更加不现实。
![](21-4.png)

因此，对强化学习的改进不能仅仅局限于具体的方法，还可以从工作流程的角度让整个强化学习更具可行性。本课程介绍了许多相关研究方向：
- 离线强化学习：如果我们能从预先收集好的数据集中学习，那么整个强化学习的工作流程就会变得和监督学习一样，即便修改了算法，也无需重新收集数据。
- 元学习：这同样是一种工作流程的改进。如果能得到一个元学习器，我们就可以从过去的经验中更快地学习。另一方面，预训练+微调也可看作元学习的一种方式。

刚才我们主要侧重于规模扩大方面，而在泛化层面，我们仍然有很长的路要走。目前，如果训练一个人形机器人在完全平坦的平面上奔跑，换算到真实世界中的时间需要 6 天。但现实世界远比这样的环境复杂，难道对于每一种环境，我们都要专门训练一次吗？

为了完成多样的任务，一个可能的思路是我们之前讨论过的迁移学习和元学习方法。一个简单的例子是多任务学习：在这样的算法中，可能会出现更严重的方差和样本效率问题，一方面我们可以考虑直接求解一个增强型马尔可夫决策过程（MDP），也可以考虑专门设计一些算法来处理多任务的问题。
![](21-5.png)
通过在多个马尔可夫决策过程（MDP）的初始状态分布中采样来选择执行的任务，以实现多任务学习，这种做法可能会面临更严重的方差和样本效率问题。

### 2.4 Assumptions: Where does the supervision come from?
我们都清楚强化学习的建模过程，在强化学习中我们会有一个奖励函数，它是监督信号的来源。但很多时候归根结底，奖励从何而来？是我们设计的。  

在这种思路下，如果我们要进行多任务学习，就需要为每个任务设计一个奖励。在一些问题中，奖励非常简单，但在更多情况下，奖励作为一种监督信号是很难设计的。例如让机器人倒水，一个稀疏的奖励是幼稚的，问题在于我们的智能体往往从中什么都学不到。  

实际上，很多时候奖励并非我们唯一的选择，很多时候还可以有其他监督信号的来源：
- 演示：参见 Muelling, K et al. (2013). Learning to Select and Generalize Striking Movements in Robot Table Tennis.  
- 语言：参见 Andreas et al. (2018). Learning with latent language.  
- 人类偏好：参见 Christiano et al. (2017). Deep reinforcement learning from human preferences  

还有一些可能的方式，例如能否自动生成目标（利用自动技能发现）？

关于监督的选择，有一些值得思考的问题：我们的监督需要告诉智能体做什么还是怎么做？
- 对于演示而言，我们的监督不仅回答了 “做什么”，还回答了 “如何做”。例如，我们给出一个机器人倒水的演示，那么机器人不仅知道需要倒水，还知道了如何倒水。
- 对于奖励函数而言，我们的监督通常仅回答 “做什么”。例如，仅当机器人倒水成功时才给出奖励，但并未告诉机器人如何倒水。不过，如果我们设计出一个良好的奖励函数，也能够部分回答 “如何做” 的问题。
- 这实际上存在一种权衡：我们希望算法能够找到更好的解决方案，因此不应施加过多细节性的监督。但如果监督过于高层抽象，也可能导致整个学习过程变得极为困难。

从上述关于监督的讨论中，我们其实想引出一件更为重要的事：很多时候，我们不必局限于强化学习本身已有的诸多公式化表达，这些并非完全不可改变。我们需要仔细考量它们是否真正适合具体的问题场景，例如我们可以思考以下方面：
- 数据是什么？
- 目标是什么？（奖励 / 示例 / 偏好）
- 监督是什么？这可能不等同于目标，我们或许只是想提供一些提示，将某些示例作为指导而非目标，这是一个有待研究的开放领域。

## 3 Philosophical Perspective on Deep RL
这一部分我们将提供一些视角，关于深度强化学习理解的一些理念。
### 3.1 Reinforcement Learning as an Engineering Tool
我们实际上可以将强化学习视作一种工程工具。  

通常情况下，对于一个控制系统的工程问题，我们会在纸上写下一系列关于系统的数学方程，然后求解方程组，从期望的结果中反推出控制方式。但这样的反推过程并不容易，例如对于复杂的系统，我们可能会面对一个复杂的方程组。
![](21-6.png)
但是虽然反推控制方式非常困难，但是我们可以把这些方程组写进一个模拟器中，通过数值的方式来计算复杂的系统将会如何演化，这就像是我们在强化学习中的模拟器。 

当我们在这样的模拟器中运行强化学习算法时，我们实质上就在尝试从这些方程中反推出控制方式，但是我们并不是通过人来求解这个问题，而是通过机器学习的方式来进行。

因此强化学习提供了一种强大的工程工具，即一切我们能够模拟的都可以控制：
- 原先的工程方式是，我们会对问题进行建模，并实施控制。
- 而强化学习为我们带来了另一种工程方式：我们对问题进行建模并开展模拟，然后运行强化学习算法，得出控制方式。

注意：
- 从这一角度理解的启示是，我们要开发更加高效的模拟器，并且开发能够更好地利用模拟的强化学习算法。  
- 但是存在的问题是我们依然需要模拟。

### 3.2 Reinforcement Learning and the Real World
在这一部分中，我们想提出的是在真实世界中进行强化学习才是我们的真正目标，具体来说我们先考虑到以下几个例子。

例如，莫拉韦克悖论（Moravec's paradox）
研究强化学习（RL）的一个动机源于莫拉维克悖论（Moravec's paradox）：  
人工智能可以在棋类比赛中击败世界冠军，但实际上并非机器人在下棋，而是由现实中的人类代替人工智能完成实际操作。这是一个非常奇怪的现象 —— 我们的模型 “智能” 足以在棋盘上战胜世界冠军，却无法完成任何一个人类都能做到的实际棋子移动。
![](21-7.png)
这看似是一个悖论，但如果从另一角度看，则是完全有道理的：在一个纯粹的智力游戏上胜过人类，可能并没有想象的那么困难。之所以在这方面比不过人工智能，只是因为我们并不擅长它们，与之相比，我们只是远远地擅长移动我们的身体，以至于觉得后者是理所当然的。

莫拉维克悖论看似是一个关于人工智能的命题，实则可理解为一个关于物理宇宙的命题，即现实世界是一个“困难模式”的宇宙：
- 在 “简单模式” 的宇宙（如棋类游戏）中，运动控制和感知问题根本不存在。
- 而在我们所处的 “困难模式” 宇宙中，运动控制和感知是极具挑战性的。

再例如，另一个例子是考虑一个人在荒岛上生存，这个问题中有一系列特点：
- 极少的外部指导告诉我们应当做什么 ；
- 大量意料之外的情境需要适应；
- 必须要自发的找到解决问题的方案；
- 我们必须要存活足够长的时间来发现它们，与 Atari 游戏不同的是，在现实中我们只有一条命。

从这个角度出发，现实世界这个复杂的领域的另一个困难在于，现实世界中充满了可变性和意想不到的东西，在训练数据中 “永远不可能” 出现的那些东西在现实中随时有可能发生。

换句话说，在简单的宇宙中：
- 成功=高回报（最优控制）；
- 封闭世界，规则已知；
- 大量模拟；
- 核心问题：强化学习算法能否真正实现良好优化。

在困难的环境中：
- 成功 = 生存（足够好的控制能力）；
- 开放世界，一切必须来自数据；
- 无模拟（规则未知）；
- 核心问题：强化学习能否实现泛化与适应。

上述的几个例子直观展现了现实世界这个“困难宇宙”中的问题与我们通常强化学习研究中的“简单宇宙”中的问题的不同。很显然，我们的最终目标应当是让强化学习能够在现实世界这样的“困难宇宙”中取得好的效果。但是在目前的强化学习研究中，我们通常只考虑“简单宇宙”中的任务，我们需要更多地尝试解决这些“困难宇宙”中的问题。

但是如果要解决现实世界中的问题，我们需要考虑一些问题：
- 如何告诉强化学习智能体需要做什么？现实中没有分数。如果我们的目标是存活，那么这样的反馈过于滞后。
- 如何在持续的环境中完全自主地学习？现实中我们不可能重置世界从头再来。
- 在环境改变时，我们如何保持模型的鲁棒性？
- 利用过去经验和数据进行泛化的正确方式是什么？
- 利用先验经验进行自举探索的正确方式是什么？

接下来我们从几个机器人任务的例子来给出一些解决上述问题的启发：

例如，传达目标的其他方式：
除了奖励函数之外，有没有其他方式来传达目标呢？实际上，一个可行的做法是从偏好中学习。这里我们没有固定的奖励函数，而是像基于人类反馈的强化学习（RLHF）那样，让人类评价哪个行为更符合指令。
![](21-8.png)

参见: Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei. Deep reinforcement learning from human preferences. 2017.

再例如，完全自主学习：
在现实中，如果我们部署一个机器人操作任务，那么需要构建一整套系统（或进行人为复位），以使机械臂能够复位并不断尝试。这相当繁琐，而且从本质上来说不利于策略的泛化性。  

一个可行的思路是考虑多任务，具体的处理方式是让任务之间相互转化。这里以倒咖啡为例：

任务 1 很显然是"倒咖啡"。
但是如果失败了呢？通常需要一个人将杯子复位后再试一次，但我们可以设定任务 2 为 “捡起杯子”。
要是成功了呢？可以让任务 3 为 “把杯子放到一边，换一个杯子”，在此过程中若失败，就会产生任务 4 “清理洒出的咖啡”，依此类推。如果我们同时学习多个任务，那么每一次失败都会为我们提供学习全新任务的机会。

这里的例子是我们设计了一个多阶段且可以相互转化的任务，使得每一阶段如果智能体失败了，它都马上有一个新的任务可以练习，就不需要人为复位。

参见:
- Nagabandi, Konolige, Levine, Kumar. Deep Dynamics Models for Learning Dexterous Manipulation. CoRL 2019
![](21-9.png)
- Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, Levine. Reset-Free Reinforcement Learning via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without Human Intervention. 2021.
![](21-10.png)

再例如，如何从经验中引导探索？
一个角度是，在现实中并非所有行为都可以探索，例如某些行为可能会给智能体本身带来极其严重的伤害。而如何避免这些行为就需要一些先验知识。
另一个角度是，如果我们想要训练一个机械臂进行抓取操作，那么如果使用完全随机的行为作为初始化，机械臂只会随机乱动，很难进行有效的探索。但如果它已经具备完成一系列其他任务的经验，那么我们可以利用这些任务的数据构建一个行为先验，具体来说，可以从这几个任务的策略中进行随机采样。尽管机械臂执行的动作并不完全是我们想要的，但这是一种明显更高效的探索方式。
![](21-11.png)

参见: Singh\*, Hui\*, Zhou, Yu, Rhinehart, Levine. Parrot: Data-driven behavioral priors for reinforcement learning. 2020

现在我们清楚，在现实世界中进行强化学习是一件极具挑战性的事情，需要考虑诸多问题。但这是值得的：如果我们想看到有趣的涌现行为，就需要设计一个足够复杂的环境，以容纳这些新颖的解决方案。现实世界中的强化学习或许困难重重，但因其可能产生的涌现行为，依然值得我们去探索。

### 3.3 Reinforcement Learning as "Universal" Learning
在这一视角中，我们将强化学习（RL）视为一种 “通用” 的学习方式。  

当前大型语言模型（LLM）的成功基于大量无标注的预训练数据与少量标注数据，其背后的知识很大程度上来自我们通过自监督学习习得的分布 $p_\theta(\boldsymbol{x})$ 。这种训练方式之所以取得巨大成功，很大程度上是因为我们能够从大量低成本数据中学习知识。当然，这里面临的一个挑战是这些数据不能全是无用信息，即我们不能学习低质量的数据分布。

事实上，更好的做法是使用强化学习：  
机器学习的目标可以理解为生成具有适应性和复杂性的决策。（这一点可能并不明显，以图像分类为例，其背后的决策是预测标签后发生的事情，例如识别到险情时，决定是否报警。）  

实际上，强化学习是一种更好地利用低质量数据的方式。当我们获取大量低质量数据时，对其密度分布进行建模可能并非最佳选择。我们需要从中学习的不是在世界中 “如何去做”，而是在世界中 “能够做到什么”，换句话说，是学习 “世界如何运作”（动态）的信息。之后，我们再通过奖励函数等学习与任务相关的信息，并从关于世界的知识中找出那些 “最佳可能” 的决策。
![](21-12.png)

具体来说，我们可以得到如下的学习方式：
- 利用大规模数据进行离线强化学习，通过人为定义技能、目标条件强化学习、自监督技能发现等方式进行预训练。在这种方式中，我们学习关于（动态）世界的知识。
- 之后，我们在特定的下游任务上进行微调。在这种方式中，我们学习关于任务的知识（奖励函数）。
![](21-13.png)

一个将离线强化学习当作方法的例子如下：

例如，使用离线强化学习训练大语言模型：
通常情况下我们使用基于人类反馈的强化学习（RLHF）等方式来对齐大型语言模型（LLM），尽管这种方式可以让模型的回答更加符合人类偏好，但这种单步建模的方式并不擅长完成整个对话目标。

或许我们可以通过离线强化学习来进行改进，首先通过大型语言模型获取一系列合成的对话轨迹，然后在这些数据上进行离线强化学习，借助基于模型的强化学习等方式更好地理解人类需求（例如构建部分可观测马尔可夫决策过程模型（POMDP）），通过更简短的多轮对话来更贴合用户要求。
![](21-14.png)

参见: Hong, Levine, Dragan. Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations. 2023.

## 4 Back to the Bigger Picture
### 4.1 Why we need Deep RL?
回到这门课程介绍的部分中我们引出深度强化学习的原因：
- 学习是智能的基础；
- 强化学习是对决策进行推理的方式；
- 深度模型允许强化学习算法学习到复杂的映射，通过端到端的方式解决复杂的问题。

### 4.2 What's missing for an intelligent system?
我们目前还无法实现一个真正的智能系统，其真正重要的是什么呢？
从杨立昆（Yann LeCun）的 “蛋糕理论” 角度来看，通过不同的学习方法，我们能够获得的监督量有很大差异，因此重要的是：
- 无监督学习或自监督学习；
- 学习一个世界模型（预测未来）；
- 对世界进行生成式建模。
![](21-15.png)
在实现目标之前，还有很多事情要做：
- 监督还可以来自其他方面，例如模仿以及对其他智能体的理解；
- 也许强化学习本身已足够，尽管奖励可能非常稀疏，但叠加复杂动态后得到的值函数备份已携带足够信息；
- 或许以上都是需要的。

### 4.3 How should we answer these questions?
这一部分是莱文（Levine）对强化学习科研的思考，也可以理解为莱文对强化学习科研的一些建议：
- 选择正确的问题：问问自己，这有机会解决一个重要问题吗？保持对不确定性的乐观态度是一种很好的探索策略。
- 不要害怕改变问题的表述：仅仅冲击基准测试（benchmark）并不会遇到很多有意义的挑战。
- 应用很重要：很多时候，将方法应用于具有挑战性的现实场景中，能让我们发现缺失的重要内容。强化学习的历史上，曾有很长一段时间忽视了这一点。
- 大胆设想，小处着手（Think big and start small）！