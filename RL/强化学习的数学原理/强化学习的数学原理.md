# 1 基本概念
## 1.1 概念
状态、状态空间；动作、动作空间；
状态转移：当采取一个动作，智能体从一个状态转移到另一个状态。
策略：智能体在一个状态应该采取什么动作。

奖励：智能体采取动作后获得的一个实数值，正为鼓励，负为惩罚。
轨迹：一条轨迹是一个状态-动作-奖励链。

## 1.2 马尔可夫决策过程
马尔科夫决策过程（MDP）：
- 状态空间：$S$
- 动作空间：$A(s),\; s \in S$
- 奖励：$R(s, a),\; s \in S,\; a \in A(s)$
- 状态转移概率：$p(s'\;| s,\; a),\; s \in S,\; a \in A(s)$
- 策略：$\pi (a\;|s),\; s \in S,\; a \in A(s)$
马尔可夫性质：未来状态的条件转移概率只依赖于当前的状态和动作，与过去的状态和动作是独立的。

# 2 贝尔曼方程
## 2.1 确定性问题的贝尔曼方程
确定性问题的贝尔曼公式：
$$
\boldsymbol{v} = \boldsymbol{r} + \gamma \boldsymbol{P}\boldsymbol{v}
$$
其中 $\boldsymbol{r}$ 为奖励，$\boldsymbol{P}$ 为状态转移矩阵，$\boldsymbol{v}$ 为状态价值。
表明一个状态的价值依赖于其他状态的价值。

## 2.2 随机性问题的折扣回报
对于随机性问题：
$$
S_t \xrightarrow{A_t} R_{t + 1},\; S_{t + 1}
$$
- $S_t, A_t, R_{t + 1}$ 均为随机变量。
- $S_t \to A_t$ 依赖于 $\pi (A_t = a | S_t = s)$，即策略函数。
- $S_t, A_t \to R_{t + 1}$ 依赖于 $p(R_{t + 1} = r| S_t=s, A_t = a)$。
- $S_t,A_t \to S_{t + 1}$ 依赖于 $p(S_{t + 1}=s'|S_t = s, A_t = a)$。

考虑以下多步轨迹：

$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \ldots$$
折扣回报定义为：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

其中 $\gamma \in [0, 1)$ 是折扣率，$G_t$ 也是一个随机变量，因为 $R_{t+1}, R_{t+2}, \ldots$ 都是随机变量。
折扣回报通过折扣因子 $\gamma$ 降低了未来奖励的权重，使得智能体更关注近期奖励。较小的 $\gamma$ 值会使智能体更加"近视"，而接近1的 $\gamma$ 值则使智能体更加"远视"。

## 2.3 状态价值函数
状态价值函数（或简称状态价值）是折扣回报 $G_t$ 的期望值（也称为期望价值或均值），定义为：

$$v_{\pi}(s) = \mathbb{E}[G_t|S_t = s]$$

- 状态价值是关于状态 $s$ 的函数，表示在给定起始状态 $s$ 的条件下的条件期望。
- 状态价值基于策略 $\pi$。不同的策略可能导致不同的状态价值。
- 状态价值代表一个状态的"价值"。状态价值越大，说明该策略越好，因为能获得更高的累积奖励。

## 2.4 推导贝尔曼方程

考虑一个随机轨迹：
$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \dots$$

回报 $G_t$ 可以写成：
$$
\begin{aligned}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\ 
    & = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
    & = R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$
根据状态价值函数的定义，有：
$$
\begin{aligned}
v_\pi(s) & = \mathbb{E}_\pi [G_t | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
\end{aligned}
$$

接下来，分别计算这两个期望。
首先，计算第一项 $\mathbb{E}_\pi [R_{t+1} | S_t = s]$（这是即时奖励的均值）： 
$$ \begin{aligned} \mathbb{E}_\pi [R_{t+1} | S_t = s] &= \sum_a \pi(a|s) \mathbb{E} [R_{t+1} | S_t = s, A_t = a] \\ &= \sum_a \pi(a|s) \sum_{r} p(r|s, a) r \end{aligned} $$
接下来，计算第二项 $\mathbb{E}_\pi [G_{t+1} | S_t = s]$（这是未来回报的均值）：
$$
\begin{aligned}
\mathbb{E}_\pi [G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_t = s, S_{t+1} = s'] p(s'|s) \\
&= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s'] p(s'|s) \text{（由于马尔可夫性质）}\\
&= \sum_{s'} v_\pi(s') p(s'|s) \\
&= \sum_{s'} v_\pi(s') \sum_a p(s'|s, a) \pi(a|s)
\end{aligned}
$$
因此，我们得到：
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s] \\
&= \underbrace{\sum_a \pi(a|s) \sum_r p(r|s, a) r}_{\text{即时奖励的均值}} + \gamma \underbrace{\sum_a \pi(a|s) \sum_{s'} p(s'|s, a) v_\pi(s')}_{\text{未来奖励的均值}} \\
&= \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right], \quad \forall s \in \mathcal{S}
\end{aligned}
$$
- $v_\pi(s)$ 和 $v_\pi(s')$ 是需要计算的状态价值。
- $\pi(a|s)$ 是给定的策略。求解这个方程被称为**策略评估 (policy evaluation)**。
- $p(r|s, a)$ 和 $p(s'|s, a)$ 代表环境。

上述方程被称为**贝尔曼方程**，描述了不同状态的状态价值函数之间的关系，由两项组成：即时奖励项和未来奖励项。这是一个方程组：每个状态都有一个这样的方程。

## 2.5 贝尔曼方程的向量形式
假设状态可以被索引为 $s_i$ ($i = 1, \dots, n$)。对于状态 $s_i$，贝尔曼方程是： $$v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)$$将所有状态的这些方程放在一起，并改写为矩阵向量形式： $$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$其中： 
- $\mathbf{v}_\pi = [v_\pi(s_1), \dots, v_\pi(s_n)]^T \in \mathbb{R}^n$ 是价值向量
- $\mathbf{r}_\pi = [r_\pi(s_1), \dots, r_\pi(s_n)]^T \in \mathbb{R}^n$ 是奖励向量，$r_\pi(s) = \sum_a \pi(a|s) \sum_r p(r|s, a) r$
- $\mathbf{P}_\pi \in \mathbb{R}^{n \times n}$ 是状态转移矩阵，$[\mathbf{P}_\pi]_{ij} = p_\pi(s_j|s_i) = \sum_a \pi(a|s_i) p(s_j|s_i, a)$

## 2.6 求解状态价值

贝尔曼方程的矩阵向量形式是：
$$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$

闭式解 (The closed-form solution) 是：
$$ \mathbf{v}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi $$
在实践中，我们仍然需要使用数值工具来计算矩阵的逆。

通过迭代算法可以避免矩阵求逆运算，一个迭代解 (An iterative solution) 是：
$$ \mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k $$
这个算法产生一个序列 $\{\mathbf{v}_0, \mathbf{v}_1, \mathbf{v}_2, \dots\}$。可以证明：
$$ \mathbf{v}_k \rightarrow (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi, \quad k \rightarrow \infty $$

**证明：**
定义误差为 $\delta_k = \mathbf{v}_k - \mathbf{v}_\pi$。我们只需要证明当 $k \rightarrow \infty$ 时，$\delta_k \rightarrow \mathbf{0}$。

将 $\mathbf{v}_{k+1} = \delta_{k+1} + \mathbf{v}_\pi$ 和 $\mathbf{v}_k = \delta_k + \mathbf{v}_\pi$ 代入迭代公式 $\mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k$，得到：
$$
\begin{aligned}
\delta_{k+1} + \mathbf{v}_\pi 
&= \mathbf{r}_\pi + \gamma \mathbf{P}_\pi (\delta_k + \mathbf{v}_\pi) \\
& = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \delta_k + \gamma \mathbf{P}_\pi \mathbf{v}_\pi 
\end{aligned}
$$
由于 $\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi$，上式可以改写为：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k $$
因此，我们可以得到：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k = \gamma^2 \mathbf{P}_\pi^2 \delta_{k-1} = \dots = \gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 $$
注意 $0 \le [\mathbf{P}_\pi]_{ij} \le 1$，这表示 $\mathbf{P}_\pi$ 的每一行的元素都是非负的且和为 1。这是因为 $\mathbf{P}_\pi$ 是状态转移矩阵。因此，$\mathbf{P}_\pi \mathbf{1} = \mathbf{1}$，其中 $\mathbf{1} = [1, 1, \dots, 1]^T$。
另一方面，由于 $\gamma < 1$，我们知道 $\gamma^k \rightarrow 0$，因此 $\gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 \rightarrow \mathbf{0}$ 当 $k \rightarrow \infty$ 时。
因此，$\delta_k \rightarrow \mathbf{0}$，这意味着 $\mathbf{v}_k \rightarrow \mathbf{v}_\pi$ 当 $k \rightarrow \infty$ 时。

## 2.7 动作价值函数
状态价值：从一个状态开始，智能体能够获得的平均回报。
动作价值：从一个状态开始并采取一个动作，智能体能够获得的平均回报。

定义：
$$ q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
* $q_\pi(s, a)$ 是状态-动作对 $(s, a)$ 的函数。
* $q_\pi(s, a)$ 依赖于策略 $\pi$。

根据条件期望的性质，有：
$$
\underbrace{\mathbb{E}[G_t | S_t = s]}_{v_\pi(s)} = \sum_a \underbrace{\mathbb{E}[G_t | S_t = s, A_t = a]}_{q_\pi(s,a)} \pi(a|s)
$$
因此，
$$ v_\pi(s) = \sum_a \pi(a|s) q_\pi(s, a) \quad (1) $$
回顾状态价值函数由下式给出：
$$ v_\pi(s) = \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right] \quad (2) $$

通过比较 (1) 和 (2)，我们得到**动作价值函数**为：
$$ q_\pi(s, a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \quad (3) $$
(1) 和 (3) 是同一枚硬币的两面：
* (1) 展示了如何从动作价值函数获得状态价值函数。
* (3) 展示了如何从状态价值函数获得动作价值函数。

# 3 贝尔曼最优方程
## 3.1 最优策略
状态值函数可以用来评估一个策略的好坏：如果对于所有状态 $s \in \mathcal{S}$，都有 
$$
v_{\pi_1}(s) \ge v_{\pi_2}(s),
$$
那么我们认为策略 $\pi_1$ 比策略 $\pi_2$ “更好”。 

一个策略 $\pi^*$ 被称为**最优策略**，如果对于所有状态 $s \in \mathcal{S}$ 和任何其他策略 $\pi$，都有
$$
v_{\pi^*}(s) \ge v_{\pi}(s).
$$

## 3.2 贝尔曼最优方程
贝尔曼最优方程（元素形式）：
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
也可以写成：
$$
v_*(s) = \max_{a} q_*(s, a), \quad s \in \mathcal{S}
$$
其中，
$$
q_*(s, a) = \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right)
$$
是最优状态-动作值函数。
* $p(r|s, a)$ 和 $p(s'|s, a)$ 是已知的。
* $v_*(s)$ 和 $v_*(s')$ 是未知的，需要计算。
* 在最优性方程中，我们正在寻找最优策略，通常情况下 $\pi(a|s)$ 是我们想要确定的。

贝尔曼最优性方程（矩阵-向量形式）：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
其中，对应于状态 $s$ 或 $s'$ 的元素是：
$$
[\mathbf{r}_{\boldsymbol{\pi}}]_s \triangleq \sum_a \pi(a|s) \sum_{s'} p(s'|s, a) r(s, a, s')
$$
$$
[\mathbf{P}_{\boldsymbol{\pi}}]_{s, s'} \triangleq \sum_a \pi(a|s) p(s'|s, a)
$$
这里的 $\max_{\boldsymbol{\pi}}$ 是**按元素**取最大值。

对于贝尔曼最优方程：
* **算法 (Algorithm):** 如何求解这个方程？
* **存在性 (Existence):** 这个方程是否有解？
* **唯一性 (Uniqueness):** 这个方程的解是唯一的吗？
* **最优性 (Optimality):** 它与最优策略是如何相关的？

## 3.3 贝尔曼最优方程右侧的最大化
贝尔曼最优方程: 元素形式
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
贝尔曼最优方程: 矩阵-向量形式
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
首先固定 $v'(s)$ 并求解 $\pi$：
$$
\begin{aligned}
v(s) 
&= \max_{\pi} \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v'(s')) \right) \\
&= \max_{\pi} \sum_a \pi(a|s) q(s, a), \quad \forall s \in \mathcal{S}
\end{aligned}
$$
考虑到 $\sum_a \pi(a|s) = 1$，我们有：
$$
\max_{\pi} \sum_a \pi(a|s) q(s, a) = \max_{a \in \mathcal{A}(s)} q(s, a)
$$
最优性在以下情况下取得：
$$
\pi(a|s) =
\begin{cases}
1 & \text{如果 } a = a^* \\
0 & \text{如果 } a \neq a^*
\end{cases}
$$
其中 $a^* = \arg \max_a q(s, a)$。

## 3.4 求解贝尔曼最优性方程

贝尔曼最优性方程是：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*).
$$
令：

$$
f(\mathbf{v}) := \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
那么，贝尔曼最优性方程可以写成：
$$
\mathbf{v}_* = f(\mathbf{v}_*)
$$
其中，$f(\mathbf{v})$ 的第 $s$ 个元素是：
$$
[f(\mathbf{v})]_s = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v(s')) = \max_a q(s, a)
$$
### 3.4.1 预备知识：压缩映射定理
#### 3.4.1.1 一些概念
**不动点 (Fixed point):** $x \in \mathcal{X}$ 是函数 $f: \mathcal{X} \rightarrow \mathcal{X}$ 的一个不动点，如果：$$f(x) = x$$

**压缩映射 (Contraction mapping or contractive function)：** 函数 $f$ 是一个压缩映射，如果存在一个常数 $\gamma \in [0, 1)$，使得对于所有 $x_1, x_2 \in \mathcal{X}$，都有：$$\|f(x_1) - f(x_2)\| \le \gamma \|x_1 - x_2\|$$其中：
- $\gamma \in [0, 1)$。
- $\gamma$ 必须严格小于 1，这样才能保证许多极限成立，例如当 $k \rightarrow \infty$ 时，$\gamma^k \rightarrow 0$。
- $\|\cdot\|$ 可以是任何向量范数。

#### 3.4.1.2 压缩映射定理
对于任何形式为 $x = f(x)$ 的方程，如果 $f$ 是一个压缩映射，那么：
* **存在性：** 存在一个不动点 $x^*$ 满足 $f(x^*) = x^*$。
* **唯一性：** 这个不动点 $x^*$ 是唯一的。
* **算法：** 考虑一个序列 $\{x_k\}$，其中 $x_{k+1} = f(x_k)$，那么 $x_k \rightarrow x^*$ 当 $k \rightarrow \infty$。此外，收敛速度是指数级的。

#### 3.1.4.3 压缩映射定理的证明
**第一部分：我们证明序列 $\{x_k\}_{k=1}^\infty$，其中 $x_k = f(x_{k-1})$ 是收敛的。**
这个证明依赖于柯西序列（Cauchy sequence）。
一个序列 $x_1, x_2, ... \in \mathbb{R}$ 被称为柯西序列，如果对于任意小的 $\epsilon > 0$，存在一个正整数 $N$ 使得对于所有的 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。
直观的理解是，存在一个有限的整数 $N$，使得所有在 $N$ 之后的元素都充分地靠近彼此。
柯西序列很重要，因为可以保证一个柯西序列收敛到一个极限。它的收敛性质将被用于证明压缩映射定理。
注意，我们必须有 $||x_m - x_n|| < \epsilon$ 对于所有 $m, n > N$。如果我们仅仅有 $x_{n+1} - x_n \to 0$，不足以声明该序列是一个柯西序列。例如，对于 $x_n = \sqrt{n}$，有 $x_{n+1} - x_n \to 0$，但是显然 $x_n = \sqrt{n}$ 是发散的。
我们接下来证明 $\{x_k = f(x_{k-1})\}_{k=1}^\infty$ 是一个柯西序列，因此是收敛的。
首先，由于 $f$ 是一个压缩映射，我们有
$$||x_{k+1} - x_k|| = ||f(x_k) - f(x_{k-1})|| \le \gamma ||x_k - x_{k-1}||.$$
类似地，我们有 $||x_k - x_{k-1}|| \le \gamma ||x_{k-1} - x_{k-2}||, ..., ||x_2 - x_1|| \le \gamma ||x_1 - x_0||$。
因此，我们有
$$
\begin{align*} 
||x_{k+1} - x_k|| 
&\le \gamma ||x_k - x_{k-1}|| \\ 
&\le \gamma^2 ||x_{k-1} - x_{k-2}|| \\ 
&\vdots \\ 
&\le \gamma^k ||x_1 - x_0||. 
\end{align*}
$$
由于 $\gamma < 1$，我们知道 $||x_{k+1} - x_k||$ 以指数速度收敛到零，当 $k \to \infty$ 时，给定任意的 $x_1, x_0$。特别地，$\{|x_{k+1} - x_k|\}$ 的收敛性不足以推出 $\{x_k\}$ 的收敛性。因此，我们需要进一步考虑对于任意 $m > n$ 的 $||x_m - x_n||$。特别地，
$$
\begin{align*} 
||x_m - x_n|| 
&= ||x_m - x_{m-1} + x_{m-1} - ... - x_{n+1} + x_{n+1} - x_n|| \\ 
&\le ||x_m - x_{m-1}|| + ||x_{m-1} - x_{m-2}|| + ... + ||x_{n+1} - x_n|| \\ 
&\le \gamma^{m-1} ||x_1 - x_0|| + ... + \gamma^n ||x_1 - x_0|| \\ 
&= \gamma^n (\gamma^{m-n-1} + ... + 1) ||x_1 - x_0|| \\ 
&= \gamma^n (1 + \gamma + ... + \gamma^{m-n-1}) ||x_1 - x_0|| \\ 
&= \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.
\end{align*}
$$
因此，对于任意 $\epsilon$，我们总能找到一个 $N$ 使得对于所有 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。因此，这个序列是柯西序列，并且收敛到一个极限点，记为 $x^* = \lim_{k \to \infty} x_k$。

**第二部分：我们证明极限 $x^* = \lim_{k \to \infty} x_k$ 是一个不动点。**
为了做到这一点，由于
$$||f(x_k) - x_k|| = ||x_{k+1} - x_k|| \le \gamma^k ||x_1 - x_0||,$$
我们知道 $||f(x_k) - x_k||$ 以指数速度收敛到零。因此，在极限情况下，我们有 $f(x^*) = x^*$。

**第三部分：我们证明不动点是唯一的。**
假设存在另一个不动点 $x'$ 满足 $f(x') = x'$。那么，
$$||x' - x^*|| = ||f(x') - f(x^*)|| \le \gamma ||x' - x^*||.$$
由于 $\gamma < 1$，这个不等式成立当且仅当 $||x' - x^*|| = 0$。因此，$x' = x^*$。

**第四部分：我们证明 $x_k$ 以指数速度收敛到 $x^*$。**
回想一下，正如上述证明的，$||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||$。由于 $m$ 可以任意大，我们有
$$||x^* - x_n|| = \lim_{m \to \infty} ||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.$$
由于 $\gamma < 1$，当 $n \to \infty$ 时，误差以指数速度收敛到零。

### 3.4.2 贝尔曼最优性方程的压缩性质
让我们回到贝尔曼最优性方程：
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
**定理 (压缩性质 - Contraction Property)**
$f(\mathbf{v})$ 是一个压缩映射，满足：
$$
\|f(\mathbf{v}_1) - f(\mathbf{v}_2)\| \le \gamma \|\mathbf{v}_1 - \mathbf{v}_2\|
$$
其中 $\gamma$ 是折扣率（discount rate），且 $\gamma \in [0, 1)$。

**证明：**
考虑任意两个向量 $v_1, v_2 \in \mathbb{R}^{|S|}$，
假设 $\pi_1^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_1)$ 并且 $\pi_2^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_2)$。那么，
$$
\begin{aligned}
f(v_1) &= \max_{\pi} (r_\pi + \gamma P_\pi v_1) = r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 \ge r_{\pi_2^*} + \gamma P_{\pi_2^*} v_1, \\
f(v_2) &= \max_{\pi} (r_\pi + \gamma P_\pi v_2) = r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2 \ge r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2,
\end{aligned}
$$
其中 $\ge$ 是按元素比较。因此，
$$
\begin{aligned}
f(v_1) - f(v_2) &= r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2) \\
&\le r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2) \\
&= \gamma P_{\pi_1^*} (v_1 - v_2).
\end{aligned}
$$
类似地，可以证明 $f(v_2) - f(v_1) \le \gamma P_{\pi_2^*} (v_2 - v_1)$。因此，
$$
\gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2).
$$
定义
$$
z \doteq \max \{|\gamma P_{\pi_2^*} (v_1 - v_2)|, |\gamma P_{\pi_1^*} (v_1 - v_2)|\} \in \mathbb{R}^{|S|},
$$
其中 $\max\{\cdot, \cdot\}$，$|\cdot|$，以及 $\ge$ 都是按元素操作。根据定义，$z \ge 0$。一方面，很容易看出
$$
-z \le \gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2) \le z,
$$
这蕴含着
$$
|f(v_1) - f(v_2)| \le z.
$$
由此可得
$$
\|f(v_1) - f(v_2)\|_\infty \le \|z\|_\infty. \quad (*)
$$
其中 $\|\cdot\|_\infty$ 是最大范数。另一方面，假设 $z_i$ 是 $z$ 的第 $i$ 个元素，并且 $p_i^T$ 和 $q_i^T$ 分别是 $P_{\pi_1^*}$ 和 $P_{\pi_2^*}$ 的第 $i$ 行。那么，
$$
z_i = \max \{|\gamma p_i^T (v_1 - v_2)|, |\gamma q_i^T (v_1 - v_2)|\}.
$$
由于 $p_i$ 是一个所有元素非负且元素之和等于 1 的向量，因此有
$$
|p_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty.
$$
类似地，我们有 $|q_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty$。因此，$z_i \le \gamma \|v_1 - v_2\|_\infty$，从而
$$
\|z\|_\infty = \max_i |z_i| \le \gamma \|v_1 - v_2\|_\infty.
$$
将此不等式代入 (\*) 得到
$$
\|f(v_1) - f(v_2)\|_\infty \le \gamma \|v_1 - v_2\|_\infty,
$$
这证明了 $f(v)$ 的压缩性质。

应用压缩映射定理可以得到以下结果：
**定理 (存在性、唯一性和算法 - Existence, Uniqueness, and Algorithm)**
对于贝尔曼最优性方程
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*),
$$
总是存在一个解 $\mathbf{v}_*$ 并且解是唯一的。这个解可以通过迭代的方式求解：
$$
\mathbf{v}_{k+1} = f(\mathbf{v}_k) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_k)
$$
这个序列 $\{\mathbf{v}_k\}$ 以指数级的速度收敛到 $\mathbf{v}_*$，给定任何初始猜测 $\mathbf{v}_0$。收敛速度由 $\gamma$ 决定。


## 3.5 策略最优性
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程的解。它满足：
$$
\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
假设存在一个策略 $\boldsymbol{\pi}^*$，对于所有状态 $s$，它都能达到上述最大值，即：
$$
\boldsymbol{\pi}^* = \arg \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
那么，对于这个特定的策略 $\boldsymbol{\pi}^*$，我们有：
$$
\mathbf{v}^* = \mathbf{r}_{\boldsymbol{\pi}^*} + \gamma \mathbf{P}_{\boldsymbol{\pi}^*} \mathbf{v}^*
$$
因此，$\boldsymbol{\pi}^*$ 是一个策略，且 $\mathbf{v}^*$ 是其对应的状态值函数。

问题：$\boldsymbol{\pi}^*$ 是最优策略吗？$\mathbf{v}^*$ 是可以达到的最大的状态值吗？

**定理 (策略最优性 - Policy Optimality Theorem)**
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程 
$$
\mathbf{v} = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
的唯一解，并且对于任意给定的策略 $\boldsymbol{\pi}$，其状态值函数 $\mathbf{v}_{\boldsymbol{\pi}}$ 满足贝尔曼期望方程 $\mathbf{v}_{\boldsymbol{\pi}} = \mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_{\boldsymbol{\pi}}$，那么：
$$
\mathbf{v}^* \ge \mathbf{v}_{\boldsymbol{\pi}}, \quad \forall \boldsymbol{\pi}
$$
**证明：**
对于任意策略 $\pi$，有
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi.
$$
由于
$$
v^* = \max_{\pi} (r_\pi + \gamma P_\pi v^*) = r_{\pi^*} + \gamma P_{\pi^*} v^* \ge r_\pi + \gamma P_\pi v^*,
$$
我们有
$$
v^* - v_\pi \ge (r_\pi + \gamma P_\pi v^*) - (r_\pi + \gamma P_\pi v_\pi) = \gamma P_\pi (v^* - v_\pi).
$$
重复应用上述不等式得到 $v^* - v_\pi \ge \gamma P_\pi (v^* - v_\pi) \ge \gamma^2 P_\pi^2 (v^* - v_\pi) \ge \cdots \ge \gamma^n P_\pi^n (v^* - v_\pi)$。由此可知
$$
v^* - v_\pi \ge \lim_{n \to \infty} \gamma^n P_\pi^n (v^* - v_\pi) = 0,
$$
其中最后一个等式成立是因为 $\gamma < 1$ 且 $P_\pi$ 是一个非负矩阵，其所有元素小于等于 1（因为 $P_\pi \mathbf{1} = \mathbf{1}$）。因此，$v^* \ge v_\pi$ 对于任意 $\pi$ 成立。


**定理 (贪婪最优策略 - Greedy Optimal Policy Theorem)**
对于任何状态 $s \in \mathcal{S}$，确定性的贪婪策略 $\boldsymbol{\pi}^*$ 定义为：
$$
\pi^*(a|s) =
\begin{cases}
1 & \text{if } a = a^*(s) \\
0 & \text{if  } a \neq a^*(s)
\end{cases} \quad (1)
$$
是求解贝尔曼最优性方程的一个最优策略。这里，$a^*(s)$ 定义为：
$$
a^*(s) = \arg \max_a q^*(s, a)
$$
其中最优动作值函数 $q^*(s, a)$ 定义为：
$$
q^*(s, a) := \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$

**证明：**
简单来说，
$$
\begin{aligned}
\pi^*(s) 
&= \arg \max_a \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s')) \right) \\
&= \arg \max_a q^*(s, a)
\end{aligned}
$$

## 3.6 分析最优策略
哪些因素决定了最优策略？
从贝尔曼最优性方程中可以清晰地看出：

$$
v^*(s) = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$
有三个关键因素：
* **奖励设计 (Reward design):** $r(s, a, s')$
* **系统模型 (System model):** $p(s'|s, a)$
* **折扣率 (Discount rate):** $\gamma$
需要计算的未知量是：$v^*(s), v^*(s'), \pi^*(a|s)$。

**定理 (最优策略不变性 - Optimal Policy Invariance)**
考虑一个马尔可夫决策过程，其最优状态值函数为 $\mathbf{v}^* \in \mathbb{R}^{|\mathcal{S}|}$，满足 $\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)$。如果每个奖励 $r$ 都经过一个仿射变换 $ar + b$，其中 $a, b \in \mathbb{R}$ 且 $a > 0$，那么对应的最优状态值函数 $\mathbf{v}'^*$ 也是 $\mathbf{v}^*$ 的一个仿射变换：
$$
\mathbf{v}'^* = a\mathbf{v}^* + \frac{b}{1 - \gamma} \mathbf{1}
$$
其中 $\gamma \in [0, 1)$ 是折扣率，$\mathbf{1} = [1, \dots, 1]^T$ 是一个所有元素都为 1 的向量。
因此，最优策略对于奖励信号的仿射变换是不变的。

**证明：**
对于任意策略 $\pi$，定义 $r_\pi = [..., r_\pi(s), ...]^T$，其中
$$
r_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s,a)r, \quad s \in \mathcal{S}.
$$
如果 $r \rightarrow \alpha r + \beta$，那么 $r_\pi(s) \rightarrow \alpha r_\pi(s) + \beta$，因此 $r_\pi \rightarrow \alpha r_\pi + \beta \mathbf{1}$，其中 $\mathbf{1} = [..., 1, ...]^T$。在这种情况下，BOE 变为
$$
v' = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi v'). \quad (*)
$$
我们通过证明 $v' = \alpha v^* + c\mathbf{1}$ 是新 BOE (\*) 的解来求解它，其中 $c = \beta/(1-\gamma)$。特别地，将 $v' = \alpha v^* + c\mathbf{1}$ 代入 (\*) 得到
$$
\alpha v^* + c\mathbf{1} = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi (\alpha v^* + c\mathbf{1})) = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \alpha \gamma P_\pi v^* + \gamma c P_\pi \mathbf{1}),
$$
其中最后一个等式成立是因为 $P_\pi \mathbf{1} = \mathbf{1}$。上述方程可以重新整理为
$$
\alpha v^* = \max_{\pi \in \Pi} (\alpha r_\pi + \alpha \gamma P_\pi v^*) + \beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1},
$$
这等价于
$$
\beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1} = 0.
$$
由于 $c = \beta/(1-\gamma)$，上述方程成立，因此 $v' = \alpha v^* + c\mathbf{1}$ 是 (\*) 的解。由于 (\*) 是 BOE，$v'$ 也是唯一的解。最后，由于 $v'$ 是 $v^*$ 的仿射变换，动作值之间的相对关系保持不变。因此，从 $v'$ 导出的贪婪最优策略与从 $v^*$ 导出的相同：$\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v')$ 与 $\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*)$ 相同。

# 4 值迭代与策略迭代
## 4.1 值迭代算法
如何求解贝尔曼最优性方程？
$$
v = f(v) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v)
$$
在上一讲中，我们知道压缩映射定理提出了一种迭代算法：
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k), \quad k = 1, 2, 3...
$$
其中 $v_0$ 可以是任意的。 这个算法最终可以找到最优状态值和最优策略。 这个算法被称为**值迭代**。 我们将看到我们所学的关于贝尔曼最优性方程（BOE）的数学知识最终会发挥作用。

值迭代算法
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k), \quad k = 1, 2, 3...
$$
可以分解为两步。
步骤 1：策略更新。这一步是求解
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)
$$
其中 $v_k$ 已知。
步骤 2：值更新。
$$
v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k
$$

问题：$v_k$ 是一个状态值吗？不，因为它不能保证 $v_k$ 满足贝尔曼方程。

对于步骤 1: 策略更新，其逐元素形式为
$$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)$$
是
$$
\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}
$$
求解上述优化问题的最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 & a = a_k^*(s) \\ 0 & a \neq a_k^*(s) \end{cases}
$$
其中 $a_k^*(s) = \arg \max_{a} q_k(s,a)$。$\pi_{k+1}$ 被称为一个**贪婪策略**，因为它简单地选择了最大的 $q$ 值。

对于步骤 2：值更新，其逐元素形式为
$$
v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k
$$
是
$$
v_{k+1}(s) = \sum_{a} \pi_{k+1}(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}
$$
由于 $\pi_{k+1}$ 是贪婪的，上述方程简化为
$$
v_{k+1}(s) = \max_{a} q_k(a, s)
$$

算法总结：
$$
v_k(s) \to q_k(s,a) \to \text{greedy policy } \pi_{k+1}(a|s) \to \text{new value } v_{k+1} = \max_a q_k(s,a)
$$

**伪代码：值迭代算法**
初始化：对于所有的 $(s,a)$，概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 是已知的。
任意猜测 $v_0$。
目标：搜索求解贝尔曼最优性方程的最优状态值和最优策略。

当 $v_k$ 在 $\|v_k - v_{k-1}\|$ 大于预定义的小阈值的意义上尚未收敛时，对于第 k 次迭代，执行以下操作：
	对于每个状态 $s \in S$，执行：
		对于每个动作 $a \in A(s)$，执行：
			$q$ 值: $q_k(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$
			最大动作值：$a_k^*(s) = \arg \max_a q_k(a,s)$
			策略更新：$\pi_{k+1}(a|s) = 1$ 如果 $a = a_k^*$，否则 $\pi_{k+1}(a|s) = 0$
			值更新：$v_{k+1}(s) = \max_a q_k(a,s)$

## 4.2 策略迭代算法
给定一个随机初始策略 $\pi_0$，
步骤 1：策略评估（PE）
这一步是计算 $\pi_k$ 的状态值：
$$
v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
$$
请注意，$v_{\pi_k}$ 是一个状态值函数。
步骤 2：策略改进（PI）
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
最大化是逐分量的。

在策略评估步骤中，如何通过求解贝尔曼方程得到状态值 $v_{\pi_k}$？
$$
v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
$$
闭式解：
$$
v_{\pi_k} = (I - \gamma P_{\pi_k})^{-1} r_{\pi_k}
$$
迭代解：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0,1,2,...
$$
已在贝尔曼方程的求解中学习过。
策略迭代是一个迭代算法，其中嵌入了另一个迭代算法在策略评估步骤中。

在策略改进步骤中，为什么新策略 $\pi_{k+1}$ 比 $\pi_k$ 更好？
**引理（策略改进）：**
如果
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
那么对于任何 $k$，有 $v_{\pi_{k+1}} \ge v_{\pi_k}$。

**证明：**
由于 $v_{\pi_{k+1}}$ 和 $v_{\pi_k}$ 是状态值，它们满足贝尔曼方程：
$$
\begin{aligned}
v_{\pi_{k+1}} &= r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}, \\
v_{\pi_k} &= r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}.
\end{aligned}
$$
由于 $\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})$，我们知道
$$
r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k} \ge r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}.
$$
因此有
$$
\begin{aligned}
v_{\pi_k} - v_{\pi_{k+1}} &= (r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) \\
&\le (r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) \\
&\le \gamma P_{\pi_{k+1}} (v_{\pi_k} - v_{\pi_{k+1}}).
\end{aligned}
$$
因此，
$$
\begin{aligned}
v_{\pi_k} - v_{\pi_{k+1}} 
& \le \gamma^2 P_{\pi_{k+1}}^2 (v_{\pi_k} - v_{\pi_{k+1}}) \le \dots \le \gamma^n P_{\pi_{k+1}}^n (v_{\pi_k} - v_{\pi_{k+1}}) \\
& \le \lim_{n \to \infty} \gamma^n P_{\pi_{k+1}}^n (v_{\pi_k} - v_{\pi_{k+1}}) = 0.
\end{aligned}
$$
该极限是由于 $n \to \infty$ 时 $\gamma^n \to 0$ 以及 $P_{\pi_{k+1}}^n$ 对于任何 $n$ 都是一个非负随机矩阵。这里，随机矩阵指的是一个非负随机矩阵，其所有行的行和都等于一。

为什么这种迭代算法最终能达到最优策略？
由于每次迭代都会改进策略，我们知道
$$
v_{\pi_0} \le v_{\pi_1} \le v_{\pi_2} \le \cdots \le v_{\pi_k} \le \cdots \le v^*.
$$
结果是，$v_{\pi_k}$ 持续增加并会收敛。仍然需要证明它收敛到 $v^*$。

**定理（策略迭代的收敛性）：**
由策略迭代算法生成的状态值序列 $\{v_{\pi_k}\}_{k=0}^\infty$ 收敛到最优状态值 $v^*$。结果是，策略序列 $\{\pi_k\}_{k=0}^\infty$ 收敛到一个最优策略。

**证明：**
证明的思路是表明策略迭代算法比值迭代算法收敛得更快。
特别是，为了证明 $\{v_{\pi_k}\}_{k=0}^\infty$ 的收敛性，我们引入由以下公式生成的另一个序列$\{v_k\}_{k=0}^\infty$：
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k).
$$
这个迭代算法正是值迭代算法。我们已经知道当给定任何初始值 $v_0$ 时，$v_k$ 会收敛到 $v^*$。
对于 $k=0$，我们总是可以找到一个 $v_0$ 使得对于任何 $\pi_0$，都有 $v_{\pi_0} \ge v_0$。
我们接下来通过归纳法证明对于所有 $k$，有 $v_{\pi_k} \le v^*$。
对于 $k \ge 0$，假设 $v_{\pi_k} \ge v_k$。
对于 $k+1$，我们有
$$
\begin{aligned}
v_{\pi_{k+1}} - v_{k+1} &= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) - \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k) \\
&\ge (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) - \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k) \\
&\quad (\text{因为 } v_{\pi_{k+1}} \ge v_{\pi_k} \text{ 根据策略改进引理和 } P_{\pi_{k+1}} \ge 0) \\
&= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k) \\
&\quad (\text{假设 } \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)) \\
&\ge (r_{\pi_{k+1}} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_k} + \gamma P_{\pi_k} v_k) \\
&\quad (\text{因为 } \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})) \\
&= \gamma P_{\pi_k} (v_{\pi_k} - v_k).
\end{aligned}
$$
由于 $v_{\pi_k} - v_k \ge 0$ 且 $P_{\pi_k}$ 是非负的，我们有 $P_{\pi_k}(v_{\pi_k} - v_k) \ge 0$，因此 $v_{\pi_{k+1}} - v_{k+1} \ge 0$。
因此，我们可以通过归纳法证明对于任何 $k \ge 0$，有 $v_k \le v_{\pi_k} \le v^*$。由于 $v_k$ 收敛到 $v^*$， $v_{\pi_k}$ 也收敛到 $v^*$。

对于步骤 1：策略评估
矩阵-向量形式：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0,1,2,...
$$
逐元素形式：
$$
v_{\pi_k}^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right), \quad s \in \mathcal{S}
$$
当 $j \to \infty$ 或 $j$ 足够大，或 $\|v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}\|$ 足够小时停止。

对于步骤 2：策略改进
矩阵-向量形式：
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
逐元素形式：
$$
\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s') \right)}_{q_{\pi_k}(s,a)}, \quad s \in \mathcal{S}
$$
这里，$q_{\pi_k}(s,a)$ 是在策略 $\pi_k$ 下的动作值。令
$$
a_k^*(s) = \arg \max_{a} q_{\pi_k}(a, s)
$$
那么，贪婪策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 & a = a_k^*(s) \\ 0 & a \neq a_k^*(s) \end{cases}
$$

**伪代码：策略迭代算法**
初始化：概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 对于所有的 $(s,a)$ 都是已知的。初始猜测 $\pi_{(0)}$。
目标：搜索最优状态值和一个最优策略。
当策略尚未收敛时，对于第 $k$ 次迭代，执行：
策略评估：
    初始化: 任意初始猜测 $v_{\pi_k}^{(0)}$
    当 $v_{\pi_k}^{(j)}$ 尚未收敛时，对于第 $j$ 次迭代，执行：
        对于每个状态 $s \in S$，执行：$v_{\pi_k}^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right]$
策略改进：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
	        $q_{\pi_k}(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s')$
	    $a_k^*(s) = \arg \max_{a} q_{\pi_k}(s,a)$
	    $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

## 4.3 截断迭代算法
比较值迭代和策略迭代：

|          | 策略迭代算法                                                        | 值迭代算法                                                   | 备注                                 |
| -------- | ------------------------------------------------------------- | ------------------------------------------------------- | ---------------------------------- |
| 1) 策略    | $\pi_0$                                                       | N/A                                                     |                                    |
| 2) 值     | $v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}$          | $v_0 := v_{\pi_0}$                                      |                                    |
| 3) 策略    | $\pi_1 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_0})$ | $\pi_1 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_0)$ | 两个策略是相同的                           |
| 4) 值     | $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$          | $v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0$                | $v_1 \ge v_0$ 因为 $\pi_1 \ge \pi_0$ |
| 5) 策略    | $\pi_2 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_1})$ | $\pi_2 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_1)$ |                                    |
| $\vdots$ | $\vdots$                                                      | $\vdots$                                                |                                    |
注意到：
* 它们从相同的初始条件开始。
* 前三个步骤是相同的。
* 第四个步骤变得不同：
    * 在策略迭代中，求解 $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$ 需要一个迭代算法（无限次迭代）
    * 在值迭代中，$v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0$ 是一步迭代


考虑求解 $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$ 的步骤：
$$
v_{\pi_1}^{(0)} = v_0
$$
值迭代 $\leftarrow v_1 \leftarrow v_{\pi_1}^{(1)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(0)}$
$$
v_{\pi_1}^{(2)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(1)}
$$
$$\vdots$$
截断策略迭代 $\leftarrow \bar{v_1} \leftarrow v_{\pi_1}^{(j)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(j-1)}$
$$\vdots$$
策略迭代 $\leftarrow v_{\pi_1} \leftarrow v_{\pi_1}^{(\infty)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(\infty)}$

注意到：
* 值迭代算法计算一次。
* 策略迭代算法计算无限次迭代。
* **截断策略迭代算法**计算有限次迭代（例如 $j$ 次）。从 $j$ 到 $\infty$ 的其余迭代都被截断。

**伪代码：截断策略迭代算法**
初始化：概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 对于所有的 $(s,a)$ 都是已知的。初始猜测 $\pi_0$。
目标：搜索最优状态值和一个最优策略。
当策略尚未收敛时，对于第 $k$ 次迭代，执行：
  策略评估：
    初始化: 选择初始猜测 $v_k^{(0)} = v_{k-1}$。最大迭代次数设置为 $j_{truncate}$。
    当 $j < j_{truncate}$ 时，执行：
        对于每个状态 $s \in S$，执行：
            $v_k^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k^{(j)}(s') \right]$
    设置 $v_k = v_k^{(j_{truncate})}$
  策略改进：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
            $q_k(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$
        $a_k^*(s) = \arg \max_{a} q_k(s,a)$
        $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

截断会破坏收敛性吗？

**命题（价值改进）**
考虑用于求解策略评估步骤的迭代算法：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0, 1, 2, \ldots
$$
如果初始猜测选择为 $v_{\pi_k}^{(0)} = v_{\pi_{k-1}}$，则有
$$
v_{\pi_k}^{(j+1)} \ge v_{\pi_k}^{(j)}
$$
对于每一个 $j = 0, 1, 2, \ldots$

**证明：**
首先，由于 $v_{\pi_k}^{(j)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j-1)}$ 和 $v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}$，我们有
$$v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)} = \gamma P_{\pi_k} (v_{\pi_k}^{(j)} - v_{\pi_k}^{(j-1)}) = \cdots = \gamma^j P_{\pi_k}^j (v_{\pi_k}^{(1)} - v_{\pi_k}^{(0)}). \quad (*)$$
其次，由于 $v_{\pi_k}^{(0)} = v_{\pi_{k-1}}$，我们有
$$v_{\pi_k}^{(1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(0)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_{k-1}} \ge r_{\pi_{k-1}} + \gamma P_{\pi_{k-1}} v_{\pi_{k-1}} = v_{\pi_{k-1}} = v_{\pi_k}^{(0)},$$
其中不等式是由于 $\pi_k = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_{k-1}})$。将 $v_{\pi_k}^{(1)} \ge v_{\pi_k}^{(0)}$ 代入 (\*) 得到 $v_{\pi_k}^{(j+1)} \ge v_{\pi_k}^{(j)}$。

# 5 蒙特卡洛迭代
## 5.1 蒙特卡洛估计
蒙特卡洛方法是一种通过随机抽样来估计数值结果的计算技术。它依赖于重复的随机抽样来获得概率结果，并利用这些结果来逼近真实值。这种方法特别适用于解决那些难以通过确定性算法或解析方法解决的问题，尤其是在涉及高维度、复杂模型或不确定性的情况下。

**大数定律：**
对于一个随机变量 $X$。假设 $\{x_j\}_{j=1}^N$ 是一些独立同分布的样本。令 $\bar{x} = \frac{1}{N}\sum_{j=1}^N x_j$ 为样本的平均值。那么，
$$
\begin{aligned}
\mathbb{E}[\bar{x}] &= \mathbb{E}[X], \\
\text{Var}[\bar{x}] &= \frac{1}{N}\text{Var}[X].
\end{aligned}
$$
因此，$\bar{x}$ 是 $\mathbb{E}[X]$ 的无偏估计，其方差随着 $N$ 趋于无穷而趋于零。
样本必须是 iid (独立同分布)。

**证明：**
首先，$\mathbb{E}[\bar{x}] = \mathbb{E}[\sum_{i=1}^n x_i/n] = \sum_{i=1}^n \mathbb{E}[x_i]/n = \mathbb{E}[X]$，其中最后一个等式是由于样本是同分布的（即 $\mathbb{E}[x_i] = \mathbb{E}[X]$）。
其次，$\text{var}(\bar{x}) = \text{var}(\sum_{i=1}^n x_i/n) = \sum_{i=1}^n \text{var}[x_i]/n^2 = (n \cdot \text{var}[X])/n^2 = \text{var}[X]/n$，其中第二个等式是由于样本是独立的，第三个等式是由于样本是同分布的（即 $\text{var}[x_i] = \text{var}[X]$）。

将策略迭代转换为无模型：
策略迭代在每次迭代中有两个步骤：
$$
\begin{cases}
\text{Policy Evaluation:}\; v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k} \\
\text{Policy Improvment:}\; \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
\end{cases}
$$
策略改进步骤的逐元素形式是：
$$
\begin{aligned}
\pi_{k+1}(s) &= \arg \max_{\pi} \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s') \right] \\
&= \arg \max_{\pi} \sum_{a} \pi(a|s) q_{\pi_k}(s,a), \quad s \in \mathcal{S}
\end{aligned}
$$
关键是 $q_{\pi_k}(s,a)$。

动作值的两种表达方式：
表达方式 1 需要模型：
$$
q_{\pi_k}(s, a) = \sum_{r} p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v_{\pi_k}(s')
$$
表达方式 2 不需要模型：
$$
q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]
$$
实现无模型强化学习的想法：我们可以使用表达方式 2 基于数据（样本或经验）来计算 $q_{\pi_k}(s,a)$。

动作值蒙特卡洛估计的步骤：
* 从 $(s, a)$ 开始，遵循策略 $\pi_k$，生成一个片段；
* 该片段的回报是 $g(s, a)$；
* $g(s, a)$ 是 $G_t$ 的一个样本，在$$q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]$$
* 假设我们有一组片段，因此有 $\{g^{(j)}(s, a)\}$。那么，$$q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a] \approx \frac{1}{N}\sum_{i=1}^N g^{(i)}(s, a).$$
基本思想：当模型不可用时，我们可以使用数据。

## 5.2 基本蒙特卡洛算法
基本蒙特卡洛算法描述：
给定一个初始策略 $\pi_0$，在第 $k$ 次迭代中有两个步骤。
* 步骤 1：策略评估。这一步是获取所有 $(s, a)$ 的 $q_{\pi_k}(s, a)$。具体来说，对于每个动作-状态对 $(s, a)$，运行无限多（或足够多）的片段。它们的平均回报用于近似 $q_{\pi_k}(s, a)$。
* 步骤 2：策略改进。这一步是求解
$$\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s)q_{\pi_k}(s, a) \text{ for all } s \in \mathcal{S}.$$
贪婪最优策略是 $\pi_{k+1}(a|s) = 1$，其中 $a_k^* = \arg \max_a q_{\pi_k}(s, a)$。

与策略迭代算法完全相同，除了：直接估计 $q_{\pi_k}(s, a)$，而不是求解 $v_{\pi_k}(s)$。

**伪代码：基本蒙特卡洛算法（策略迭代的无模型变体）**
初始化：初始猜测 $\pi_0$。
目标：搜索最优策略。
当价值估计尚未收敛时，对于第 $k$ 次迭代，执行：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
            收集足够多的从 $(s, a)$ 开始并遵循 $\pi_k$ 的片段
            基本蒙特卡洛策略评估步骤：
                $q_{\pi_k}(s, a) = \text{所有从 } (s, a) \text{ 开始的片段的平均回报}$
            策略改进步骤：
                $a_k^*(s) = \arg \max_a q_{\pi_k}(s, a)$
                $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

注意：
* 基本蒙特卡洛算法是策略迭代算法的一个变体。
* 无模型算法是在有模型算法的基础上建立起来的。因此，在学习无模型算法之前，有必要先理解有模型算法。
* 基本蒙特卡洛算法有助于揭示基于 MC 的无模型强化学习的核心思想，但由于效率低下而不实用。
* 为什么基本蒙特卡洛算法估计动作值而不是状态值？那是因为状态值不能直接用于改进策略。当模型不可用时，我们应该直接估计动作值。
* 由于策略迭代是收敛的，因此基本蒙特卡洛算法的收敛性在给定足够片段的情况下也保证是收敛的。

对于算法采样的的片段长度参数来说：
* 当片段长度较短时，只有接近目标的那些状态才具有非零状态值。
* 随着片段长度的增加，接近目标的状态比远离目标的状态更早地具有非零值。
* 片段长度应该足够长。
* 片段长度不必是无限长。

## 5.2 探索开始的蒙特卡洛算法
考虑一个网格世界示例，遵循策略 $\pi$，我们可以得到一个片段，例如
$$s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots$$
访问：每次一个状态-动作对出现在片段中，它被称为对该状态-动作对的一次**访问**。
使用数据的方法：首次访问法
* 只计算回报并近似 $q_{\pi}(s_1, a_2)$。
* 这就是基本蒙特卡洛算法所做的。
* 缺点：未充分利用数据。

该片段也访问了其他状态-动作对。
$$
\begin{aligned}
s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{原始片段}] \\
s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_2, a_4) \text{ 开始的片段}] \\
s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_1, a_2) \text{ 开始的片段}] \\
s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_2, a_3) \text{ 开始的片段}] \\
s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_5, a_1) \text{ 开始的片段}]
\end{aligned}
$$
可以估计 $q_{\pi}(s_1, a_2)$, $q_{\pi}(s_2, a_4)$, $q_{\pi}(s_2, a_3)$, $q_{\pi}(s_5, a_1)$, ......
数据高效的方法：
* 首次访问法
* 每次访问法

基于蒙特卡洛方法的强化学习的另一个方面是何时更新策略。有两种方法：
* 第一种方法是在策略评估步骤中，收集所有从一个状态-动作对开始的片段，然后使用平均回报来近似动作值。
    * 这是基本蒙特卡洛算法所采用的方法。
    * 这种方法的问题是，智能体必须等到所有片段都收集完毕。
* 第二种方法使用单个片段的回报来近似动作值。
    * 这样，我们可以逐片段地改进策略。

第二种方法会引发问题吗？
* 有人可能会说单个片段的回报不能准确近似相应的动作值。
* 事实上，我们在之前介绍的截断策略迭代算法中已经做到了这一点！
广义策略迭代：
* 不是一个具体的算法。
* 它指的是在策略评估和策略改进过程之间切换的普遍思想或框架。
* 许多基于模型和无模型的强化学习算法都属于这个框架。

如果我们使用数据并更有效地更新估计，我们得到一个新算法，称为 MC 探索开始：

**伪代码：探索开始的蒙特卡洛算法（基本蒙特卡洛算法的样本高效变体）**
初始化：初始猜测 $\pi_0$。
目标：搜索最优策略。
对于每个片段，执行：
片段生成：随机选择一个起始状态-动作对 $(s_0, a_0)$ 并确保所有对都可以被可能地选择。遵循当前策略，生成一个长度为 $T$ 的片段：$s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。
策略评估和策略改进：
    初始化：$g \leftarrow 0$
    对于片段的每个步骤，$t = T - 1, T - 2, \dots, 0$，执行：
        $g \leftarrow \gamma g + r_{t+1}$
        使用首次访问策略：
        如果 $(s_t, a_t)$ 未出现在 $(s_0, a_0, s_1, a_1, \dots, s_{t-1}, a_{t-1})$ 中，则
            $Returns(s_t, a_t) \leftarrow Returns(s_t, a_t) + g$
            $q(s_t, a_t) = \text{average}(Returns(s_t, a_t))$
            $\pi(a|s_t) = 1 \text{ if } a = \arg \max_a q(s_t, a)$


## 5.3 $\varepsilon$-贪婪蒙特卡洛算法
为什么要考虑探索开始？
* 理论上，只有当每个状态的每个动作值都得到充分探索时，我们才能正确选择最优动作。
  相反，如果一个动作没有被探索，这个动作可能恰好是最优的，从而被错过。
* 实践中，探索开始很难实现。对于许多应用，特别是那些涉及与环境进行物理交互的应用，很难收集从每个状态-动作对开始的片段。

因此，理论与实践之间存在差距。
我们能移除探索开始的要求吗？我们接下来将展示通过使用软策略可以做到这一点。

当采取任何动作的概率都为正时，一个策略被称为**软**策略。
为什么要引入软策略？
* 使用软策略，一些足够长的片段可以足够多次地访问每个状态-动作对。
* 这样，我们就不需要有大量从每个状态-动作对开始的片段。因此，探索开始的要求可以被移除。

我们将使用哪种软策略？答案：$\varepsilon$-贪婪策略
什么是 $\varepsilon$-贪婪策略？
$$\pi(a|s) = \begin{cases} 1 - \frac{\varepsilon}{|A(s)|}(|A(s)| - 1), & \text{for the greedy action,} \\ \\ \frac{\varepsilon}{|A(s)|}, & \text{for th eother } |A(s)| - 1 \text{ actions.} \end{cases}$$
其中 $\varepsilon \in [0, 1]$ 且 $|A(s)|$ 是状态 $s$ 的动作数量。

注意：
* 选择贪婪动作的机会总是大于其他动作，因为 $1 - \frac{\varepsilon}{|A(s)|}(|A(s)| - 1) = 1 - \varepsilon + \frac{\varepsilon}{|A(s)|} \ge \frac{\varepsilon}{|A(s)|}$。
* 为什么要使用 $\varepsilon$-贪婪？平衡探索（exploration）和利用（expoitation）。
* 当 $\varepsilon = 0$ 时，它变成贪婪策略。探索较少但利用较多。
* 当 $\varepsilon = 1$ 时，它变成均匀分布。探索较多但利用较少。


如何将 $\varepsilon$-贪婪嵌入到基于蒙特卡洛的强化学习算法中？
最初，基本蒙特卡洛算法和探索开始蒙特卡洛算法中的策略改进步骤是求解
$$
\pi_{k+1}(s) = \arg \max_{\pi \in \Pi} \sum_{a} \pi(a|s)q_{\pi_k}(s, a),
$$
其中 $\Pi$ 表示所有可能策略的集合。这里最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1, & a = a_k^*; \\ 0, & a \neq a_k^*, \end{cases}
$$
其中 $a_k^* = \arg \max_a q_{\pi_k}(s, a)$。

现在，策略改进步骤被修改为求解
$$
\pi_{k+1}(s) = \arg \max_{\pi \in \Pi_{\varepsilon}} \sum_{a} \pi(a|s)q_{\pi_k}(s, a),
$$
其中 $\Pi_{\varepsilon}$ 表示具有固定 $\varepsilon$ 值的**所有 $\varepsilon$-贪婪策略**的集合。这里最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 - \frac{|A(s)|-1}{|A(s)|}\varepsilon, & a = a_k^*; \\ \frac{1}{|A(s)|}\varepsilon, & a \neq a_k^*. \end{cases}
$$
注意：
* $\varepsilon$-贪婪蒙特卡洛算法与探索开始蒙特卡洛算法相同，除了前者使用 $\varepsilon$-贪婪策略。
* 它不要求探索开始，但仍然需要以不同的形式访问所有状态-动作对。


**伪代码：$\varepsilon$-贪婪蒙特卡洛算法（探索开始的蒙特卡洛算法的变体）**
初始化：初始猜测 $\pi_0$ 和 $\varepsilon \in [0,1]$ 的值。
目标：搜索最优策略。
对于每个片段，执行：
片段生成：随机选择一个起始状态-动作对 $(s_0, a_0)$。遵循当前策略，生成一个长度为 $T$ 的片段：$s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。
策略评估和策略改进：
    初始化: $g \leftarrow 0$
    对于片段的每个步骤，$t = T - 1, T - 2, \dots, 0$，执行：
        $g \leftarrow \gamma g + r_{t+1}$
        使用每次访问法：
        如果 $(s_t, a_t)$ 未出现在 $(s_0, a_0, s_1, a_1, \dots, s_{t-1}, a_{t-1})$ 中，则
            $Returns(s_t, a_t) \leftarrow Returns(s_t, a_t) + g$
            $q(s_t, a_t) = \text{average}(Returns(s_t, a_t))$
            令 $a^* = \arg \max_a q(s_t, a)$并且$$\pi(a|s_t) = \begin{cases} 1 - \frac{|A(s_t)|-1}{|A(s_t)|}\varepsilon, & a = a^* \\ \frac{1}{|A(s_t)|}\varepsilon, & a \neq a^* \end{cases}$$

与贪婪策略相比，
* $\varepsilon$-贪婪策略的优点在于它们具有更强的探索能力，因此不需要探索开始条件。
* 缺点在于 $\varepsilon$-贪婪策略通常不是最优的（我们只能证明总是存在最优的贪婪策略）。
    * $\varepsilon$-贪婪蒙特卡洛算法给出的最终策略只在所有 $\varepsilon$-贪婪策略的集合 $\Pi_{\varepsilon}$ 中是最优的。
* $\varepsilon$ 不能太大。

# 6 随机近似与随机梯度下降

