# 1 基本概念
## 1.1 概念
状态、状态空间；动作、动作空间；
状态转移：当采取一个动作，智能体从一个状态转移到另一个状态。
策略：智能体在一个状态应该采取什么动作。

奖励：智能体采取动作后获得的一个实数值，正为鼓励，负为惩罚。
轨迹：一条轨迹是一个状态-动作-奖励链。

## 1.2 马尔可夫决策过程
马尔科夫决策过程（MDP）：
- 状态空间：$S$
- 动作空间：$A(s),\; s \in S$
- 奖励：$R(s, a),\; s \in S,\; a \in A(s)$
- 状态转移概率：$p(s'\;| s,\; a),\; s \in S,\; a \in A(s)$
- 策略：$\pi (a\;|s),\; s \in S,\; a \in A(s)$
马尔可夫性质：未来状态的条件转移概率只依赖于当前的状态和动作，与过去的状态和动作是独立的。

# 2 贝尔曼方程
## 2.1 确定性问题的贝尔曼方程
确定性问题的贝尔曼公式：
$$
\boldsymbol{v} = \boldsymbol{r} + \gamma \boldsymbol{P}\boldsymbol{v}
$$
其中 $\boldsymbol{r}$ 为奖励，$\boldsymbol{P}$ 为状态转移矩阵，$\boldsymbol{v}$ 为状态价值。
表明一个状态的价值依赖于其他状态的价值。

## 2.2 随机性问题的折扣回报
对于随机性问题：
$$
S_t \xrightarrow{A_t} R_{t + 1},\; S_{t + 1}
$$
- $S_t, A_t, R_{t + 1}$ 均为随机变量。
- $S_t \to A_t$ 依赖于 $\pi (A_t = a | S_t = s)$，即策略函数。
- $S_t, A_t \to R_{t + 1}$ 依赖于 $p(R_{t + 1} = r| S_t=s, A_t = a)$。
- $S_t,A_t \to S_{t + 1}$ 依赖于 $p(S_{t + 1}=s'|S_t = s, A_t = a)$。

考虑以下多步轨迹：

$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \ldots$$
折扣回报定义为：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

其中 $\gamma \in [0, 1)$ 是折扣率，$G_t$ 也是一个随机变量，因为 $R_{t+1}, R_{t+2}, \ldots$ 都是随机变量。
折扣回报通过折扣因子 $\gamma$ 降低了未来奖励的权重，使得智能体更关注近期奖励。较小的 $\gamma$ 值会使智能体更加"近视"，而接近1的 $\gamma$ 值则使智能体更加"远视"。

## 2.3 状态价值函数
状态价值函数（或简称状态价值）是折扣回报 $G_t$ 的期望值（也称为期望价值或均值），定义为：

$$v_{\pi}(s) = \mathbb{E}[G_t|S_t = s]$$

- 状态价值是关于状态 $s$ 的函数，表示在给定起始状态 $s$ 的条件下的条件期望。
- 状态价值基于策略 $\pi$。不同的策略可能导致不同的状态价值。
- 状态价值代表一个状态的"价值"。状态价值越大，说明该策略越好，因为能获得更高的累积奖励。

## 2.4 推导贝尔曼方程

考虑一个随机轨迹：
$$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, \dots$$

回报 $G_t$ 可以写成：
$$
\begin{aligned}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\ 
    & = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
    & = R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$
根据状态价值函数的定义，有：
$$
\begin{aligned}
v_\pi(s) & = \mathbb{E}_\pi [G_t | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] \\
		 & = \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
\end{aligned}
$$

接下来，分别计算这两个期望。
首先，计算第一项 $\mathbb{E}_\pi [R_{t+1} | S_t = s]$（这是即时奖励的均值）： 
$$ \begin{aligned} \mathbb{E}_\pi [R_{t+1} | S_t = s] &= \sum_a \pi(a|s) \mathbb{E} [R_{t+1} | S_t = s, A_t = a] \\ &= \sum_a \pi(a|s) \sum_{r} p(r|s, a) r \end{aligned} $$
接下来，计算第二项 $\mathbb{E}_\pi [G_{t+1} | S_t = s]$（这是未来回报的均值）：
$$
\begin{aligned}
\mathbb{E}_\pi [G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_t = s, S_{t+1} = s'] p(s'|s) \\
&= \sum_{s'} \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s'] p(s'|s) \text{（由于马尔可夫性质）}\\
&= \sum_{s'} v_\pi(s') p(s'|s) \\
&= \sum_{s'} v_\pi(s') \sum_a p(s'|s, a) \pi(a|s)
\end{aligned}
$$
因此，我们得到：
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi [R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s] \\
&= \underbrace{\sum_a \pi(a|s) \sum_r p(r|s, a) r}_{\text{即时奖励的均值}} + \gamma \underbrace{\sum_a \pi(a|s) \sum_{s'} p(s'|s, a) v_\pi(s')}_{\text{未来奖励的均值}} \\
&= \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right], \quad \forall s \in \mathcal{S}
\end{aligned}
$$
- $v_\pi(s)$ 和 $v_\pi(s')$ 是需要计算的状态价值。
- $\pi(a|s)$ 是给定的策略。求解这个方程被称为**策略评估 (policy evaluation)**。
- $p(r|s, a)$ 和 $p(s'|s, a)$ 代表环境。

上述方程被称为**贝尔曼方程**，描述了不同状态的状态价值函数之间的关系，由两项组成：即时奖励项和未来奖励项。这是一个方程组：每个状态都有一个这样的方程。

## 2.5 贝尔曼方程的向量形式
假设状态可以被索引为 $s_i$ ($i = 1, \dots, n$)。对于状态 $s_i$，贝尔曼方程是： $$v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)$$将所有状态的这些方程放在一起，并改写为矩阵向量形式： $$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$其中： 
- $\mathbf{v}_\pi = [v_\pi(s_1), \dots, v_\pi(s_n)]^T \in \mathbb{R}^n$ 是价值向量
- $\mathbf{r}_\pi = [r_\pi(s_1), \dots, r_\pi(s_n)]^T \in \mathbb{R}^n$ 是奖励向量，$r_\pi(s) = \sum_a \pi(a|s) \sum_r p(r|s, a) r$
- $\mathbf{P}_\pi \in \mathbb{R}^{n \times n}$ 是状态转移矩阵，$[\mathbf{P}_\pi]_{ij} = p_\pi(s_j|s_i) = \sum_a \pi(a|s_i) p(s_j|s_i, a)$

## 2.6 求解状态价值

贝尔曼方程的矩阵向量形式是：
$$ \mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi $$

闭式解 (The closed-form solution) 是：
$$ \mathbf{v}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi $$
在实践中，我们仍然需要使用数值工具来计算矩阵的逆。

通过迭代算法可以避免矩阵求逆运算，一个迭代解 (An iterative solution) 是：
$$ \mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k $$
这个算法产生一个序列 $\{\mathbf{v}_0, \mathbf{v}_1, \mathbf{v}_2, \dots\}$。可以证明：
$$ \mathbf{v}_k \rightarrow (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi, \quad k \rightarrow \infty $$

**证明：**
定义误差为 $\delta_k = \mathbf{v}_k - \mathbf{v}_\pi$。我们只需要证明当 $k \rightarrow \infty$ 时，$\delta_k \rightarrow \mathbf{0}$。

将 $\mathbf{v}_{k+1} = \delta_{k+1} + \mathbf{v}_\pi$ 和 $\mathbf{v}_k = \delta_k + \mathbf{v}_\pi$ 代入迭代公式 $\mathbf{v}_{k+1} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_k$，得到：
$$
\begin{aligned}
\delta_{k+1} + \mathbf{v}_\pi 
&= \mathbf{r}_\pi + \gamma \mathbf{P}_\pi (\delta_k + \mathbf{v}_\pi) \\
& = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \delta_k + \gamma \mathbf{P}_\pi \mathbf{v}_\pi 
\end{aligned}
$$
由于 $\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi$，上式可以改写为：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k $$
因此，我们可以得到：
$$ \delta_{k+1} = \gamma \mathbf{P}_\pi \delta_k = \gamma^2 \mathbf{P}_\pi^2 \delta_{k-1} = \dots = \gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 $$
注意 $0 \le [\mathbf{P}_\pi]_{ij} \le 1$，这表示 $\mathbf{P}_\pi$ 的每一行的元素都是非负的且和为 1。这是因为 $\mathbf{P}_\pi$ 是状态转移矩阵。因此，$\mathbf{P}_\pi \mathbf{1} = \mathbf{1}$，其中 $\mathbf{1} = [1, 1, \dots, 1]^T$。
另一方面，由于 $\gamma < 1$，我们知道 $\gamma^k \rightarrow 0$，因此 $\gamma^{k+1} \mathbf{P}_\pi^{k+1} \delta_0 \rightarrow \mathbf{0}$ 当 $k \rightarrow \infty$ 时。
因此，$\delta_k \rightarrow \mathbf{0}$，这意味着 $\mathbf{v}_k \rightarrow \mathbf{v}_\pi$ 当 $k \rightarrow \infty$ 时。

## 2.7 动作价值函数
状态价值：从一个状态开始，智能体能够获得的平均回报。
动作价值：从一个状态开始并采取一个动作，智能体能够获得的平均回报。

定义：
$$ q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
* $q_\pi(s, a)$ 是状态-动作对 $(s, a)$ 的函数。
* $q_\pi(s, a)$ 依赖于策略 $\pi$。

根据条件期望的性质，有：
$$
\underbrace{\mathbb{E}[G_t | S_t = s]}_{v_\pi(s)} = \sum_a \underbrace{\mathbb{E}[G_t | S_t = s, A_t = a]}_{q_\pi(s,a)} \pi(a|s)
$$
因此，
$$ v_\pi(s) = \sum_a \pi(a|s) q_\pi(s, a) \quad (1) $$
回顾状态价值函数由下式给出：
$$ v_\pi(s) = \sum_a \pi(a|s) \left[ \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \right] \quad (2) $$

通过比较 (1) 和 (2)，我们得到**动作价值函数**为：
$$ q_\pi(s, a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_\pi(s') \quad (3) $$
(1) 和 (3) 是同一枚硬币的两面：
* (1) 展示了如何从动作价值函数获得状态价值函数。
* (3) 展示了如何从状态价值函数获得动作价值函数。

# 3 贝尔曼最优方程
## 3.1 最优策略
状态值函数可以用来评估一个策略的好坏：如果对于所有状态 $s \in \mathcal{S}$，都有 
$$
v_{\pi_1}(s) \ge v_{\pi_2}(s),
$$
那么我们认为策略 $\pi_1$ 比策略 $\pi_2$ “更好”。 

一个策略 $\pi^*$ 被称为**最优策略**，如果对于所有状态 $s \in \mathcal{S}$ 和任何其他策略 $\pi$，都有
$$
v_{\pi^*}(s) \ge v_{\pi}(s).
$$

## 3.2 贝尔曼最优方程
贝尔曼最优方程（元素形式）：
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
也可以写成：
$$
v_*(s) = \max_{a} q_*(s, a), \quad s \in \mathcal{S}
$$
其中，
$$
q_*(s, a) = \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right)
$$
是最优状态-动作值函数。
* $p(r|s, a)$ 和 $p(s'|s, a)$ 是已知的。
* $v_*(s)$ 和 $v_*(s')$ 是未知的，需要计算。
* 在最优性方程中，我们正在寻找最优策略，通常情况下 $\pi(a|s)$ 是我们想要确定的。

贝尔曼最优性方程（矩阵-向量形式）：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
其中，对应于状态 $s$ 或 $s'$ 的元素是：
$$
[\mathbf{r}_{\boldsymbol{\pi}}]_s \triangleq \sum_a \pi(a|s) \sum_{s'} p(s'|s, a) r(s, a, s')
$$
$$
[\mathbf{P}_{\boldsymbol{\pi}}]_{s, s'} \triangleq \sum_a \pi(a|s) p(s'|s, a)
$$
这里的 $\max_{\boldsymbol{\pi}}$ 是**按元素**取最大值。

对于贝尔曼最优方程：
* **算法 (Algorithm):** 如何求解这个方程？
* **存在性 (Existence):** 这个方程是否有解？
* **唯一性 (Uniqueness):** 这个方程的解是唯一的吗？
* **最优性 (Optimality):** 它与最优策略是如何相关的？

## 3.3 贝尔曼最优方程右侧的最大化
贝尔曼最优方程: 元素形式
$$
v_*(s) = \max_{a} \sum_{s'} p(s'|s, a) \left( r(s, a, s') + \gamma v_*(s') \right), \quad \forall s \in \mathcal{S}
$$
贝尔曼最优方程: 矩阵-向量形式
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
首先固定 $v'(s)$ 并求解 $\pi$：
$$
\begin{aligned}
v(s) 
&= \max_{\pi} \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v'(s')) \right) \\
&= \max_{\pi} \sum_a \pi(a|s) q(s, a), \quad \forall s \in \mathcal{S}
\end{aligned}
$$
考虑到 $\sum_a \pi(a|s) = 1$，我们有：
$$
\max_{\pi} \sum_a \pi(a|s) q(s, a) = \max_{a \in \mathcal{A}(s)} q(s, a)
$$
最优性在以下情况下取得：
$$
\pi(a|s) =
\begin{cases}
1 & \text{如果 } a = a^* \\
0 & \text{如果 } a \neq a^*
\end{cases}
$$
其中 $a^* = \arg \max_a q(s, a)$。

## 3.4 求解贝尔曼最优性方程

贝尔曼最优性方程是：
$$
\mathbf{v}_* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*).
$$
令：

$$
f(\mathbf{v}) := \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
那么，贝尔曼最优性方程可以写成：
$$
\mathbf{v}_* = f(\mathbf{v}_*)
$$
其中，$f(\mathbf{v})$ 的第 $s$ 个元素是：
$$
[f(\mathbf{v})]_s = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v(s')) = \max_a q(s, a)
$$
### 3.4.1 预备知识：压缩映射定理
#### 3.4.1.1 一些概念
**不动点 (Fixed point):** $x \in \mathcal{X}$ 是函数 $f: \mathcal{X} \rightarrow \mathcal{X}$ 的一个不动点，如果：$$f(x) = x$$

**压缩映射 (Contraction mapping or contractive function)：** 函数 $f$ 是一个压缩映射，如果存在一个常数 $\gamma \in [0, 1)$，使得对于所有 $x_1, x_2 \in \mathcal{X}$，都有：$$\|f(x_1) - f(x_2)\| \le \gamma \|x_1 - x_2\|$$其中：
- $\gamma \in [0, 1)$。
- $\gamma$ 必须严格小于 1，这样才能保证许多极限成立，例如当 $k \rightarrow \infty$ 时，$\gamma^k \rightarrow 0$。
- $\|\cdot\|$ 可以是任何向量范数。

#### 3.4.1.2 压缩映射定理
对于任何形式为 $x = f(x)$ 的方程，如果 $f$ 是一个压缩映射，那么：
* **存在性：** 存在一个不动点 $x^*$ 满足 $f(x^*) = x^*$。
* **唯一性：** 这个不动点 $x^*$ 是唯一的。
* **算法：** 考虑一个序列 $\{x_k\}$，其中 $x_{k+1} = f(x_k)$，那么 $x_k \rightarrow x^*$ 当 $k \rightarrow \infty$。此外，收敛速度是指数级的。

#### 3.1.4.3 压缩映射定理的证明
**第一部分：我们证明序列 $\{x_k\}_{k=1}^\infty$，其中 $x_k = f(x_{k-1})$ 是收敛的。**
这个证明依赖于柯西序列（Cauchy sequence）。
一个序列 $x_1, x_2, ... \in \mathbb{R}$ 被称为柯西序列，如果对于任意小的 $\epsilon > 0$，存在一个正整数 $N$ 使得对于所有的 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。
直观的理解是，存在一个有限的整数 $N$，使得所有在 $N$ 之后的元素都充分地靠近彼此。
柯西序列很重要，因为可以保证一个柯西序列收敛到一个极限。它的收敛性质将被用于证明压缩映射定理。
注意，我们必须有 $||x_m - x_n|| < \epsilon$ 对于所有 $m, n > N$。如果我们仅仅有 $x_{n+1} - x_n \to 0$，不足以声明该序列是一个柯西序列。例如，对于 $x_n = \sqrt{n}$，有 $x_{n+1} - x_n \to 0$，但是显然 $x_n = \sqrt{n}$ 是发散的。
我们接下来证明 $\{x_k = f(x_{k-1})\}_{k=1}^\infty$ 是一个柯西序列，因此是收敛的。
首先，由于 $f$ 是一个压缩映射，我们有
$$||x_{k+1} - x_k|| = ||f(x_k) - f(x_{k-1})|| \le \gamma ||x_k - x_{k-1}||.$$
类似地，我们有 $||x_k - x_{k-1}|| \le \gamma ||x_{k-1} - x_{k-2}||, ..., ||x_2 - x_1|| \le \gamma ||x_1 - x_0||$。
因此，我们有
$$
\begin{align*} 
||x_{k+1} - x_k|| 
&\le \gamma ||x_k - x_{k-1}|| \\ 
&\le \gamma^2 ||x_{k-1} - x_{k-2}|| \\ 
&\vdots \\ 
&\le \gamma^k ||x_1 - x_0||. 
\end{align*}
$$
由于 $\gamma < 1$，我们知道 $||x_{k+1} - x_k||$ 以指数速度收敛到零，当 $k \to \infty$ 时，给定任意的 $x_1, x_0$。特别地，$\{|x_{k+1} - x_k|\}$ 的收敛性不足以推出 $\{x_k\}$ 的收敛性。因此，我们需要进一步考虑对于任意 $m > n$ 的 $||x_m - x_n||$。特别地，
$$
\begin{align*} 
||x_m - x_n|| 
&= ||x_m - x_{m-1} + x_{m-1} - ... - x_{n+1} + x_{n+1} - x_n|| \\ 
&\le ||x_m - x_{m-1}|| + ||x_{m-1} - x_{m-2}|| + ... + ||x_{n+1} - x_n|| \\ 
&\le \gamma^{m-1} ||x_1 - x_0|| + ... + \gamma^n ||x_1 - x_0|| \\ 
&= \gamma^n (\gamma^{m-n-1} + ... + 1) ||x_1 - x_0|| \\ 
&= \gamma^n (1 + \gamma + ... + \gamma^{m-n-1}) ||x_1 - x_0|| \\ 
&= \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.
\end{align*}
$$
因此，对于任意 $\epsilon$，我们总能找到一个 $N$ 使得对于所有 $m, n > N$，都有 $||x_m - x_n|| < \epsilon$。因此，这个序列是柯西序列，并且收敛到一个极限点，记为 $x^* = \lim_{k \to \infty} x_k$。

**第二部分：我们证明极限 $x^* = \lim_{k \to \infty} x_k$ 是一个不动点。**
为了做到这一点，由于
$$||f(x_k) - x_k|| = ||x_{k+1} - x_k|| \le \gamma^k ||x_1 - x_0||,$$
我们知道 $||f(x_k) - x_k||$ 以指数速度收敛到零。因此，在极限情况下，我们有 $f(x^*) = x^*$。

**第三部分：我们证明不动点是唯一的。**
假设存在另一个不动点 $x'$ 满足 $f(x') = x'$。那么，
$$||x' - x^*|| = ||f(x') - f(x^*)|| \le \gamma ||x' - x^*||.$$
由于 $\gamma < 1$，这个不等式成立当且仅当 $||x' - x^*|| = 0$。因此，$x' = x^*$。

**第四部分：我们证明 $x_k$ 以指数速度收敛到 $x^*$。**
回想一下，正如上述证明的，$||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||$。由于 $m$ 可以任意大，我们有
$$||x^* - x_n|| = \lim_{m \to \infty} ||x_m - x_n|| \le \frac{\gamma^n}{1 - \gamma} ||x_1 - x_0||.$$
由于 $\gamma < 1$，当 $n \to \infty$ 时，误差以指数速度收敛到零。

### 3.4.2 贝尔曼最优性方程的压缩性质
让我们回到贝尔曼最优性方程：
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*)
$$
**定理 (压缩性质 - Contraction Property)**
$f(\mathbf{v})$ 是一个压缩映射，满足：
$$
\|f(\mathbf{v}_1) - f(\mathbf{v}_2)\| \le \gamma \|\mathbf{v}_1 - \mathbf{v}_2\|
$$
其中 $\gamma$ 是折扣率（discount rate），且 $\gamma \in [0, 1)$。

**证明：**
考虑任意两个向量 $v_1, v_2 \in \mathbb{R}^{|S|}$，
假设 $\pi_1^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_1)$ 并且 $\pi_2^* \doteq \arg \max_{\pi} (r_\pi + \gamma P_\pi v_2)$。那么，
$$
\begin{aligned}
f(v_1) &= \max_{\pi} (r_\pi + \gamma P_\pi v_1) = r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 \ge r_{\pi_2^*} + \gamma P_{\pi_2^*} v_1, \\
f(v_2) &= \max_{\pi} (r_\pi + \gamma P_\pi v_2) = r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2 \ge r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2,
\end{aligned}
$$
其中 $\ge$ 是按元素比较。因此，
$$
\begin{aligned}
f(v_1) - f(v_2) &= r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2) \\
&\le r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2) \\
&= \gamma P_{\pi_1^*} (v_1 - v_2).
\end{aligned}
$$
类似地，可以证明 $f(v_2) - f(v_1) \le \gamma P_{\pi_2^*} (v_2 - v_1)$。因此，
$$
\gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2).
$$
定义
$$
z \doteq \max \{|\gamma P_{\pi_2^*} (v_1 - v_2)|, |\gamma P_{\pi_1^*} (v_1 - v_2)|\} \in \mathbb{R}^{|S|},
$$
其中 $\max\{\cdot, \cdot\}$，$|\cdot|$，以及 $\ge$ 都是按元素操作。根据定义，$z \ge 0$。一方面，很容易看出
$$
-z \le \gamma P_{\pi_2^*} (v_1 - v_2) \le f(v_1) - f(v_2) \le \gamma P_{\pi_1^*} (v_1 - v_2) \le z,
$$
这蕴含着
$$
|f(v_1) - f(v_2)| \le z.
$$
由此可得
$$
\|f(v_1) - f(v_2)\|_\infty \le \|z\|_\infty. \quad (*)
$$
其中 $\|\cdot\|_\infty$ 是最大范数。另一方面，假设 $z_i$ 是 $z$ 的第 $i$ 个元素，并且 $p_i^T$ 和 $q_i^T$ 分别是 $P_{\pi_1^*}$ 和 $P_{\pi_2^*}$ 的第 $i$ 行。那么，
$$
z_i = \max \{|\gamma p_i^T (v_1 - v_2)|, |\gamma q_i^T (v_1 - v_2)|\}.
$$
由于 $p_i$ 是一个所有元素非负且元素之和等于 1 的向量，因此有
$$
|p_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty.
$$
类似地，我们有 $|q_i^T (v_1 - v_2)| \le \|v_1 - v_2\|_\infty$。因此，$z_i \le \gamma \|v_1 - v_2\|_\infty$，从而
$$
\|z\|_\infty = \max_i |z_i| \le \gamma \|v_1 - v_2\|_\infty.
$$
将此不等式代入 (\*) 得到
$$
\|f(v_1) - f(v_2)\|_\infty \le \gamma \|v_1 - v_2\|_\infty,
$$
这证明了 $f(v)$ 的压缩性质。

应用压缩映射定理可以得到以下结果：
**定理 (存在性、唯一性和算法 - Existence, Uniqueness, and Algorithm)**
对于贝尔曼最优性方程
$$
\mathbf{v}_* = f(\mathbf{v}_*) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_*),
$$
总是存在一个解 $\mathbf{v}_*$ 并且解是唯一的。这个解可以通过迭代的方式求解：
$$
\mathbf{v}_{k+1} = f(\mathbf{v}_k) = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_k)
$$
这个序列 $\{\mathbf{v}_k\}$ 以指数级的速度收敛到 $\mathbf{v}_*$，给定任何初始猜测 $\mathbf{v}_0$。收敛速度由 $\gamma$ 决定。


## 3.5 策略最优性
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程的解。它满足：
$$
\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
假设存在一个策略 $\boldsymbol{\pi}^*$，对于所有状态 $s$，它都能达到上述最大值，即：
$$
\boldsymbol{\pi}^* = \arg \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)
$$
那么，对于这个特定的策略 $\boldsymbol{\pi}^*$，我们有：
$$
\mathbf{v}^* = \mathbf{r}_{\boldsymbol{\pi}^*} + \gamma \mathbf{P}_{\boldsymbol{\pi}^*} \mathbf{v}^*
$$
因此，$\boldsymbol{\pi}^*$ 是一个策略，且 $\mathbf{v}^*$ 是其对应的状态值函数。

问题：$\boldsymbol{\pi}^*$ 是最优策略吗？$\mathbf{v}^*$ 是可以达到的最大的状态值吗？

**定理 (策略最优性 - Policy Optimality Theorem)**
假设 $\mathbf{v}^*$ 是贝尔曼最优性方程 
$$
\mathbf{v} = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v})
$$
的唯一解，并且对于任意给定的策略 $\boldsymbol{\pi}$，其状态值函数 $\mathbf{v}_{\boldsymbol{\pi}}$ 满足贝尔曼期望方程 $\mathbf{v}_{\boldsymbol{\pi}} = \mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}_{\boldsymbol{\pi}}$，那么：
$$
\mathbf{v}^* \ge \mathbf{v}_{\boldsymbol{\pi}}, \quad \forall \boldsymbol{\pi}
$$
**证明：**
对于任意策略 $\pi$，有
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi.
$$
由于
$$
v^* = \max_{\pi} (r_\pi + \gamma P_\pi v^*) = r_{\pi^*} + \gamma P_{\pi^*} v^* \ge r_\pi + \gamma P_\pi v^*,
$$
我们有
$$
v^* - v_\pi \ge (r_\pi + \gamma P_\pi v^*) - (r_\pi + \gamma P_\pi v_\pi) = \gamma P_\pi (v^* - v_\pi).
$$
重复应用上述不等式得到 $v^* - v_\pi \ge \gamma P_\pi (v^* - v_\pi) \ge \gamma^2 P_\pi^2 (v^* - v_\pi) \ge \cdots \ge \gamma^n P_\pi^n (v^* - v_\pi)$。由此可知
$$
v^* - v_\pi \ge \lim_{n \to \infty} \gamma^n P_\pi^n (v^* - v_\pi) = 0,
$$
其中最后一个等式成立是因为 $\gamma < 1$ 且 $P_\pi$ 是一个非负矩阵，其所有元素小于等于 1（因为 $P_\pi \mathbf{1} = \mathbf{1}$）。因此，$v^* \ge v_\pi$ 对于任意 $\pi$ 成立。


**定理 (贪婪最优策略 - Greedy Optimal Policy Theorem)**
对于任何状态 $s \in \mathcal{S}$，确定性的贪婪策略 $\boldsymbol{\pi}^*$ 定义为：
$$
\pi^*(a|s) =
\begin{cases}
1 & \text{if } a = a^*(s) \\
0 & \text{if  } a \neq a^*(s)
\end{cases} \quad (1)
$$
是求解贝尔曼最优性方程的一个最优策略。这里，$a^*(s)$ 定义为：
$$
a^*(s) = \arg \max_a q^*(s, a)
$$
其中最优动作值函数 $q^*(s, a)$ 定义为：
$$
q^*(s, a) := \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$

**证明：**
简单来说，
$$
\begin{aligned}
\pi^*(s) 
&= \arg \max_a \sum_a \pi(a|s) \left( \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s')) \right) \\
&= \arg \max_a q^*(s, a)
\end{aligned}
$$

## 3.6 分析最优策略
哪些因素决定了最优策略？
从贝尔曼最优性方程中可以清晰地看出：

$$
v^*(s) = \max_a \sum_{s'} p(s'|s, a) (r(s, a, s') + \gamma v^*(s'))
$$
有三个关键因素：
* **奖励设计 (Reward design):** $r(s, a, s')$
* **系统模型 (System model):** $p(s'|s, a)$
* **折扣率 (Discount rate):** $\gamma$
需要计算的未知量是：$v^*(s), v^*(s'), \pi^*(a|s)$。

**定理 (最优策略不变性 - Optimal Policy Invariance)**
考虑一个马尔可夫决策过程，其最优状态值函数为 $\mathbf{v}^* \in \mathbb{R}^{|\mathcal{S}|}$，满足 $\mathbf{v}^* = \max_{\boldsymbol{\pi}} (\mathbf{r}_{\boldsymbol{\pi}} + \gamma \mathbf{P}_{\boldsymbol{\pi}} \mathbf{v}^*)$。如果每个奖励 $r$ 都经过一个仿射变换 $ar + b$，其中 $a, b \in \mathbb{R}$ 且 $a > 0$，那么对应的最优状态值函数 $\mathbf{v}'^*$ 也是 $\mathbf{v}^*$ 的一个仿射变换：
$$
\mathbf{v}'^* = a\mathbf{v}^* + \frac{b}{1 - \gamma} \mathbf{1}
$$
其中 $\gamma \in [0, 1)$ 是折扣率，$\mathbf{1} = [1, \dots, 1]^T$ 是一个所有元素都为 1 的向量。
因此，最优策略对于奖励信号的仿射变换是不变的。

**证明：**
对于任意策略 $\pi$，定义 $r_\pi = [..., r_\pi(s), ...]^T$，其中
$$
r_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s,a)r, \quad s \in \mathcal{S}.
$$
如果 $r \rightarrow \alpha r + \beta$，那么 $r_\pi(s) \rightarrow \alpha r_\pi(s) + \beta$，因此 $r_\pi \rightarrow \alpha r_\pi + \beta \mathbf{1}$，其中 $\mathbf{1} = [..., 1, ...]^T$。在这种情况下，BOE 变为
$$
v' = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi v'). \quad (*)
$$
我们通过证明 $v' = \alpha v^* + c\mathbf{1}$ 是新 BOE (\*) 的解来求解它，其中 $c = \beta/(1-\gamma)$。特别地，将 $v' = \alpha v^* + c\mathbf{1}$ 代入 (\*) 得到
$$
\alpha v^* + c\mathbf{1} = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \gamma P_\pi (\alpha v^* + c\mathbf{1})) = \max_{\pi \in \Pi} (\alpha r_\pi + \beta \mathbf{1} + \alpha \gamma P_\pi v^* + \gamma c P_\pi \mathbf{1}),
$$
其中最后一个等式成立是因为 $P_\pi \mathbf{1} = \mathbf{1}$。上述方程可以重新整理为
$$
\alpha v^* = \max_{\pi \in \Pi} (\alpha r_\pi + \alpha \gamma P_\pi v^*) + \beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1},
$$
这等价于
$$
\beta \mathbf{1} + \gamma c \mathbf{1} - c\mathbf{1} = 0.
$$
由于 $c = \beta/(1-\gamma)$，上述方程成立，因此 $v' = \alpha v^* + c\mathbf{1}$ 是 (\*) 的解。由于 (\*) 是 BOE，$v'$ 也是唯一的解。最后，由于 $v'$ 是 $v^*$ 的仿射变换，动作值之间的相对关系保持不变。因此，从 $v'$ 导出的贪婪最优策略与从 $v^*$ 导出的相同：$\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v')$ 与 $\arg \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*)$ 相同。

# 4 值迭代与策略迭代
## 4.1 值迭代算法
### 4.1.1 算法陈述
如何求解贝尔曼最优性方程？
$$
v = f(v) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v)
$$
在上一讲中，我们知道压缩映射定理提出了一种迭代算法：
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k), \quad k = 1, 2, 3...
$$
其中 $v_0$ 可以是任意的。 这个算法最终可以找到最优状态值和最优策略。 这个算法被称为**值迭代**。 我们将看到我们所学的关于贝尔曼最优性方程（BOE）的数学知识最终会发挥作用。

### 4.1.2 算法步骤
值迭代算法
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k), \quad k = 1, 2, 3...
$$
可以分解为两步。
步骤 1：策略更新。这一步是求解
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)
$$
其中 $v_k$ 已知。
步骤 2：值更新。
$$
v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k
$$

问题：$v_k$ 是一个状态值吗？不，因为它不能保证 $v_k$ 满足贝尔曼方程。

对于步骤 1: 策略更新，其逐元素形式为
$$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)$$
是
$$
\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}
$$
求解上述优化问题的最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 & a = a_k^*(s) \\ 0 & a \neq a_k^*(s) \end{cases}
$$
其中 $a_k^*(s) = \arg \max_{a} q_k(s,a)$。$\pi_{k+1}$ 被称为一个**贪婪策略**，因为它简单地选择了最大的 $q$ 值。

对于步骤 2：值更新，其逐元素形式为
$$
v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k
$$
是
$$
v_{k+1}(s) = \sum_{a} \pi_{k+1}(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}
$$
由于 $\pi_{k+1}$ 是贪婪的，上述方程简化为
$$
v_{k+1}(s) = \max_{a} q_k(a, s)
$$

### 4.1.3 算法总结
算法总结：
$$
v_k(s) \to q_k(s,a) \to \text{greedy policy } \pi_{k+1}(a|s) \to \text{new value } v_{k+1} = \max_a q_k(s,a)
$$

**伪代码：值迭代算法**
初始化：对于所有的 $(s,a)$，概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 是已知的。
任意猜测 $v_0$。
目标：搜索求解贝尔曼最优性方程的最优状态值和最优策略。

当 $v_k$ 在 $\|v_k - v_{k-1}\|$ 大于预定义的小阈值的意义上尚未收敛时，对于第 k 次迭代，执行以下操作：
	对于每个状态 $s \in S$，执行：
		对于每个动作 $a \in A(s)$，执行：
			$q$ 值: $q_k(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$
			最大动作值：$a_k^*(s) = \arg \max_a q_k(a,s)$
			策略更新：$\pi_{k+1}(a|s) = 1$ 如果 $a = a_k^*$，否则 $\pi_{k+1}(a|s) = 0$
			值更新：$v_{k+1}(s) = \max_a q_k(a,s)$

## 4.2 策略迭代算法
#### 4.2.1 算法陈述
给定一个随机初始策略 $\pi_0$，
步骤 1：策略评估（PE）
这一步是计算 $\pi_k$ 的状态值：
$$
v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
$$
请注意，$v_{\pi_k}$ 是一个状态值函数。
步骤 2：策略改进（PI）
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
最大化是逐分量的。

在策略评估步骤中，如何通过求解贝尔曼方程得到状态值 $v_{\pi_k}$？
$$
v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
$$
闭式解：
$$
v_{\pi_k} = (I - \gamma P_{\pi_k})^{-1} r_{\pi_k}
$$
迭代解：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0,1,2,...
$$
已在贝尔曼方程的求解中学习过。
策略迭代是一个迭代算法，其中嵌入了另一个迭代算法在策略评估步骤中。

#### 4.2.2 算法的最优性
在策略改进步骤中，为什么新策略 $\pi_{k+1}$ 比 $\pi_k$ 更好？
**引理（策略改进）：**
如果
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
那么对于任何 $k$，有 $v_{\pi_{k+1}} \ge v_{\pi_k}$。

**证明：**
由于 $v_{\pi_{k+1}}$ 和 $v_{\pi_k}$ 是状态值，它们满足贝尔曼方程：
$$
\begin{aligned}
v_{\pi_{k+1}} &= r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}, \\
v_{\pi_k} &= r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}.
\end{aligned}
$$
由于 $\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})$，我们知道
$$
r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k} \ge r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}.
$$
因此有
$$
\begin{aligned}
v_{\pi_k} - v_{\pi_{k+1}} &= (r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) \\
&\le (r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) \\
&\le \gamma P_{\pi_{k+1}} (v_{\pi_k} - v_{\pi_{k+1}}).
\end{aligned}
$$
因此，
$$
\begin{aligned}
v_{\pi_k} - v_{\pi_{k+1}} 
& \le \gamma^2 P_{\pi_{k+1}}^2 (v_{\pi_k} - v_{\pi_{k+1}}) \le \dots \le \gamma^n P_{\pi_{k+1}}^n (v_{\pi_k} - v_{\pi_{k+1}}) \\
& \le \lim_{n \to \infty} \gamma^n P_{\pi_{k+1}}^n (v_{\pi_k} - v_{\pi_{k+1}}) = 0.
\end{aligned}
$$
该极限是由于 $n \to \infty$ 时 $\gamma^n \to 0$ 以及 $P_{\pi_{k+1}}^n$ 对于任何 $n$ 都是一个非负随机矩阵。这里，随机矩阵指的是一个非负随机矩阵，其所有行的行和都等于一。

为什么这种迭代算法最终能达到最优策略？
由于每次迭代都会改进策略，我们知道
$$
v_{\pi_0} \le v_{\pi_1} \le v_{\pi_2} \le \cdots \le v_{\pi_k} \le \cdots \le v^*.
$$
结果是，$v_{\pi_k}$ 持续增加并会收敛。仍然需要证明它收敛到 $v^*$。

#### 4.2.3 算法的收敛性
**定理（策略迭代的收敛性）：**
由策略迭代算法生成的状态值序列 $\{v_{\pi_k}\}_{k=0}^\infty$ 收敛到最优状态值 $v^*$。结果是，策略序列 $\{\pi_k\}_{k=0}^\infty$ 收敛到一个最优策略。

**证明：**
证明的思路是表明策略迭代算法比值迭代算法收敛得更快。
特别是，为了证明 $\{v_{\pi_k}\}_{k=0}^\infty$ 的收敛性，我们引入由以下公式生成的另一个序列$\{v_k\}_{k=0}^\infty$：
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k).
$$
这个迭代算法正是值迭代算法。我们已经知道当给定任何初始值 $v_0$ 时，$v_k$ 会收敛到 $v^*$。
对于 $k=0$，我们总是可以找到一个 $v_0$ 使得对于任何 $\pi_0$，都有 $v_{\pi_0} \ge v_0$。
我们接下来通过归纳法证明对于所有 $k$，有 $v_{\pi_k} \le v^*$。
对于 $k \ge 0$，假设 $v_{\pi_k} \ge v_k$。
对于 $k+1$，我们有
$$
\begin{aligned}
v_{\pi_{k+1}} - v_{k+1} &= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) - \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k) \\
&\ge (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) - \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k) \\
&\quad (\text{因为 } v_{\pi_{k+1}} \ge v_{\pi_k} \text{ 根据策略改进引理和 } P_{\pi_{k+1}} \ge 0) \\
&= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_k}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k) \\
&\quad (\text{假设 } \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_k)) \\
&\ge (r_{\pi_{k+1}} + \gamma P_{\pi_k} v_{\pi_k}) - (r_{\pi_k} + \gamma P_{\pi_k} v_k) \\
&\quad (\text{因为 } \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})) \\
&= \gamma P_{\pi_k} (v_{\pi_k} - v_k).
\end{aligned}
$$
由于 $v_{\pi_k} - v_k \ge 0$ 且 $P_{\pi_k}$ 是非负的，我们有 $P_{\pi_k}(v_{\pi_k} - v_k) \ge 0$，因此 $v_{\pi_{k+1}} - v_{k+1} \ge 0$。
因此，我们可以通过归纳法证明对于任何 $k \ge 0$，有 $v_k \le v_{\pi_k} \le v^*$。由于 $v_k$ 收敛到 $v^*$， $v_{\pi_k}$ 也收敛到 $v^*$。

#### 4.2.4 算法形式
对于步骤 1：策略评估
矩阵-向量形式：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0,1,2,...
$$
逐元素形式：
$$
v_{\pi_k}^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right), \quad s \in \mathcal{S}
$$
当 $j \to \infty$ 或 $j$ 足够大，或 $\|v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}\|$ 足够小时停止。

对于步骤 2：策略改进
矩阵-向量形式：
$$
\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
$$
逐元素形式：
$$
\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s') \right)}_{q_{\pi_k}(s,a)}, \quad s \in \mathcal{S}
$$
这里，$q_{\pi_k}(s,a)$ 是在策略 $\pi_k$ 下的动作值。令
$$
a_k^*(s) = \arg \max_{a} q_{\pi_k}(a, s)
$$
那么，贪婪策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 & a = a_k^*(s) \\ 0 & a \neq a_k^*(s) \end{cases}
$$

#### 4.2.5 算法总结
**伪代码：策略迭代算法**
初始化：概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 对于所有的 $(s,a)$ 都是已知的。初始猜测 $\pi_{(0)}$。
目标：搜索最优状态值和一个最优策略。
当策略尚未收敛时，对于第 $k$ 次迭代，执行：
策略评估：
    初始化: 任意初始猜测 $v_{\pi_k}^{(0)}$
    当 $v_{\pi_k}^{(j)}$ 尚未收敛时，对于第 $j$ 次迭代，执行：
        对于每个状态 $s \in S$，执行：$v_{\pi_k}^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right]$
策略改进：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
	        $q_{\pi_k}(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s')$
	    $a_k^*(s) = \arg \max_{a} q_{\pi_k}(s,a)$
	    $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

## 4.3 截断迭代算法
#### 4.3.1 值迭代和策略迭代的比较
比较值迭代和策略迭代：

|          | 策略迭代算法                                                        | 值迭代算法                                                   | 备注                                 |
| -------- | ------------------------------------------------------------- | ------------------------------------------------------- | ---------------------------------- |
| 1) 策略    | $\pi_0$                                                       | N/A                                                     |                                    |
| 2) 值     | $v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}$          | $v_0 := v_{\pi_0}$                                      |                                    |
| 3) 策略    | $\pi_1 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_0})$ | $\pi_1 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_0)$ | 两个策略是相同的                           |
| 4) 值     | $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$          | $v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0$                | $v_1 \ge v_0$ 因为 $\pi_1 \ge \pi_0$ |
| 5) 策略    | $\pi_2 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_1})$ | $\pi_2 = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_1)$ |                                    |
| $\vdots$ | $\vdots$                                                      | $\vdots$                                                |                                    |
注意到：
* 它们从相同的初始条件开始。
* 前三个步骤是相同的。
* 第四个步骤变得不同：
    * 在策略迭代中，求解 $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$ 需要一个迭代算法（无限次迭代）
    * 在值迭代中，$v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0$ 是一步迭代


#### 4.3.2 算法陈述
考虑求解 $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$ 的步骤：
$$
v_{\pi_1}^{(0)} = v_0
$$
值迭代 $\leftarrow v_1 \leftarrow v_{\pi_1}^{(1)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(0)}$
$$
v_{\pi_1}^{(2)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(1)}
$$
$$\vdots$$
截断策略迭代 $\leftarrow \bar{v_1} \leftarrow v_{\pi_1}^{(j)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(j-1)}$
$$\vdots$$
策略迭代 $\leftarrow v_{\pi_1} \leftarrow v_{\pi_1}^{(\infty)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(\infty)}$

注意到：
* 值迭代算法计算一次。
* 策略迭代算法计算无限次迭代。
* **截断策略迭代算法**计算有限次迭代（例如 $j$ 次）。从 $j$ 到 $\infty$ 的其余迭代都被截断。

#### 4.2.3 算法总结
**伪代码：截断策略迭代算法**
初始化：概率模型 $p(r|s,a)$ 和 $p(s'|s,a)$ 对于所有的 $(s,a)$ 都是已知的。初始猜测 $\pi_0$。
目标：搜索最优状态值和一个最优策略。
当策略尚未收敛时，对于第 $k$ 次迭代，执行：
  策略评估：
    初始化: 选择初始猜测 $v_k^{(0)} = v_{k-1}$。最大迭代次数设置为 $j_{truncate}$。
    当 $j < j_{truncate}$ 时，执行：
        对于每个状态 $s \in S$，执行：
            $v_k^{(j+1)}(s) = \sum_{a} \pi_k(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k^{(j)}(s') \right]$
    设置 $v_k = v_k^{(j_{truncate})}$
  策略改进：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
            $q_k(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$
        $a_k^*(s) = \arg \max_{a} q_k(s,a)$
        $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

截断会破坏收敛性吗？
**命题（价值改进）**
考虑用于求解策略评估步骤的迭代算法：
$$
v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j = 0, 1, 2, \ldots
$$
如果初始猜测选择为 $v_{\pi_k}^{(0)} = v_{\pi_{k-1}}$，则有
$$
v_{\pi_k}^{(j+1)} \ge v_{\pi_k}^{(j)}
$$
对于每一个 $j = 0, 1, 2, \ldots$

**证明：**
首先，由于 $v_{\pi_k}^{(j)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j-1)}$ 和 $v_{\pi_k}^{(j+1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}$，我们有
$$v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)} = \gamma P_{\pi_k} (v_{\pi_k}^{(j)} - v_{\pi_k}^{(j-1)}) = \cdots = \gamma^j P_{\pi_k}^j (v_{\pi_k}^{(1)} - v_{\pi_k}^{(0)}). \quad (*)$$
其次，由于 $v_{\pi_k}^{(0)} = v_{\pi_{k-1}}$，我们有
$$v_{\pi_k}^{(1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(0)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_{k-1}} \ge r_{\pi_{k-1}} + \gamma P_{\pi_{k-1}} v_{\pi_{k-1}} = v_{\pi_{k-1}} = v_{\pi_k}^{(0)},$$
其中不等式是由于 $\pi_k = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_{k-1}})$。将 $v_{\pi_k}^{(1)} \ge v_{\pi_k}^{(0)}$ 代入 (\*) 得到 $v_{\pi_k}^{(j+1)} \ge v_{\pi_k}^{(j)}$。

# 5 蒙特卡洛迭代
## 5.1 蒙特卡洛估计
蒙特卡洛方法是一种通过随机抽样来估计数值结果的计算技术。它依赖于重复的随机抽样来获得概率结果，并利用这些结果来逼近真实值。这种方法特别适用于解决那些难以通过确定性算法或解析方法解决的问题，尤其是在涉及高维度、复杂模型或不确定性的情况下。

**大数定律：**
对于一个随机变量 $X$。假设 $\{x_j\}_{j=1}^N$ 是一些独立同分布的样本。令 $\bar{x} = \frac{1}{N}\sum_{j=1}^N x_j$ 为样本的平均值。那么，
$$
\begin{aligned}
\mathbb{E}[\bar{x}] &= \mathbb{E}[X], \\
\text{Var}[\bar{x}] &= \frac{1}{N}\text{Var}[X].
\end{aligned}
$$
因此，$\bar{x}$ 是 $\mathbb{E}[X]$ 的无偏估计，其方差随着 $N$ 趋于无穷而趋于零。
样本必须是 iid (独立同分布)。

**证明：**
首先，$\mathbb{E}[\bar{x}] = \mathbb{E}[\sum_{i=1}^n x_i/n] = \sum_{i=1}^n \mathbb{E}[x_i]/n = \mathbb{E}[X]$，其中最后一个等式是由于样本是同分布的（即 $\mathbb{E}[x_i] = \mathbb{E}[X]$）。
其次，$\text{var}(\bar{x}) = \text{var}(\sum_{i=1}^n x_i/n) = \sum_{i=1}^n \text{var}[x_i]/n^2 = (n \cdot \text{var}[X])/n^2 = \text{var}[X]/n$，其中第二个等式是由于样本是独立的，第三个等式是由于样本是同分布的（即 $\text{var}[x_i] = \text{var}[X]$）。

将策略迭代转换为无模型：
策略迭代在每次迭代中有两个步骤：
$$
\begin{cases}
\text{Policy Evaluation:}\; v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k} \\
\text{Policy Improvment:}\; \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})
\end{cases}
$$
策略改进步骤的逐元素形式是：
$$
\begin{aligned}
\pi_{k+1}(s) &= \arg \max_{\pi} \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s') \right] \\
&= \arg \max_{\pi} \sum_{a} \pi(a|s) q_{\pi_k}(s,a), \quad s \in \mathcal{S}
\end{aligned}
$$
关键是 $q_{\pi_k}(s,a)$。

动作值的两种表达方式：
表达方式 1 需要模型：
$$
q_{\pi_k}(s, a) = \sum_{r} p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v_{\pi_k}(s')
$$
表达方式 2 不需要模型：
$$
q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]
$$
实现无模型强化学习的想法：我们可以使用表达方式 2 基于数据（样本或经验）来计算 $q_{\pi_k}(s,a)$。

动作值蒙特卡洛估计的步骤：
* 从 $(s, a)$ 开始，遵循策略 $\pi_k$，生成一个片段；
* 该片段的回报是 $g(s, a)$；
* $g(s, a)$ 是 $G_t$ 的一个样本，在$$q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]$$
* 假设我们有一组片段，因此有 $\{g^{(j)}(s, a)\}$。那么，$$q_{\pi_k}(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a] \approx \frac{1}{N}\sum_{i=1}^N g^{(i)}(s, a).$$
基本思想：当模型不可用时，我们可以使用数据。

## 5.2 基本蒙特卡洛算法
基本蒙特卡洛算法描述：
给定一个初始策略 $\pi_0$，在第 $k$ 次迭代中有两个步骤。
* 步骤 1：策略评估。这一步是获取所有 $(s, a)$ 的 $q_{\pi_k}(s, a)$。具体来说，对于每个动作-状态对 $(s, a)$，运行无限多（或足够多）的片段。它们的平均回报用于近似 $q_{\pi_k}(s, a)$。
* 步骤 2：策略改进。这一步是求解
$$\pi_{k+1}(s) = \arg \max_{\pi} \sum_{a} \pi(a|s)q_{\pi_k}(s, a) \text{ for all } s \in \mathcal{S}.$$
贪婪最优策略是 $\pi_{k+1}(a|s) = 1$，其中 $a_k^* = \arg \max_a q_{\pi_k}(s, a)$。

与策略迭代算法完全相同，除了：直接估计 $q_{\pi_k}(s, a)$，而不是求解 $v_{\pi_k}(s)$。

**伪代码：基本蒙特卡洛算法（策略迭代的无模型变体）**
初始化：初始猜测 $\pi_0$。
目标：搜索最优策略。
当价值估计尚未收敛时，对于第 $k$ 次迭代，执行：
    对于每个状态 $s \in S$，执行：
        对于每个动作 $a \in A(s)$，执行：
            收集足够多的从 $(s, a)$ 开始并遵循 $\pi_k$ 的片段
            基本蒙特卡洛策略评估步骤：
                $q_{\pi_k}(s, a) = \text{所有从 } (s, a) \text{ 开始的片段的平均回报}$
            策略改进步骤：
                $a_k^*(s) = \arg \max_a q_{\pi_k}(s, a)$
                $\pi_{k+1}(a|s) = 1 \text{ if } a = a_k^* \text{, and } \pi_{k+1}(a|s) = 0 \text{ otherwise}$

注意：
* 基本蒙特卡洛算法是策略迭代算法的一个变体。
* 无模型算法是在有模型算法的基础上建立起来的。因此，在学习无模型算法之前，有必要先理解有模型算法。
* 基本蒙特卡洛算法有助于揭示基于 MC 的无模型强化学习的核心思想，但由于效率低下而不实用。
* 为什么基本蒙特卡洛算法估计动作值而不是状态值？那是因为状态值不能直接用于改进策略。当模型不可用时，我们应该直接估计动作值。
* 由于策略迭代是收敛的，因此基本蒙特卡洛算法的收敛性在给定足够片段的情况下也保证是收敛的。

对于算法采样的的片段长度参数来说：
* 当片段长度较短时，只有接近目标的那些状态才具有非零状态值。
* 随着片段长度的增加，接近目标的状态比远离目标的状态更早地具有非零值。
* 片段长度应该足够长。
* 片段长度不必是无限长。

## 5.2 探索开始的蒙特卡洛算法
考虑一个网格世界示例，遵循策略 $\pi$，我们可以得到一个片段，例如
$$s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots$$
访问：每次一个状态-动作对出现在片段中，它被称为对该状态-动作对的一次**访问**。
使用数据的方法：首次访问法
* 只计算回报并近似 $q_{\pi}(s_1, a_2)$。
* 这就是基本蒙特卡洛算法所做的。
* 缺点：未充分利用数据。

该片段也访问了其他状态-动作对。
$$
\begin{aligned}
s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{原始片段}] \\
s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_2, a_4) \text{ 开始的片段}] \\
s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_1, a_2) \text{ 开始的片段}] \\
s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_2, a_3) \text{ 开始的片段}] \\
s_5 \xrightarrow{a_1} \dots & \quad [\text{从 }(s_5, a_1) \text{ 开始的片段}]
\end{aligned}
$$
可以估计 $q_{\pi}(s_1, a_2)$, $q_{\pi}(s_2, a_4)$, $q_{\pi}(s_2, a_3)$, $q_{\pi}(s_5, a_1)$, ......
数据高效的方法：
* 首次访问法
* 每次访问法

基于蒙特卡洛方法的强化学习的另一个方面是何时更新策略。有两种方法：
* 第一种方法是在策略评估步骤中，收集所有从一个状态-动作对开始的片段，然后使用平均回报来近似动作值。
    * 这是基本蒙特卡洛算法所采用的方法。
    * 这种方法的问题是，智能体必须等到所有片段都收集完毕。
* 第二种方法使用单个片段的回报来近似动作值。
    * 这样，我们可以逐片段地改进策略。

第二种方法会引发问题吗？
* 有人可能会说单个片段的回报不能准确近似相应的动作值。
* 事实上，我们在之前介绍的截断策略迭代算法中已经做到了这一点！
广义策略迭代：
* 不是一个具体的算法。
* 它指的是在策略评估和策略改进过程之间切换的普遍思想或框架。
* 许多基于模型和无模型的强化学习算法都属于这个框架。

如果我们使用数据并更有效地更新估计，我们得到一个新算法，称为 MC 探索开始：

**伪代码：探索开始的蒙特卡洛算法（基本蒙特卡洛算法的样本高效变体）**
初始化：初始猜测 $\pi_0$。
目标：搜索最优策略。
对于每个片段，执行：
片段生成：随机选择一个起始状态-动作对 $(s_0, a_0)$ 并确保所有对都可以被可能地选择。遵循当前策略，生成一个长度为 $T$ 的片段：$s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。
策略评估和策略改进：
    初始化：$g \leftarrow 0$
    对于片段的每个步骤，$t = T - 1, T - 2, \dots, 0$，执行：
        $g \leftarrow \gamma g + r_{t+1}$
        使用首次访问策略：
        如果 $(s_t, a_t)$ 未出现在 $(s_0, a_0, s_1, a_1, \dots, s_{t-1}, a_{t-1})$ 中，则
            $Returns(s_t, a_t) \leftarrow Returns(s_t, a_t) + g$
            $q(s_t, a_t) = \text{average}(Returns(s_t, a_t))$
            $\pi(a|s_t) = 1 \text{ if } a = \arg \max_a q(s_t, a)$


## 5.3 $\varepsilon$-贪婪蒙特卡洛算法
为什么要考虑探索开始？
* 理论上，只有当每个状态的每个动作值都得到充分探索时，我们才能正确选择最优动作。
  相反，如果一个动作没有被探索，这个动作可能恰好是最优的，从而被错过。
* 实践中，探索开始很难实现。对于许多应用，特别是那些涉及与环境进行物理交互的应用，很难收集从每个状态-动作对开始的片段。

因此，理论与实践之间存在差距。
我们能移除探索开始的要求吗？我们接下来将展示通过使用软策略可以做到这一点。

当采取任何动作的概率都为正时，一个策略被称为**软**策略。
为什么要引入软策略？
* 使用软策略，一些足够长的片段可以足够多次地访问每个状态-动作对。
* 这样，我们就不需要有大量从每个状态-动作对开始的片段。因此，探索开始的要求可以被移除。

我们将使用哪种软策略？答案：$\varepsilon$-贪婪策略
什么是 $\varepsilon$-贪婪策略？
$$\pi(a|s) = \begin{cases} 1 - \frac{\varepsilon}{|A(s)|}(|A(s)| - 1), & \text{for the greedy action,} \\ \\ \frac{\varepsilon}{|A(s)|}, & \text{for th eother } |A(s)| - 1 \text{ actions.} \end{cases}$$
其中 $\varepsilon \in [0, 1]$ 且 $|A(s)|$ 是状态 $s$ 的动作数量。

注意：
* 选择贪婪动作的机会总是大于其他动作，因为 $1 - \frac{\varepsilon}{|A(s)|}(|A(s)| - 1) = 1 - \varepsilon + \frac{\varepsilon}{|A(s)|} \ge \frac{\varepsilon}{|A(s)|}$。
* 为什么要使用 $\varepsilon$-贪婪？平衡探索（exploration）和利用（expoitation）。
* 当 $\varepsilon = 0$ 时，它变成贪婪策略。探索较少但利用较多。
* 当 $\varepsilon = 1$ 时，它变成均匀分布。探索较多但利用较少。


如何将 $\varepsilon$-贪婪嵌入到基于蒙特卡洛的强化学习算法中？
最初，基本蒙特卡洛算法和探索开始蒙特卡洛算法中的策略改进步骤是求解
$$
\pi_{k+1}(s) = \arg \max_{\pi \in \Pi} \sum_{a} \pi(a|s)q_{\pi_k}(s, a),
$$
其中 $\Pi$ 表示所有可能策略的集合。这里最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1, & a = a_k^*; \\ 0, & a \neq a_k^*, \end{cases}
$$
其中 $a_k^* = \arg \max_a q_{\pi_k}(s, a)$。

现在，策略改进步骤被修改为求解
$$
\pi_{k+1}(s) = \arg \max_{\pi \in \Pi_{\varepsilon}} \sum_{a} \pi(a|s)q_{\pi_k}(s, a),
$$
其中 $\Pi_{\varepsilon}$ 表示具有固定 $\varepsilon$ 值的**所有 $\varepsilon$-贪婪策略**的集合。这里最优策略是
$$
\pi_{k+1}(a|s) = \begin{cases} 1 - \frac{|A(s)|-1}{|A(s)|}\varepsilon, & a = a_k^*; \\ \frac{1}{|A(s)|}\varepsilon, & a \neq a_k^*. \end{cases}
$$
注意：
* $\varepsilon$-贪婪蒙特卡洛算法与探索开始蒙特卡洛算法相同，除了前者使用 $\varepsilon$-贪婪策略。
* 它不要求探索开始，但仍然需要以不同的形式访问所有状态-动作对。


**伪代码：$\varepsilon$-贪婪蒙特卡洛算法（探索开始的蒙特卡洛算法的变体）**
初始化：初始猜测 $\pi_0$ 和 $\varepsilon \in [0,1]$ 的值。
目标：搜索最优策略。
对于每个片段，执行：
片段生成：随机选择一个起始状态-动作对 $(s_0, a_0)$。遵循当前策略，生成一个长度为 $T$ 的片段：$s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。
策略评估和策略改进：
    初始化: $g \leftarrow 0$
    对于片段的每个步骤，$t = T - 1, T - 2, \dots, 0$，执行：
        $g \leftarrow \gamma g + r_{t+1}$
        使用每次访问法：
        如果 $(s_t, a_t)$ 未出现在 $(s_0, a_0, s_1, a_1, \dots, s_{t-1}, a_{t-1})$ 中，则
            $Returns(s_t, a_t) \leftarrow Returns(s_t, a_t) + g$
            $q(s_t, a_t) = \text{average}(Returns(s_t, a_t))$
            令 $a^* = \arg \max_a q(s_t, a)$并且$$\pi(a|s_t) = \begin{cases} 1 - \frac{|A(s_t)|-1}{|A(s_t)|}\varepsilon, & a = a^* \\ \frac{1}{|A(s_t)|}\varepsilon, & a \neq a^* \end{cases}$$

与贪婪策略相比，
* $\varepsilon$-贪婪策略的优点在于它们具有更强的探索能力，因此不需要探索开始条件。
* 缺点在于 $\varepsilon$-贪婪策略通常不是最优的（我们只能证明总是存在最优的贪婪策略）。
    * $\varepsilon$-贪婪蒙特卡洛算法给出的最终策略只在所有 $\varepsilon$-贪婪策略的集合 $\Pi_{\varepsilon}$ 中是最优的。
* $\varepsilon$ 不能太大。

# 6 随机近似与随机梯度下降
## 6.1 均值估计
回顾均值估计问题：
* 考虑一个随机变量 $X$。
* 我们的目标是估计 $E[X]$。
* 假设我们收集了一系列独立同分布的样本 $\{x_i\}_{i=1}^N$。
* $X$ 的期望值可以通过以下方式近似：$$
    E[X] \approx \bar{x} := \frac{1}{N} \sum_{i=1}^{N} x_i.
    $$
我们从上一节已经了解到：
* 这种近似是蒙特卡洛估计的基本思想。
* 我们知道当 $N \to \infty$ 时，$\bar{x} \to E[X]$。

为什么我们如此关注均值估计？强化学习中许多值，例如状态/动作值，都定义为均值。

新问题：如何计算均值 $\bar{x}$？
$$
E[X] \approx \bar{x} := \frac{1}{N} \sum_{i=1}^{N} x_i.
$$
我们有两种方法。
* 第一种方法，这是微不足道的，就是收集所有样本然后计算平均值。
	* 这种方法的缺点是，如果样本是在一段时间内逐个收集的，我们必须等到所有样本都收集完毕。
* 第二种方法可以避免这个缺点，因为它以**增量**和**迭代**的方式计算平均值。

特别地，假设
$$
w_{k+1} = \frac{1}{k} \sum_{i=1}^{k} x_i, \quad k=1, 2, \dots
$$
因此
$$
w_k = \frac{1}{k-1} \sum_{i=1}^{k-1} x_i, \quad k=2, 3, \dots
$$
那么，$w_{k+1}$ 可以用 $w_k$ 表示为
$$
\begin{aligned}
w_{k+1} &= \frac{1}{k} \sum_{i=1}^{k} x_i \\
&= \frac{1}{k} \left( \sum_{i=1}^{k-1} x_i + x_k \right) \\
&= \frac{1}{k} ((k-1)w_k + x_k) \\
&= w_k - \frac{1}{k}(w_k - x_k).
\end{aligned}
$$
因此，我们得到以下迭代算法：
$$
w_{k+1} = w_k - \frac{1}{k}(w_k - x_k).
$$

关于此算法的备注：
* 此算法的一个优点是，一旦接收到一个样本，就可以立即获得均值估计。然后，该均值估计可以立即用于其他目的。
* 由于样本不足，均值估计在开始时不准确（即 $w_k \neq E[X]$）。然而，总比没有好。随着获得更多样本，估计可以逐渐改进（即 $w_k \to E[X]$ 当 $k \to N$）。

此外，考虑一个更通用的算法表达式：
$$
w_{k+1} = w_k - \alpha_k (w_k - x_k),
$$
其中 $1/k$ 被 $\alpha_k > 0$ 替换。
* 这个算法仍然收敛到均值 $E[X]$ 吗？如果 $\{\alpha_k\}$ 满足一些温和的条件，我们将证明答案是肯定的。
* 我们还将证明这个算法是一个特殊的随机近似算法，也是一个特殊的随机梯度下降算法。
* 在下一节中，我们将看到时序差分算法有类似（但更复杂）的表达式。

## 6.2 Robbins-Monro 算法
### 6.2.1 算法陈述
随机近似 (SA)：
* SA 指的是一类广泛的随机迭代算法，用于解决寻根或优化问题。
* 与许多其他寻根算法（如基于梯度的方法）相比，SA 功能强大，因为它不需要知道目标函数及其导数的表达式。

Robbins-Monro (RM) 算法：
* 这是随机近似领域的一项开创性工作。
* 著名的随机梯度下降算法是 RM 算法的一种特殊形式。
* 它可用于分析开头介绍的均值估计算法。

问题陈述：假设我们想找到方程的根
$$
g(w) = 0,
$$
其中 $w \in \mathbb{R}$ 是待求解的变量，$g : \mathbb{R} \to \mathbb{R}$ 是一个函数。
* 许多问题最终都可以转化为这个寻根问题。例如，假设 $J(w)$ 是一个需要最小化的目标函数。那么，优化问题可以转化为$$g(w) = \nabla_w J(w) = 0$$
* 注意，像 $g(w) = c$ 这样 $c$ 是常数的方程也可以通过将 $g(w) - c$ 重写为一个新函数来转换为上述方程。

如何计算 $g(w)=0$ 的根？
* 如果 $g$ 或其导数的表达式已知，有许多数值算法可以解决这个问题。
* 如果函数 $g$ 的表达式未知怎么办？例如，该函数由人工神经网络表示。

Robbins-Monro (RM) 算法可以解决这个问题：
$$
w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k), \quad k=1, 2, 3, \dots
$$
其中
* $w_k$ 是根的第 $k$ 次估计
* $\tilde{g}(w_k, \eta_k) = g(w_k) + \eta_k$ 是第 $k$ 次带噪声的观测
* $a_k$ 是一个正系数。

函数 $g(w)$ 是一个黑箱。该算法依赖于数据：
* 输入序列：$\{w_k\}$
* 带噪声的输出序列：$\{\tilde{g}(w_k, \eta_k)\}$

哲学：没有模型，我们需要数据。模型指的是函数的表达式。

### 6.2.2 算法收敛的条件
**定理 (Robbins-Monro 定理)**
在 Robbins-Monro 算法中，如果
1) $0 < c_1 \le \nabla_w g(w) \le c_2$ 对于所有的 $w$；
2) $\sum_{k=1}^\infty a_k = \infty$ 并且 $\sum_{k=1}^\infty a_k^2 < \infty$；
3) $E[\eta_k | \mathcal{H}_k] = 0$ 并且 $E[\eta_k^2 | \mathcal{H}_k] < \infty$；
其中 $\mathcal{H}_k = \{w_k, w_{k-1}, \dots\}$，那么 $w_k$ 以概率 1 (w.p.1) 收敛到满足 $g(w^*) = 0$ 的根 $w^*$。

三个条件的解释：
* $0 < c_1 \le \nabla_w g(w) \le c_2$ for all $w$
    这个条件表明：
    * $g$ 是单调递增的，这确保了 $g(w)=0$ 的根存在且唯一
    * 梯度有上界。
* $\sum_{k=1}^\infty a_k = \infty$ and $\sum_{k=1}^\infty a_k^2 < \infty$
    * $\sum_{k=1}^\infty a_k^2 < \infty$ 条件确保 $a_k$ 在 $k \to \infty$ 时收敛到零。
    * $\sum_{k=1}^\infty a_k = \infty$ 条件确保 $a_k$ 不会过快地收敛到零。
* $E[\eta_k | \mathcal{H}_k] = 0$ and $E[\eta_k^2 | \mathcal{H}_k] < \infty$
    * 一个特殊而常见的情况是 $\{\eta_k\}$ 是一个满足 $E[\eta_k] = 0$ 和 $E[\eta_k^2] < \infty$ 的独立同分布随机序列。观测误差 $\eta_k$ 不要求是高斯分布。

更仔细地检查第二个条件：
$$
\sum_{k=1}^\infty a_k^2 < \infty \quad \sum_{k=1}^\infty a_k = \infty
$$
* 首先，$\sum_{k=1}^\infty a_k^2 < \infty$ 表明当 $k \to \infty$ 时，$a_k \to 0$。
    * 为什么这个条件很重要？因为$$w_{k+1} - w_k = -a_k \tilde{g}(w_k, \eta_k),$$
        * 如果 $a_k \to 0$，那么 $a_k \tilde{g}(w_k, \eta_k) \to 0$，因此 $w_{k+1} - w_k \to 0$。
        * 我们需要 $w_{k+1} - w_k \to 0$ 这个事实，这样 $w_k$ 最终才能收敛。
        * 如果 $w_k \to w^*$，$g(w_k) \to 0$，那么 $\tilde{g}(w_k, \eta_k)$ 主要由 $\eta_k$ 决定。
* 其次，$\sum_{k=1}^\infty a_k = \infty$ 表明 $a_k$ 不应该收敛到零太快。
	* 为什么这个条件很重要？
	    总结 $w_2 = w_1 - a_1 \tilde{g}(w_1, \eta_1)$、$w_3 = w_2 - a_2 \tilde{g}(w_2, \eta_2)$、…、
	    $w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k)$ 得到$$w_\infty - w_1 = \sum_{k=1}^\infty a_k \tilde{g}(w_k, \eta_k).$$
	    假设 $w_\infty = w^*$。如果 $\sum_{k=1}^\infty a_k < \infty$，那么 $\sum_{k=1}^\infty a_k \tilde{g}(w_k, \eta_k)$ 可能是有界的。那么，如果初始猜测 $w_1$ 被选择得与 $w^*$ 任意远，则上述等式将失效。

什么样的 $\{a_k\}$ 满足这两个条件？
一个典型的序列是
$$
a_k = \frac{1}{k}
$$
它满足
$$
\lim_{n \to \infty} \left( \sum_{k=1}^n \frac{1}{k} - \ln n \right) = \kappa,
$$
其中 $\kappa \approx 0.577$ 称为欧拉-马歇罗尼常数（也称欧拉常数）。
值得注意的是
$$
\sum_{k=1}^\infty \frac{1}{k^2} = \frac{\pi^2}{6} < \infty.
$$
极限 $\sum_{k=1}^\infty 1/k^2$ 在数论中也有一个特定的名称：巴塞尔问题。

回想一下
$$
w_{k+1} = w_k + \alpha_k (x_k - w_k).
$$
是均值估计算法。

我们知道
* 如果 $\alpha_k = 1/k$，那么 $w_{k+1} = 1/k \sum_{i=1}^k x_i$。
* 如果 $\alpha_k$ 不是 $1/k$，则没有分析收敛性。

接下来，我们证明这个算法是 RM 算法的一个特例。
然后，其收敛性自然而然地得出。
考虑一个函数：
$$
g(w) \doteq w - E[X].
$$
我们的目标是解 $g(w) = 0$。如果能做到这一点，我们就能得到 $E[X]$。
我们能得到的观测是
$$
\tilde{g}(w,x) \doteq w - x,
$$
因为我们只能得到 $X$ 的样本。注意
$$
\begin{aligned}
\tilde{g}(w, \eta) &= w - x \\
&= w - x + E[X] - E[X] \\
&= (w - E[X]) + (E[X] - x) \\
&\doteq g(w) + \eta,
\end{aligned}
$$
用于求解 $g(x) = 0$ 的 RM 算法是
$$
w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k) = w_k - \alpha_k (w_k - x_k),
$$
这正是均值估计算法。
收敛性自然而然地得出。

### 6.2.3 算法收敛性证明
到目前为止，RM 算法的收敛性尚未被证明。为此，我们接下来介绍 Dvoretzky 定理，这是随机近似领域的一个经典结果。该定理可用于分析 RM 算法和许多强化学习算法的收敛性。
#### 6.2.3.1 Dvoretzky 定理及其证明
**定理 (Dvoretzky 定理)**
考虑一个随机过程
$$
w_{k+1} = (1 - \alpha_k)w_k + \beta_k \eta_k,
$$
其中 $\{\alpha_k\}_{k=1}^\infty$, $\{\beta_k\}_{k=1}^\infty$, $\{\eta_k\}_{k=1}^\infty$ 是随机序列。这里对所有 $k$，$\alpha_k \ge 0$, $\beta_k \ge 0$。那么，如果满足以下条件，$w_k$ 将以概率 1 收敛到零：
1) $\sum_{k=1}^\infty \alpha_k = \infty$, $\sum_{k=1}^\infty \alpha_k^2 < \infty$; $\sum_{k=1}^\infty \beta_k^2 < \infty$ 一致地 w.p.1；
2) $E[\eta_k | \mathcal{H}_k] = 0$ 并且 $E[\eta_k^2 | \mathcal{H}_k] \le C$ w.p.1；
其中 $\mathcal{H}_k = \{w_k, w_{k-1}, \dots, \eta_{k-1}, \dots, \alpha_{k-1}, \dots, \beta_{k-1}, \dots\}$。

在介绍本定理的证明之前，我们首先澄清一些问题。
* 在 RM 算法中，系数序列 $\{\alpha_k\}$ 是确定性的。然而，Dvoretzky 定理允许 $\{\alpha_k\}$、$\{\beta_k\}$ 是依赖于 $\mathcal{H}_k$ 的随机变量。因此，它在 $\alpha_k$ 或 $\beta_k$ 是 $\Delta_k$ 的函数的情况下更为有用。
* 在第一个条件中，声明为“一致地几乎必然”。这是因为 $\alpha_k$ 和 $\beta_k$ 可能是随机变量，因此它们的极限定义必须是随机意义上的。在第二个条件中，也声明为“几乎必然”。这是因为 $\mathcal{H}_k$ 是随机变量序列而不是特定值。因此，$E[\eta_k | \mathcal{H}_k]$ 和 $E[\eta_k^2 | \mathcal{H}_k]$ 是随机变量。在这种情况下，条件期望的定义是在“几乎必然”的意义上。

Dvoretzky 定理的原始证明于 1956 年给出。还有其他证明。我们现在基于拟鞅（quasimartingales）给出一个证明。有了拟鞅的收敛性结果，Dvoretzky 定理的证明就直截了当了。

**Dvoretzky 定理的证明** 
令 $h_k \doteq \Delta_k^2$。那么，
$$
\begin{aligned}
h_{k+1} - h_k &= \Delta_{k+1}^2 - \Delta_k^2 \\
&= (\Delta_{k+1} - \Delta_k)(\Delta_{k+1} + \Delta_k) \\
&= (-\alpha_k \Delta_k + \beta_k \eta_k)[(2-\alpha_k)\Delta_k + \beta_k \eta_k] \\
&= -\alpha_k(2-\alpha_k)\Delta_k^2 + \beta_k^2 \eta_k^2 + 2(1-\alpha_k)\beta_k \eta_k \Delta_k.
\end{aligned}
$$
对等式两边取期望得到
$$
E[h_{k+1} - h_k | \mathcal{H}_k] = E[-\alpha_k(2-\alpha_k)\Delta_k^2 | \mathcal{H}_k] + E[\beta_k^2 \eta_k^2 | \mathcal{H}_k] + E[2(1-\alpha_k)\beta_k \eta_k \Delta_k | \mathcal{H}_k]. \quad (1)
$$
首先，由于 $\Delta_k$ 包含在 $\mathcal{H}_k$ 中，因此它可以从期望中取出（参见引理 B.1 中的性质 (e)）。其次，考虑简单的情况
其中 $\alpha_k, \beta_k$ 由 $\mathcal{H}_k$ 确定。这种情况是有效的，例如，当 $\{\alpha_k\}$ 和 $\{\beta_k\}$ 是 $\Delta_k$ 的函数或确定性序列时。那么，它们也可以从期望中取出。因此，(1) 变为
$$
E[h_{k+1} - h_k | \mathcal{H}_k] = -\alpha_k(2-\alpha_k)\Delta_k^2 + \beta_k^2 E[\eta_k^2 | \mathcal{H}_k] + 2(1-\alpha_k)\beta_k \Delta_k E[\eta_k | \mathcal{H}_k]. \quad (2)
$$
对于第一项，由于 $\sum_{k=1}^\infty \alpha_k^2 < \infty$ 意味着 $a_k \to 0$ 几乎必然，因此存在一个有限的 $n$，使得对所有 $k \ge n$，$\alpha_k \le 1$ 几乎必然。不失一般性，我们接下来只考虑 $\alpha_k \le 1$ 的情况。那么，$-\alpha_k(2-\alpha_k)\Delta_k^2 \le 0$。对于第二项，我们假设 $\beta_k^2 E[\eta_k^2 | \mathcal{H}_k] \le \beta_k^2 C$。第三项等于零，因为 $E[\eta_k | \mathcal{H}_k] = 0$ 是假设的。因此，(2) 变为
$$
E[h_{k+1} - h_k | \mathcal{H}_k] = -\alpha_k(2-\alpha_k)\Delta_k^2 + \beta_k^2 E[\eta_k^2 | \mathcal{H}_k] \le \beta_k^2 C, \quad (3)
$$
因此
$$
\sum_{k=1}^\infty E[h_{k+1} - h_k | \mathcal{H}_k] \le \sum_{k=1}^\infty \beta_k^2 C < \infty.
$$
最后一个不等式是由于条件 $\sum_{k=1}^\infty \beta_k^2 < \infty$。然后，根据附录 C 中的拟鞅收敛定理，我们得出 $h_k$ 几乎必然收敛。

我们接下来确定 $\Delta_k$ 收敛到什么值。从 (3) 可得
$$
\sum_{k=1}^\infty \alpha_k(2-\alpha_k)\Delta_k^2 = \sum_{k=1}^\infty \beta_k^2 E[\eta_k^2 | \mathcal{H}_k] - \sum_{k=1}^\infty E[h_{k+1} - h_k | \mathcal{H}_k].
$$
右边的第一项根据假设是有界的。第二项也是有界的，因为 $h_k$ 收敛，因此 $h_{k+1} - h_k$ 是可求和的。因此，左边的 $\sum_{k=1}^\infty \alpha_k(2-\alpha_k)\Delta_k^2$ 也是有界的。由于我们考虑 $\alpha_k \le 1$ 的情况，我们有
$$
\infty > \sum_{k=1}^\infty \alpha_k(2-\alpha_k)\Delta_k^2 \ge \sum_{k=1}^\infty \alpha_k \Delta_k^2 \ge 0.
$$
因此，$\sum_{k=1}^\infty \alpha_k \Delta_k^2$ 是有界的。由于 $\sum_{k=1}^\infty \alpha_k = \infty$，我们必须有 $\Delta_k \to 0$ 几乎必然。


#### 6.2.3.2 均值估计算法收敛性证明
虽然均值估计算法 $w_{k+1} = w_k + \alpha_k (x_k - w_k)$ 已经通过 RM 定理进行了分析，我们接下来展示其收敛性也可以直接通过 Dvoretzky 定理证明。

**证明** 
令 $w^* = E[X]$。均值估计算法 $w_{k+1} = w_k + \alpha_k (x_k - w_k)$ 可以重写为
$$
w_{k+1} - w^* = w_k - w^* + \alpha_k (x_k - w^* - (w_k - w^*)).
$$
令 $\Delta_k \doteq w_k - w^*$。那么，我们有
$$
\Delta_{k+1} = (1 - \alpha_k)\Delta_k + \alpha_k \underbrace{(x_k - w^*)}_{\eta_k}.
$$
由于 $\{x_k\}$ 是独立同分布的，我们有 $E[x_k | \mathcal{H}_k] = E[X] = w^*$。因此，$E[\eta_k | \mathcal{H}_k] = E[x_k - w^* | \mathcal{H}_k] = E[x_k | \mathcal{H}_k] - w^* = 0$ 且 $E[\eta_k^2 | \mathcal{H}_k] = E[(x_k - w^*)^2 | \mathcal{H}_k] = (E[x_k^2 | \mathcal{H}_k] - (w^*)^2)$ 是有界的，因为 $x_k$ 的方差是有限的。根据 Dvoretzky 定理，我们得出 $\Delta_k$ 几乎必然收敛到零，因此 $w_k$ 几乎必然收敛到 $E[X]$。$\square$


#### 6.2.3.3 Robbins-Monro 定理收敛性证明
我们现在准备使用 Dvoretzky 定理来证明 Robbins-Monro 定理。
**Robbins-Monro 定理的证明。** 
RM 算法旨在找到满足 $g(w^*) = 0$ 的根 $w^*$。RM 算法是
$$
\begin{aligned}
w_{k+1} &= w_k - a_k \tilde{g}(w_k, \eta_k) \\
&= w_k - a_k [g(w_k) + \eta_k].
\end{aligned}
$$
那么，我们有
$$
w_{k+1} - w^* = w_k - w^* - a_k [g(w_k) - g(w^*) + \eta_k].
$$
根据中值定理，我们有 $g(w_k) - g(w^*) = \nabla_w g(w_k') (w_k - w^*)$，
其中 $w_k' \in [w_k, w^*]$。令 $\Delta_k \doteq w_k - w^*$。上述方程变为
$$
\begin{aligned}
\Delta_{k+1} &= \Delta_k - a_k [\nabla_w g(w_k')(w_k - w^*) + \eta_k] \\
&= \Delta_k - a_k \nabla_w g(w_k') \Delta_k + a_k (-\eta_k) \\
&= [1 - a_k \nabla_w g(w_k')]\Delta_k + a_k (-\eta_k).
\end{aligned}
$$
注意，$\nabla_w g(w)$ 在 $(0, c_1]$ 和 $c_2$ 之间是有界的，正如假设的 $0 < c_1 \le \nabla_w g(w) \le c_2$。由于 $\sum_{k=1}^\infty a_k = \infty$ 和 $\sum_{k=1}^\infty a_k^2 < \infty$ 成立，我们知道 $\sum_{k=1}^\infty a_k = \infty$ 和 $\sum_{k=1}^\infty a_k^2 < \infty$。因此，Dvoretzky 定理的所有条件都满足，$\Delta_k$ 几乎必然收敛到零。

RM 定理的证明展示了 Dvoretzky 定理的强大之处。特别是，在证明中 $\alpha_k$ 是一个随机序列，而不是一个确定性序列，它依赖于 $w_k$。在这种情况下，Dvoretzky 定理仍然适用。

#### 6.2.3.4 Dvoretzky 定理的扩展
我们接下来将 Dvoretzky 定理扩展到一个更一般的定理，该定理可以处理多个变量。这个通用定理可以用来分析随机迭代算法的收敛性，例如 Q-学习。

**定理 6.3** 考虑一个有限集合 $\mathcal{S}$ 的实数。对于随机过程
$$
\Delta_{k+1}(s) = (1 - \alpha_k(s))\Delta_k(s) + \beta_k(s)\eta_k(s),
$$
如果对于 $s \in \mathcal{S}$ 满足以下条件，则 $\Delta_k(s)$ 几乎必然收敛到零：
1) $\sum_{k=1}^\infty \alpha_k(s) = \infty$, $\sum_{k=1}^\infty \alpha_k^2(s) < \infty$, $\sum_{k=1}^\infty \beta_k^2(s) < \infty$，并且 $E[\beta_k(s)|\mathcal{H}_k] \le E[\alpha_k(s)|\mathcal{H}_k]$ 一致地几乎必然；
2) $E[|\eta_k(s)||\mathcal{H}_k] \le \gamma (1 + ||\Delta_k(s)||_\infty)$，其中 $\gamma \in (0, 1)$；
3) $Var[\eta_k(s)|\mathcal{H}_k] \le C (1 + ||\Delta_k(s)||_\infty)^2$，其中 $C$ 是常数。
这里，$\mathcal{H}_k = \{\Delta_k, \Delta_{k-1}, \dots, \eta_{k-1}, \dots, \alpha_{k-1}, \dots, \beta_{k-1}, \dots\}$ 表示历史信息。项 $|| \cdot ||_\infty$ 指的是最大范数。

下面给出一些关于这个定理的注释。
* 我们首先澄清定理中的一些符号。变量 $s$ 可以看作一个索引。在强化学习的背景下，它表示一个状态或一个状态-动作对。最大范数 $|| \cdot ||_\infty$ 是在一个集合上定义的。它与向量的 $L^\infty$ 范数相似但不同。特别是，$||E[\eta_k(s)|\mathcal{H}_k]||_\infty \doteq \max_{s \in \mathcal{S}} |E[\eta_k(s)|\mathcal{H}_k]|$ 和 $||\Delta_k(s)||_\infty \doteq \max_{s \in \mathcal{S}} |\Delta_k(s)|$。
* 这个定理比 Dvoretzky 定理更一般。首先，由于最大范数运算，它可以处理多变量情况。这对于强化学习问题很重要，因为存在多个状态。其次，Dvoretzky 定理要求 $E[\eta_k(s)|\mathcal{H}_k] = 0$ 和 $var[\eta_k(s)|\mathcal{H}_k] \le C$，而这个定理只要求期望和方差被误差 $\Delta_k$ 边界。
* 应该注意的是，对于所有 $s \in \mathcal{S}$，$\Delta(s)$ 的收敛性要求条件对每个 $s \in \mathcal{S}$ 都有效。因此，在应用此定理证明强化学习算法的收敛性时，我们需要证明这些条件对每个状态（或状态-动作对）都有效。

## 6.3 随机梯度下降算法
我们将介绍随机梯度下降（SGD）算法：
* SGD广泛应用于机器学习和强化学习（RL）领域。
* SGD是一种特殊的RM算法。
* 均值估计算法是一种特殊的SGD算法。

### 6.3.1 算法陈述
假设我们的目标是解决以下优化问题：
$$\min_w J(w) = \mathbb{E}[f(w, X)]$$
* $w$ 是待优化的参数。
* $X$ 是一个随机变量。期望是关于 $X$ 的。
* $w$ 和 $X$ 可以是标量或向量。函数 $f(\cdot)$ 是一个标量。

方法1：梯度下降 (GD)
$$w_{k+1} = w_k - \alpha_k \nabla_w \mathbb{E}[f(w_k, X)] = w_k - \alpha_k \mathbb{E}[\nabla_w f(w_k, X)]$$
缺点：期望值难以获得。

方法2：批量梯度下降 (BGD)
$$
\begin{aligned}
\mathbb{E}[\nabla_w f(w_k, X)] \approx \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i), \\
w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i).
\end{aligned}
$$
缺点：每次迭代都需要大量的样本来计算 $w_k$。

方法3：随机梯度下降 (SGD)
$$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k),$$
* 与梯度下降法相比：用随机梯度 $\nabla_w f(w_k, x_k)$ 替换真实梯度 $\mathbb{E}[\nabla_w f(w_k, X)]$。
* 与批量梯度下降法相比：令 $n=1$。

### 6.3.2 算法的收敛性
从 GD 到 SGD：
$$w_{k+1} = w_k - \alpha_k \mathbb{E}[\nabla_w f(w_k, X)]$$
$$\Downarrow$$
$$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)$$
$\nabla_w f(w_k, x_k)$ 可以看作是对 $\mathbb{E}[\nabla_w f(w, X)]$ 的带噪声测量：
$$\nabla_w f(w_k, x_k) = \mathbb{E}[\nabla_w f(w, X)] + \underbrace{\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w, X)]}_{\eta}.$$
由于
$$\nabla_w f(w_k, x_k) \neq \mathbb{E}[\nabla_w f(w, X)]$$
那么 $k \to \infty$ 时，SGD 是否使 $w_k \to w^*$？

接下来我们将展示 SGD 是一种特殊的RM算法。然后，收敛性自然就得出了。
SGD 的目标是最小化
$$J(w) = \mathbb{E}[f(w, X)]$$
这个问题可以转化为一个求根问题：
$$\nabla_w J(w) = \mathbb{E}[\nabla_w f(w, X)] = 0$$
令
$$g(w) = \nabla_w J(w) = \mathbb{E}[\nabla_w f(w, X)].$$
那么，SGD 的目标是找到 $g(w)=0$ 的根。

我们可以测量的是
$$\begin{aligned}
\tilde{g}(w, \eta) &= \nabla_w f(w, x) \\
&= \underbrace{\mathbb{E}[\nabla_w f(w, X)]}_{g(w)} + \underbrace{\nabla_w f(w, x) - \mathbb{E}[\nabla_w f(w, X)]}_{\eta}.
\end{aligned}$$
那么，求解 $g(w)=0$ 的 RM 算法是
$$w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k) = w_k - a_k \nabla_w f(w_k, x_k).$$
* 这正是 SGD 算法。
* 因此，SGD 是一种特殊的 RM 算法。

由于 SGD 是一种特殊的 RM 算法，其收敛性自然得出。
**定理 (SGD 的收敛性)**
在 SGD 算法中，如果
1) $0 < c_1 \le \nabla_w^2 f(w, X) \le c_2$;
2) $\sum_{k=1}^\infty a_k = \infty$ 且 $\sum_{k=1}^\infty a_k^2 < \infty$;
3) $\{x_k\}_{k=1}^\infty$ 是iid;
那么 $w_k$ 以概率1收敛到 $\nabla_w \mathbb{E}[f(w, X)] = 0$ 的根。

### 6.3.3 算法的收敛模式
问题：由于随机梯度是随机的，因此近似不准确，SGD 的收敛是缓慢的还是随机的？

为了回答这个问题，我们考虑随机梯度与批量梯度之间的相对误差：
$$\delta_k \doteq \frac{||\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]||}{||\mathbb{E}[\nabla_w f(w_k, X)]||}.$$
由于 $\mathbb{E}[\nabla_w f(w^*, X)] = 0$，我们进一步得到
$$\begin{aligned}
\delta_k &= \frac{||\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]||}{||\mathbb{E}[\nabla_w f(w_k, X)] - \mathbb{E}[\nabla_w f(w^*, X)]||} \\
&= \frac{||\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]||}{||\mathbb{E}[\nabla_w^2 f(\tilde{w}_k, X)(w_k - w^*)]||}
\end{aligned}$$
其中最后一个等式是由于均值定理和 $\tilde{w}_k \in [w_k, w^*]$。

假设 $f$ 是严格凸的，使得
$$\nabla_w^2 f \ge c > 0$$
对于所有 $w, X$，其中 $c$ 是一个正界。
那么，$\delta_k$ 的分母变为
$$\begin{aligned}
||\mathbb{E}[\nabla_w^2 f(\tilde{w}_k, X)(w_k - w^*)]|| &= |\mathbb{E}[\nabla_w^2 f(\tilde{w}_k, X)] (w_k - w^*)| \\
&= ||\mathbb{E}[\nabla_w^2 f(\tilde{w}_k, X)]|| ||(w_k - w^*)|| \ge c||w_k - w^*||.
\end{aligned}$$
将上述不等式代入 $\delta_k$ 得到
$$\delta_k \le \frac{||\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]||}{c||w_k - w^*||}.$$
请注意
$$\delta_k \le \frac{\overbrace{\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]}^{\text{随机梯度 } - \text{ 真实梯度}}}{\underbrace{c||w_k - w^*||}_{\text{到最优结果的距离}}}.$$
上述方程揭示了 SGD 一个有趣的收敛模式。
* 相对误差 $\delta_k$ 与 $||w_k - w^*||$ 成反比。
* 当 $||w_k - w^*||$ 较大时，$\delta_k$ 较小，SGD 表现得像 GD。
* 当 $w_k$ 接近 $w^*$ 时，相对误差可能较大，收敛在 $w^*$ 附近表现出更多的随机性。
* 尽管均值的初始猜测值与真实值相距甚远，SGD估计值仍能快速逼近真实值附近。
* 当估计值接近真实值时，它表现出一定的随机性，但仍逐渐逼近真实值。

### 6.3.4 不涉及随机变量的随机梯度下降
我们上面介绍的 SGD 公式涉及随机变量和期望。人们经常会遇到不涉及任何随机变量的 SGD 的确定性公式。
考虑以下优化问题：
$$\min_w J(w) = \frac{1}{n} \sum_{i=1}^n f(w, x_i),$$
* $f(w, x_i)$ 是一个参数化函数。
* $w$ 是待优化的参数。
* 一个实数集 $\{x_i\}_{i=1}^n$，其中 $x_i$ 不必是任何随机变量的样本。

解决此问题的梯度下降算法是
$$w_{k+1} = w_k - \alpha_k \nabla_w J(w_k) = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i).$$
假设集合很大，我们每次只能获取一个数字。在这种情况下，我们可以使用以下迭代算法：
$$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k).$$
问题：
* 这个算法是 SGD 吗？它不涉及任何随机变量或期望值。
* 我们应该如何使用有限的数字集合 $\{x_i\}_{i=1}^n$？我们应该按特定顺序对这些数字进行排序然后逐一使用它们吗？还是应该从集合中随机采样一个数字？

上述问题的一个快速回答是，我们可以手动引入一个随机变量，并将确定性公式转换为SGD的随机公式。
特别地，假设 $X$ 是一个定义在集合 $\{x_i\}_{i=1}^n$ 上的随机变量。假设其概率分布是均匀的，使得
$$p(X = x_i) = 1/n$$
那么，确定性优化问题变成了一个随机问题：
$$\min_w J(w) = \frac{1}{n} \sum_{i=1}^n f(w, x_i) = \mathbb{E}[f(w, X)].$$
* 上述等式中的最后一个等号是严格的，而不是近似的。因此，该算法是 SGD。
* 如果 $x_k$ 是从 $\{x_i\}_{i=1}^n$ 中均匀且独立采样，则估计会收敛。$x_k$ 可以重复取 $\{x_i\}_{i=1}^n$ 中的相同数字，因为它是随机采样的。

### 6.3.5 BGD、SGD、MBGD 的比较
假设我们想最小化 $J(w) = \mathbb{E}[f(w, X)]$，给定一组 $X$ 的随机样本 $\{x_i\}_{i=1}^n$。解决此问题的 BGD、SGD、MBGD 算法分别为：
$$w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i), \quad \text{(BGD)}$$
$$w_{k+1} = w_k - \alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k} \nabla_w f(w_k, x_j), \quad \text{(MBGD)}$$
$$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k). \quad \text{(SGD)}$$
* 在 BGD 算法中，每次迭代都使用所有样本。当 $n$ 很大时，$(1/n) \sum_{i=1}^n \nabla_w f(w_k, x_i)$ 接近真实梯度 $\mathbb{E}[\nabla_w f(w_k, X)]$。
* 在 MBGD 算法中，$\mathcal{I}_k$ 是 $\{1, \dots, n\}$ 的一个子集，其大小为 $|\mathcal{I}_k| = m$。集合 $\mathcal{I}_k$ 是通过 $m$ 次iid采样获得的。
* 在 SGD 算法中，$x_k$ 是在时刻 $k$ 从 $\{x_i\}_{i=1}^n$ 中随机采样的。

比较 MBGD 与 BGD 和 SGD：
* 与 SGD 相比，MBGD 的随机性更小，因为它使用了更多的样本而不是像SGD那样只使用一个样本。
* 与 BGD 相比，MBGD 不需要在每次迭代中使用所有样本，这使其更灵活和高效。
* 如果 $m=1$，MBGD 就变成了 SGD。
* 如果 $m=n$，MBGD 严格来说并没有变成 BGD，因为 MBGD 使用随机获取的 $n$ 个样本，而 BGD 使用所有 $n$ 个数字。特别是，MBGD 可能会多次使用 $\{x_i\}_{i=1}^n$ 中的某个值，而 BGD 只使用每个数字一次。

## 6.4 总结
总结：
* 均值估计：使用 $\{x_k\}$ 计算 $\mathbb{E}[X]$
$$w_{k+1} = w_k - \frac{1}{k}(w_k - x_k).$$
* RM算法：使用 $\{\tilde{g}(w_k, \eta_k)\}$ 求解 $g(w)=0$
$$w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k).$$
* SGD算法：使用 $\{\nabla_w f(w_k, x_k)\}$ 最小化 $J(w) = \mathbb{E}[f(w, X)]$
$$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k).$$
这些结果很有用：
* 我们将在下一章看到，时序差分学习算法可以看作是随机逼近算法，因此具有相似的表达式。
* 它们是重要的优化技术，可以应用于许多其他领域。

# 7 时序差分方法
## 7.1 状态值的时序差分学习
### 7.1.1 算法陈述
算法所需的数据/经验：$(s_0, r_1, s_1, \dots, s_t, r_{t+1}, s_{t+1}, \dots)$ 或 $\{(s_t, r_{t+1}, s_{t+1})\}_t$ 根据给定策略 $\pi$ 生成。
TD学习算法是
$$
\begin{aligned}
& v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]\right], \quad (1) \\
& v_{t+1}(s) = v_t(s), \quad \forall s \ne s_t, \quad (2)
\end{aligned}
$$
其中 $t = 0, 1, 2, \dots$。这里，$v_t(s_t)$ 是 $v_\pi(s_t)$ 的估计状态值；$\alpha_t(s_t)$ 是时间 $t$ 时 $s_t$ 的学习率。
* 在时间 $t$，只有被访问状态 $s_t$ 的值被更新，而未被访问状态 $s \ne s_t$ 的值保持不变。
* 当上下文清晰时，(2) 中的更新将被省略。

### 7.1.2 算法性质
TD 算法可以被注释为
$$
\overbrace{v_{t+1}(s_t)}^{\text{新的估计}} = \overbrace{v_t(s_t)}^{\text{当前估计}} - \alpha_t(s_t)\left[\overbrace{v_t(s_t) - [\underbrace{r_{t+1} + \gamma v_t(s_{t+1})}_{\text{TD 目标 } \bar{v}_t}]}^{\text{TD 误差 } \delta_t}\right], \quad (3)
$$
这里，
$$
\bar{v}_t \doteq r_{t+1} + \gamma v(s_{t+1})
$$
被称为 TD 目标，
$$
\delta_t \doteq v(s_t) - [r_{t+1} + \gamma v(s_{t+1})] = v(s_t) - \bar{v}_t
$$
被称为 TD 误差。
很明显，新的估计 $v_{t+1}(s_t)$ 是当前估计 $v_t(s_t)$ 和 TD 误差的组合。

首先，为什么 $\bar{v}_t$ 被称为 TD 目标？
那是因为算法将 $v(s_t)$ 趋向于 $\bar{v}_t$。
要理解这一点，
$$
\begin{aligned}
v_{t+1}(s_t) &= v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - \bar{v}_t] \\
& \Rightarrow v_{t+1}(s_t) - \bar{v}_t = v_t(s_t) - \bar{v}_t - \alpha_t(s_t)[v_t(s_t) - \bar{v}_t] \\
& \Rightarrow v_{t+1}(s_t) - \bar{v}_t = [1 - \alpha_t(s_t)][v_t(s_t) - \bar{v}_t] \\
& \Rightarrow |v_{t+1}(s_t) - \bar{v}_t| = |1 - \alpha_t(s_t)| |v_t(s_t) - \bar{v}_t|
\end{aligned}
$$
由于 $\alpha_t(s_t)$ 是一个小的正数，我们有
$$
0 < 1 - \alpha_t(s_t) < 1
$$
因此，
$$
|v_{t+1}(s_t) - \bar{v}_t| \le |v_t(s_t) - \bar{v}_t|
$$
这意味着 $v(s_t)$ 被驱动向 $\bar{v}_t$。

其次，TD 误差的解释是什么？
$$
\delta_t = v(s_t) - [r_{t+1} + \gamma v(s_{t+1})]
$$
* 它是两个连续时间步之间的差异。
* 它反映了 $v_t$ 和 $v_\pi$ 之间的不足。为此，记
$$
\delta_{\pi,t} \doteq v_\pi(s_t) - [r_{t+1} + \gamma v_\pi(s_{t+1})]
$$
注意
$$
\mathbb{E}[\delta_{\pi,t} | S_t = s_t] = v_\pi(s_t) - \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s_t] = 0.
$$
* 如果 $v_t = v_\pi$，那么 $\delta_t$ 应该为零（在期望意义上）。
* 因此，如果 $\delta_t$ 不为零，那么 $v_t$ 不等于 $v_\pi$。
* TD 误差可以被解释为创新，这意味着从经验 $(s_t, r_{t+1}, s_{t+1})$ 中获得的新信息。

其他性质：
* (3) 中的 TD 算法只估计给定策略的状态值。
    * 它不估计动作值。
    * 它不搜索最优策略。
* 稍后，我们将看到如何估计动作值，然后搜索最优策略。
* 尽管如此，(3) 中的 TD 算法对于理解核心思想是基础性的。

### 7.1.3 算法的收敛性
#### 7.1.3.1 通过 RM 算法
首先，贝尔曼方程的一个新表达式。
$\pi$ 的状态值定义为
$$
v_\pi(s) = \mathbb{E}[R + \gamma G | S = s], \quad s \in \mathcal{S} \quad (4)
$$
其中 $G$ 是折扣回报。由于
$$
\mathbb{E}[G | S = s] = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)v_\pi(s') = \mathbb{E}[v_\pi(S') | S = s],
$$
其中 $S'$ 是下一个状态，我们可以将 (4) 重写为
$$
v_\pi(s) = \mathbb{E}[R + \gamma v_\pi(S') | S = s], \quad s \in \mathcal{S}. \quad (5)
$$
方程 (5) 是贝尔曼方程的另一个表达式。它有时被称为贝尔曼期望方程，是设计和分析 TD 算法的重要工具。

其次，使用 RM 算法求解 (5) 中的贝尔曼方程。
特别地，通过定义
$$
g(v(s)) = v(s) - \mathbb{E}[R + \gamma v_\pi(S') | s],
$$
我们可以将 (5) 重写为
$$
g(v(s)) = 0.
$$
由于我们只能获得 $R$ 和 $S'$ 的样本 $r$ 和 $s'$，我们所拥有的噪声观测为
$$
\tilde{g}(v(s)) = v(s) - [r + \gamma v_\pi(s')] \\
= \underbrace{(v(s) - \mathbb{E}[R + \gamma v_\pi(S') | s])}_{g(v(s))} + \underbrace{(\mathbb{E}[R + \gamma v_\pi(S') | s] - [r + \gamma v_\pi(s')])}_{\eta}.
$$
因此，求解 $g(v(s))=0$ 的 RM 算法是
$$
\begin{aligned}
v_{k+1}(s) &= v_k(s) - \alpha_k \tilde{g}(v_k(s)) \\
&= v_k(s) - \alpha_k \left(v_k(s) - [r_k + \gamma v_\pi(s_k')]\right), \quad k = 1,2,3,\dots \quad (6)
\end{aligned}
$$
其中 $v_k(s)$ 是在第 $k$ 步对 $v_\pi(s)$ 的估计；$r_k, s_k'$ 是在第 $k$ 步获得的 $R, S'$ 样本。

(6) 中的 RM 算法有两个值得特别注意的假设。
* 我们必须有经验集 $\{(s, r, s')\}$，对于 $k=1, 2, 3, \dots$。
* 我们假设对于任何 $s'$，$v_\pi(s')$ 都已知。

为了消除 RM 算法中的两个假设，我们可以对其进行修改。 
* 一个修改是 $\{(s, r, s')\}$ 被更改为 $\{(s_t, r_{t+1}, s_{t+1})\}$，以便算法可以利用一个回合中的序列样本。
* 另一个修改是 $v_\pi(s')$ 被替换为它的一个估计，因为我们事先不知道它。

#### 7.1.3.2 通过 Dvoretzky 定理的扩展
**定理 (TD 学习的收敛性)**
根据 TD 算法 (1)，当 $t \to \infty$ 时，对于所有 $s \in \mathcal{S}$，$v_t(s)$ 以概率 1 收敛到 $v_\pi(s)$，如果 $\sum_t \alpha_t(s) = \infty$且 $\sum_t \alpha_t^2(s) < \infty$ 对于所有 $s \in \mathcal{S}$。

备注：
* 这个定理说明了给定策略 $\pi$ 的状态值可以通过 TD 算法找到。
* $\sum_t \alpha_t(s) = \infty$ 且 $\sum_t \alpha_t^2(s) < \infty$ 必须对所有 $s \in \mathcal{S}$ 都有效。在时间步 $t$，如果 $s = s_t$，这意味着 $s$ 在时间 $t$ 被访问，那么 $\alpha_t(s) > 0$；否则，对于所有其他的 $s \ne s_t$，$\alpha_t(s) = 0$。这要求每个状态都必须被访问无限次 (或足够多次)。
* 学习率 $\alpha$ 通常被选为一个小常数。在这种情况下，条件 $\sum_t \alpha_t^2(s) < \infty$ 不再有效。当 $\alpha$ 是常数时，仍然可以证明算法在期望意义上收敛。

**我们基于 Dvoretzky 定理的扩展证明收敛性。**
为此，我们首先需要构造一个随机过程，使其符合 Dvoretzky 定理的扩展。考虑任意状态 $s \in \mathcal{S}$。在时间 $t$，根据 TD 算法可知
$$
v_{t+1}(s) = v_t(s) - \alpha_t(s)\left(v_t(s) - (r_{t+1} + \gamma v_t(s_{t+1}))\right), \quad \text{if } s = s_t, \quad (a)
$$
或
$$
v_{t+1}(s) = v_t(s), \quad \text{if } s \ne s_t. \quad (b)
$$
估计误差定义为
$$
\Delta_t(s) \doteq v_t(s) - v_\pi(s),
$$
其中 $v_\pi(s)$ 是状态 $s$ 在策略 $\pi$ 下的状态值。从 (a) 两边减去 $v_\pi(s)$ 得到
$$
\begin{aligned}
\Delta_{t+1}(s) &= (1 - \alpha_t(s))\Delta_t(s) + \alpha_t(s)\underbrace{(r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s))}_{\eta_t(s)} \\
&= (1 - \alpha_t(s))\Delta_t(s) + \alpha_t(s)\eta_t(s), \quad s = s_t. \quad (c)
\end{aligned}
$$
从 (b) 两边减去 $v_\pi(s)$ 得到
$$
\Delta_{t+1}(s) = (1 - \alpha_t(s))\Delta_t(s) + \alpha_t(s)\eta_t(s), \quad s \ne s_t,
$$
其表达式与 (c) 相同，只是 $\alpha_t(s) = 0$ 且 $\eta_t(s) = 0$。因此，无论 $s = s_t$ 还是 $s \ne s_t$，我们都得到以下统一表达式：
$$
\Delta_{t+1}(s) = (1 - \alpha_t(s))\Delta_t(s) + \alpha_t(s)\eta_t(s).
$$
这就是 Dvoretzky 定理的扩展中的过程。我们的目标是证明 Dvoretzky 定理的扩展中的三个条件都满足，从而证明过程收敛。

第一个条件在上一个证明中已经假设为真。我们接下来证明第二个条件也为真。也就是说，$||\mathbb{E}[\eta_t(s)|\mathcal{H}_t]||_\infty \le \gamma||\Delta_t(s)||_\infty$ 对于所有 $s \in \mathcal{S}$。这里，$\mathcal{H}_t$ 代表历史信息（参见 Dvoretzky 定理的扩展中的定义）。由于马尔可夫性质，$\eta_t(s) = r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s)$ 或 $\eta_t(s) = 0$ 不依赖于在给定 $s$ 时的历史信息。因此，我们有 $\mathbb{E}[\eta_t(s)|\mathcal{H}_t] = \mathbb{E}[\eta_t(s)]$。对于 $s \ne s_t$，我们有 $\eta_t(s) = 0$。那么，很容易得到
$$
||\mathbb{E}[\eta_t(s)]||_\infty = 0 \le \gamma ||\Delta_t(s)||_\infty. \quad (d)
$$
对于 $s=s_t$，我们有
$$
\begin{aligned}
\mathbb{E}[\eta_t(s)] &= \mathbb{E}[\eta_t(s_t)] \\
&= \mathbb{E}[r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s_t) | s_t] \\
&= \mathbb{E}[r_{t+1} + \gamma v_t(s_{t+1}) | s_t] - v_\pi(s_t).
\end{aligned}
$$
由于 $v_\pi(s_t) = \mathbb{E}[r_{t+1} + \gamma v_\pi(S_{t+1}) | s_t]$，上述方程意味着
$$
\begin{aligned}
\mathbb{E}[\eta_t(s)] &= \gamma \mathbb{E}[v_t(S_{t+1}) - v_\pi(S_{t+1}) | s_t] \\
&= \gamma \sum_{s' \in \mathcal{S}} p(s'|s_t) [v_t(s') - v_\pi(s')].
\end{aligned}
$$
由此可知
$$
\begin{aligned}
||\mathbb{E}[\eta_t(s)]||_\infty &= \gamma \left|\sum_{s' \in \mathcal{S}} p(s'|s_t) [v_t(s') - v_\pi(s')]\right| \\
&\le \gamma \sum_{s' \in \mathcal{S}} p(s'|s_t) \max_{s'' \in \mathcal{S}} |v_t(s'') - v_\pi(s'')| \\
&= \gamma \max_{s'' \in \mathcal{S}} |v_t(s'') - v_\pi(s'')| \\
&= \gamma ||v_t(s') - v_\pi(s')||_\infty \\
&= \gamma ||\Delta_t(s)||_\infty. \quad (e)
\end{aligned}
$$
因此，在时间 $t$，我们从 (d) 和 (e) 得知 $||\mathbb{E}[\eta_t(s)]||_\infty \le \gamma||\Delta_t(s)||_\infty$ 对于所有 $s \in \mathcal{S}$，无论 $s = s_t$。于是，
$$
||\mathbb{E}[\eta_t(s)]||_\infty \le \gamma||\Delta_t(s)||_\infty,
$$
这是 Dvoretzky 定理的扩展中的第二个条件。最后，关于第三个条件，我们有 $\text{var}[\eta_t(s)|\mathcal{H}_t] = \text{var}[r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s)|s_t] = \text{var}[r_{t+1} + \gamma v_t(s_{t+1})|s_t]$ 对于 $s = s_t$，并且 $\text{var}[\eta_t(s)|\mathcal{H}_t] = 0$ 对于 $s \ne s_t$。由于 $r_{t+1}$ 是有界的，第三个条件可以很容易地证明。

### 7.1.4 TD 与 MC 的比较
虽然 TD 学习和 MC 学习都是无模型的，但 TD 学习相对于 MC 学习的优缺点是什么？

| TD/Sarsa 学习                                                                                        | MC 学习                                                                                                                                     |
| :------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------- |
| 在线 (Online)：TD 学习是在线的。它可以在收到奖励后立即更新状态/动作值。                                                         | 离线 (Offline)：MC 学习是离线的。它必须等到一个回合完全收集完毕才能更新。                                                                                               |
| 持续任务 (Continuing tasks）：由于 TD 学习是在线的，它可以处理偶发任务和持续任务。                                               | 偶发任务 (Episodic tasks)：由于 MC 学习是离线的，它只能处理具有终止状态的偶发任务。                                                                                      |
| 自举 (Bootstrapping)：TD 自举是因为值的更新依赖于该值的前一个估计。因此，它需要初始猜测。                                             | 非自举 (Non-bootstrapping)：MC 不是自举的，因为它可以直接估计状态/动作值而不需要任何初始猜测。                                                                               |
| 低估计方差 (Low estimation variance)：TD 具有比 MC 更低的方差，因为随机变量更少。例如，Sarsa 只需要 $R_{t+1}, S_{t+1}, A_{t+1}$。 | 高估计方差 (High estimation variance)：为了估计 $q_\pi(s_t, a_t)$，我们需要 $R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$ 的样本。假设每个回合的长度是 $L$。有 $ |

## 7.2 动作值的时序差分学习：Sarsa
### 7.2.1 算法陈述
首先，我们的目标是估计给定策略 $\pi$ 的动作值。
假设我们有一些经验 $\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\}_t$。
我们可以使用以下 Sarsa 算法来估计动作值：
$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})]\right], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t),
\end{aligned}
$$
其中 $t = 0, 1, 2, \dots$。
* $q_t(s_t, a_t)$ 是 $q_\pi(s_t, a_t)$ 的一个估计；
* $\alpha_t(s_t, a_t)$ 是取决于 $s_t, a_t$ 的学习率。

为什么这个算法叫 Sarsa？那是因为算法的每一步都涉及到 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$。Sarsa 是 state-action-reward-state-action 的缩写。
Sarsa 与之前的 TD 学习算法有什么关系？我们可以通过将 TD 算法中的状态值估计 $v(s)$ 替换为动作值估计 $q(s,a)$ 来获得 Sarsa。因此，Sarsa 是 TD 算法的动作值版本。

Sarsa 算法在数学上做了什么？Sarsa 的表达式表明它是一个求解以下方程的随机近似算法：
$$
q_\pi(s, a) = \mathbb{E}[R + \gamma q_\pi(S', A') | s, a], \quad \forall s, a.
$$
这是用动作值表达的贝尔曼方程的另一个表达式。

**证明**
用动作值表示的贝尔曼方程是
$$
\begin{aligned}
q_\pi(s, a) &= \sum_r rp(r|s, a) + \gamma \sum_{s'} \sum_{a'} q_\pi(s', a')p(s'|s, a)\pi(a'|s') \\
&= \sum_r rp(r|s, a) + \gamma \sum_{s'} p(s'|s, a) \sum_{a'} q_\pi(s', a')\pi(a'|s'). \quad (a)
\end{aligned}
$$
这个方程建立了动作值之间的关系。由于
$$
\begin{aligned}
p(s', a'|s, a) &= p(s'|s, a)p(a'|s', s, a) \\
&= p(s'|s, a)p(a'|s') \quad (\text{由于条件独立性}) \\
&\doteq p(s'|s, a)\pi(a'|s'),
\end{aligned}
$$
(a) 可以改写为
$$
q_\pi(s, a) = \sum_r rp(r|s, a) + \gamma \sum_{s'} \sum_{a'} q_\pi(s', a')p(s', a'|s, a).
$$
根据期望值的定义，上述方程等价于
$$
q_\pi(s, a) = \mathbb{E}[R + \gamma q_\pi(S', A') | s, a], \quad \forall s, a.
$$
因此，它是贝尔曼方程。

### 7.2.2 算法的收敛性
**定理（Sarsa 学习的收敛性）**
根据 Sarsa 算法，当 $t \to \infty$ 时，对于所有 $(s, a)$，$q_t(s, a)$ 以概率 1 收敛到动作值 $q_\pi(s, a)$，如果 $\sum_t \alpha_t(s, a) = \infty$ 且 $\sum_t \alpha_t^2(s, a) < \infty$ 对于所有 $(s, a)$。

备注：这个定理说明了给定策略 $\pi$ 的动作值可以通过 Sarsa 找到。

RL 的最终目标是找到最优策略。
为此，我们可以将 Sarsa 与策略改进步骤结合起来。
组合后的算法也称为 Sarsa。

**伪代码：Sarsa 的策略搜索**
对于每个回合，执行：
如果当前 $s_t$ 不是目标状态，执行：
    收集经验 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$：特别地，执行遵循 $\pi_t(s_t)$ 的动作 $a_t$，生成 $r_{t+1}, s_{t+1}$，然后执行遵循 $\pi_t(s_{t+1})$ 的动作 $a_{t+1}$。
    更新 q 值：$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})]\right]$
    更新策略：$$
    \begin{aligned}
    \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\mathcal{A}|}(|\mathcal{A}| - 1) \quad \text{if } a = \arg \max_a q_{t+1}(s_t, a) \\
    \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\mathcal{A}|} \quad \text{otherwise}
    \end{aligned}
    $$

关于此算法的备注：
* 策略 $s_t$ 在 $q(s_t, a_t)$ 更新后立即更新。这是基于广义策略迭代的思想。
* 策略是 $\epsilon$-贪婪而不是贪婪，以便很好地平衡利用和探索。

明确核心思想和复杂性：
* 核心思想很简单：即使用算法求解给定策略的贝尔曼方程。
* 当我们尝试寻找最优策略并高效工作时，复杂性就出现了。

## 7.3 动作值的时序差分学习：Expected Sarsa
Sarsa 的一个变体是期望 Sarsa 算法：
$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma \mathbb{E}[q_t(s_{t+1}, A)]]\right], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t),
\end{aligned}
$$
其中
$$
\mathbb{E}[q_t(s_{t+1}, A)] = \sum_a \pi_t(a|s_{t+1}) q_t(s_{t+1}, a) \doteq v_t(s_{t+1})
$$
是策略 $\pi_t$ 下 $q_t(s_{t+1}, a)$ 的期望值。

与 Sarsa 相比：
* TD 目标从 Sarsa 中的 $r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$ 变为期望 Sarsa 中的 $r_{t+1} + \gamma \mathbb{E}[q_t(s_{t+1}, A)]$。
* 需要更多的计算。但它是有益的，因为它减少了估计方差，因为它将 Sarsa 中的随机变量从 $\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\}$ 减少到 $\{s_t, a_t, r_{t+1}, s_{t+1}\}$。

该算法在数学上做了什么？期望 Sarsa 是一个求解以下方程的随机近似算法：
$$
q_\pi(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \mathbb{E}_{A_{t+1} \sim \pi(S_{t+1})}\left[q_\pi(S_{t+1}, A_{t+1})\right] \middle| S_t = s, A_t = a\right], \quad \forall s, a.
$$
上述方程是贝尔曼方程的另一个表达式：
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a],
$$

## 7.4 动作值的时序差分学习：n-步 Sarsa
$n$-步 Sarsa：可以统一 Sarsa 和蒙特卡洛学习
动作值的定义是
$$
q_\pi(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a].
$$
折扣回报 $G_t$ 可以用不同的形式表示为
$$
\begin{aligned}
\text{Sarsa } \leftarrow \quad G_t^{(1)} &= R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}), \\
G_t^{(2)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, A_{t+2}), \\
&\vdots \\
\text{$n$-step Sarsa } \leftarrow \quad G_t^{(n)} &= R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n}), \\
&\vdots \\
\text{MC } \leftarrow \quad G_t^{(\infty)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\end{aligned}
$$
应该注意的是 $G_t = G_t^{(1)} = G_t^{(2)} = G_t^{(n)} = G_t^{(\infty)}$，其中上标仅仅表示 $G_t$ 的不同分解结构。

比较：
* Sarsa 旨在解决
$$
q_\pi(s, a) = \mathbb{E}[G_t^{(1)} | s, a] = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | s, a].
$$
* MC 学习旨在解决
$$
q_\pi(s, a) = \mathbb{E}[G_t^{(\infty)} | s, a] = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | s, a].
$$
* 一个被称为 $n$-步 Sarsa 的中间算法旨在解决
$$
q_\pi(s, a) = \mathbb{E}[G_t^{(n)} | s, a] = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n}) | s, a].
$$
* $n$-步 Sarsa 算法是
$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})]\right].
$$
$n$-步 Sarsa 更通用，因为它在 $n = 1$ 时变为 (一步) Sarsa 算法，在 $n = \infty$ 时变为 MC 学习算法。

$n$-步 Sarsa 需要 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \dots, r_{t+n}, s_{t+n}, a_{t+n})$。
由于 $(r_{t+n}, s_{t+n}, a_{t+n})$ 在时间 $t$ 尚未收集到，我们无法在步骤 $t$ 实现 $n$-步 Sarsa。但是，我们可以等到时间 $t+n$ 再更新 $(s_t, a_t)$ 的 q 值：
$$
\begin{aligned}
q_{t+n}(s_t, a_t) 
& = q_{t+n-1}(s_t, a_t) \\
& - \alpha_{t+n-1}(s_t, a_t) \left[q_{t+n-1}(s_t, a_t)
- [r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_{t+n-1}(s_{t+n}, a_{t+n})]\right] 
\end{aligned}
$$
由于 $n$-步 Sarsa 将 Sarsa 和 MC 学习作为两个极端情况，其性能是 Sarsa 和 MC 学习的混合：
* 如果 $n$ 很大，其性能接近 MC 学习，因此方差大但偏差小。
* 如果 $n$ 很小，其性能接近 Sarsa，因此由于初始猜测和相对较低的方差而具有相对较大的偏差。

最后，$n$-步 Sarsa 也用于策略评估。它可以与策略改进步骤相结合以搜索最优策略。

## 7.5 最优动作值的时序差分学习：Q-learning
### 7.5.1 算法陈述
Q-learning 算法是
$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)]\right], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t),
\end{aligned}
$$
Q-learning 与 Sarsa 非常相似。它们仅在 TD 目标方面不同：
* Q-learning 中的 TD 目标是 $r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)$。
* Sarsa 中的 TD 目标是 $r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。

Q-learning 在数学上做了什么？
它旨在解决
$$
q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \middle| S_t = s, A_t = a\right], \quad \forall s, a.
$$
这是用动作值表示的贝尔曼最优性方程。

**证明**
根据期望的定义，
$$
q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \middle| S_t = s, A_t = a\right], \quad \forall s, a.
$$
可以改写为
$$
q(s, a) = \sum_r p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a) \max_{a' \in \mathcal{A}(s')} q(s', a').
$$
对等式两边取最大值得到
$$
\max_{a \in \mathcal{A}(s)} q(s, a) = \max_{a \in \mathcal{A}(s)} \left[\sum_r p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a) \max_{a' \in \mathcal{A}(s')} q(s', a')\right].
$$
通过表示 $v(s) \doteq \max_{a \in \mathcal{A}(s)} q(s, a)$，我们可以将上述方程改写为
$$
\begin{aligned}
v(s) &= \max_{a \in \mathcal{A}(s)} \left[\sum_r p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v(s')\right] \\
&= \max_\pi \sum_{a \in \mathcal{A}(s)} \pi(a|s) \left[\sum_r p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v(s')\right],
\end{aligned}
$$
这显然是用状态值表示的贝尔曼最优性方程。

### 7.5.2 on-policy 和 off-policy
在进一步研究 Q-learning 之前，我们首先介绍两个重要概念：on-policy 学习和 off-policy 学习。
在一个 TD 学习任务中存在两种策略：
* 行为策略用于生成经验样本。
* 目标策略不断更新以趋向最优策略。
On-policy vs off-policy：
* 当行为策略与目标策略相同时，这种学习称为 on-policy。
* 当它们不同时，这种学习称为 off-policy。

off-policy 学习的优点：
* 它可以基于由任何其他策略生成的经验样本来搜索最优策略。
* 作为一个重要的特例，行为策略可以选择为探索性的。例如，如果我们想估计所有状态-动作对的动作值，我们可以使用一个探索性策略来生成访问每个状态-动作对足够多次的回合。

如何判断一个 TD 算法是 on-policy 还是 off-policy？
* 首先，检查算法在数学上做了什么。
* 其次，检查实现算法需要哪些东西。
这值得特别关注，因为它是最令人困惑的问题之一。

Sarsa 是 on-policy 的。
首先，Sarsa 旨在求解给定策略 $\pi$ 的贝尔曼方程：
$$
q_\pi(s, a) = \mathbb{E}[R + \gamma q_\pi(S', A') | s, a], \quad \forall s, a.
$$
其中 $R \sim p(R|s, a)$, $S' \sim p(S'|s, a)$, $A' \sim \pi(A'|S')$。
其次，算法是
$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})]\right],
$$
这需要 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$：
* 如果 $(s_t, a_t)$ 已知，那么 $r_{t+1}$ 和 $s_{t+1}$ 不依赖于任何策略。
* $a_{t+1}$ 是根据 $\pi_t(s_{t+1})$ 生成的。
* $\pi_t$ 既是目标策略也是行为策略。

蒙特卡洛学习是 on-policy 的。
首先，MC 方法旨在解决
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \dots | S_t = s, A_t = a], \quad \forall s, a.
$$
其中样本是根据给定策略 $\pi$ 生成的。
其次，MC 方法的实现是
$$
q(s, a) \approx r_{t+1} + \gamma r_{t+2} + \dots
$$
一个策略用于生成样本，这些样本进一步用于估计策略的动作值。基于动作值，我们可以改进策略。

Q-learning 是 off-policy 的。
首先，Q-learning 旨在解决贝尔曼最优性方程
$$
q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \middle| S_t = s, A_t = a\right], \quad \forall s, a.
$$
其次，算法是
$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - [r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)]\right]
$$
这需要 $(s_t, a_t, r_{t+1}, s_{t+1})$。
* 如果 $(s_t, a_t)$ 已知，那么 $r_{t+1}$ 和 $s_{t+1}$ 不依赖于任何策略。
* 从 $s_t$ 生成 $a_t$ 的行为策略可以是任何策略。目标策略是最优策略。

由于 Q-learning 是 off-policy 的，它可以用 off-policy 或 on-policy 的方式实现。

### 7.5.3 算法总结
**通过 Q-learning (on-policy 版本) 的最优策略学习**
初始化：$\alpha_t(s, a) = \alpha > 0$ 对于所有 $(s, a)$ 和所有 $t$. $\epsilon \in (0, 1)$. 初始 $q_0(s, a)$ 对于所有 $(s, a)$. 从 $q_0$ 派生初始 $\epsilon$-greedy 策略 $\pi_0$.
目标：学习一条最优路径，该路径可以引导智能体从初始状态 $s_0$ 到达目标状态。

对于每个回合，执行：
	  如果 $s_t$ ($t=0, 1, 2, \dots$) 不是目标状态，执行：
	    收集经验样本 $(a_t, r_{t+1}, s_{t+1})$ 给定 $s_t$：根据 $\pi_t(s_t)$ 生成 $a_t$；通过与环境交互生成 $r_{t+1}, s_{t+1}$.
	    更新 $q$-值 对于 $(s_t, a_t)$：
		    $q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[q_t(s_t, a_t) - (r_{t+1} + \gamma \max_a q_t(s_{t+1}, a))\right]$
	    更新策略 对于 $s_t$：$$
    \begin{aligned}
    \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\mathcal{A}(s_t)|}(|\mathcal{A}(s_t)| - 1) \quad \text{if } a = \arg \max_a q_{t+1}(s_t, a) \\
    \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\mathcal{A}(s_t)|} \quad \text{otherwise}
    \end{aligned}
    $$

**通过 Q-learning (off-policy 版本) 的最优策略学习**
初始化：对所有 $(s, a)$ 初始猜测 $q_0(s, a)$。对所有 $(s, a)$ 行为策略 $\pi_b(a|s)$。$\alpha_t(s, a) = \alpha > 0$ 对所有 $(s, a)$ 和所有 $t$。
目标：从 $\pi_b$ 生成的经验样本中学习所有状态的最优目标策略 $\pi_T$。
对于由 $\pi_b$ 生成的每个回合 $\{s_0, a_0, r_1, s_1, a_1, r_2, \dots \}$，执行：
	对于回合的每个步骤 $t=0, 1, 2, \dots$，执行：
	    更新 $q$-值 对于 $(s_t, a_t)$：
		    $q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[q_t(s_t, a_t) - (r_{t+1} + \gamma \max_a q_t(s_{t+1}, a))\right]$
	    更新目标策略 对于 $s_t$：$$
    \begin{aligned}
    \pi_{T,t+1}(a|s_t) &= 1 \quad \text{if } a = \arg \max_a q_{t+1}(s_t, a) \\
    \pi_{T,t+1}(a|s_t) &= 0 \quad \text{otherwise}
    \end{aligned}
    $$


## 7.6 时序差分学习算法的统一表示
我们在本节中介绍的所有算法都可以用统一的表达式表示：
$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \bar{q}_t\right],
$$
其中 $\bar{q}_t$ 是 TD 目标。
不同的 TD 算法有不同的 $\bar{q}_t$。

Sarsa：
$$
\bar{q}_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})
$$
$n$-step Sarsa：
$$
\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})
$$
Expected Sarsa：
$$
\bar{q}_t = r_{t+1} + \gamma \sum_a \pi_t(a\|s_{t+1}) q_t(s_{t+1}, a)
$$
Q-learning：
$$
\bar{q}_t = r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)
$$
Monte Carlo：
$$
\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots
$$ 

MC 方法也可以通过设置 $\alpha_t(s_t, a_t) = 1$ 来用这种统一表达式表示，从而 $q_{t+1}(s_t, a_t) = \bar{q}_t$。

所有算法都可以被视为求解贝尔曼方程或贝尔曼最优性方程的随机近似算法：
Sarsa：
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})|S_t = s, A_t = a]
$$
$n$-step Sarsa：
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n})|S_t = s, A_t = a]
$$
Expected Sarsa：
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma \mathbb{E}_{A_{t+1}}[q_\pi(S_{t+1}, A_{t+1})]|S_t = s, A_t = a]
$$
Q-learning：
$$
q(s, a) = \mathbb{E}[R_{t+1} + \max_a q(S_{t+1}, a)|S_t = s, A_t = a]
$$
Monte Carlo：
$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \dots|S_t = s, A_t = a]
$$

# 8 价值函数方法
## 8.1 曲线拟合
在本书中，到目前为止，状态值和行动值都是用表格表示的。

例如，动作价值：

| | $a_1$ | $a_2$ | $a_3$ | $a_4$ | $a_5$ |
|---|---|---|---|---|---|
| $s_1$ | $q_{\pi}(s_1, a_1)$ | $q_{\pi}(s_1, a_2)$ | $q_{\pi}(s_1, a_3)$ | $q_{\pi}(s_1, a_4)$ | $q_{\pi}(s_1, a_5)$ |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $s_9$ | $q_{\pi}(s_9, a_1)$ | $q_{\pi}(s_9, a_2)$ | $q_{\pi}(s_9, a_3)$ | $q_{\pi}(s_9, a_4)$ | $q_{\pi}(s_9, a_5)$ |

优点：直观且易于分析
缺点：难以处理大型或连续的状态或行动空间。两个方面：1) 存储；2) 泛化能力

考虑一个例子：
* 假设存在一维状态 $s_1, \dots, s_{|S|}$。
* 它们的状态值是 $v_\pi(s_1), \dots, v_\pi(s_{|S|})$, 其中 $\pi$ 是一个给定的策略。
* 假设 $|S|$ 非常大，我们希望使用一个简单的曲线来近似这些点以节省存储。

首先，我们使用最简单的直线来拟合这些点。
假设直线的方程是
$$
\hat{v}(s, w) = as + b = \underbrace{\begin{bmatrix} s & 1 \end{bmatrix}}_{\phi^T(s)} \underbrace{\begin{bmatrix} a \\ b \end{bmatrix}}_{w} = \phi^T(s)w
$$
其中
* $w$ 是参数向量，
* $\phi(s)$ 是 $s$ 的特征向量，
* $\hat{v}(s, w)$ 关于 $w$ 是线性的。

其优点是：
* 表格表示需要存储 $|S|$ 个状态值。现在，我们只需要存储两个参数 $a$ 和 $b$。
* 每次我们想要使用 $s$ 的值时，我们可以计算 $\phi^T(s)w$。
* 这样的优点并非没有代价。它伴随着一个成本：状态值无法准确表示。这就是为什么这种方法被称为**价值近似**。

其次，我们也可以使用二阶曲线来拟合这些点：
$$
\hat{v}(s, w) = as^2 + bs + c = \underbrace{\begin{bmatrix} s^2 & s & 1 \end{bmatrix}}_{\phi^T(s)} \underbrace{\begin{bmatrix} a \\ b \\ c \end{bmatrix}}_{w} = \phi^T(s)w.
$$
在这种情况下，
* $w$ 和 $\phi(s)$ 的维度增加，但值可能拟合得更准确。
* 尽管 $\hat{v}(s, w)$ 关于 $s$ 是非线性的，但它关于 $w$ 是线性的。非线性包含在 $\phi(s)$ 中。

第三，我们甚至可以使用更高阶的多项式曲线或其他复杂的曲线来拟合这些点。
* 优点：它可以更好地近似。
* 缺点：它需要更多的参数。

总结：
* 思想：使用参数化函数近似状态和动作值：$\hat{v}(s, w) \approx v_{\pi}(s)$，其中 $w \in \mathbb{R}^m$ 是参数向量。
* 优点：
  1) 存储：$w$ 的维度可能远小于 $|S|$。
  2) 泛化：当访问一个状态 $s$ 时，参数 $w$ 会更新，从而使一些其他未访问状态的值也可以更新。通过这种方式，学习到的值可以泛化到未访问的状态。

## 8.2 状态价值估计的算法
### 8.2.1 目标函数
介绍：
* 设 $v_{\pi}(s)$ 是真实状态值，$\hat{v}(s, w)$ 是一个近似函数。
* 我们的目标是找到一个最优 $w$，使得 $\hat{v}(s, w)$ 能够最好地近似每个 $s$ 的 $v_{\pi}(s)$。
* 这是一个策略评估问题。稍后我们将扩展到策略改进。
* 为了找到最优的 $w$，我们需要两个步骤。
  * 第一步是定义一个目标函数。
  * 第二步是推导出优化该目标函数的算法。

目标函数是
$$
J(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2].
$$
我们的目标是找到能使 $J(w)$ 最小化的最佳 $w$。
期望是关于随机变量 $S \in \mathcal{S}$ 的。$S$ 的概率分布是什么？
- 这通常令人困惑，因为我们到目前为止还没有讨论状态的概率分布。
- 有几种方法可以定义 $S$ 的概率分布。

第一种方法是使用**均匀分布**。
也就是说，通过将每个状态的概率设置为 $1/|S|$ 来对待所有状态都同等重要。
在这种情况下，目标函数变为
$$
J(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2] = \frac{1}{|S|}\sum_{s \in \mathcal{S}}(v_{\pi}(s) - \hat{v}(s, w))^2.
$$
缺点：状态可能不是同等重要的。例如，某些状态可能很少被策略访问。因此，这种方法没有考虑给定策略下马尔可夫过程的真实动态。

第二种方法是使用**平稳分布**。
平稳分布是本课程中将频繁使用的一个重要概念。简而言之，它描述了马尔可夫过程的长期行为。
令 $\{d_{\pi}(s)\}_{s \in \mathcal{S}}$ 表示马尔可夫过程在策略 $\pi$ 下的平稳分布。根据定义，$d_{\pi}(s) \geq 0$ 且 $\sum_{s \in \mathcal{S}} d_{\pi}(s) = 1$。
目标函数可以改写为
$$
J(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2] = \sum_{s \in \mathcal{S}} d_{\pi}(s)(v_{\pi}(s) - \hat{v}(s, w))^2.
$$
此函数是一个加权平方误差。
由于更频繁访问的状态具有更高的 $d_{\pi}(s)$ 值，因此它们在目标函数中的权重也高于那些很少访问的状态。

关于平稳分布的更多解释：
* 分布：状态的分布
* 平稳：长期行为
* 总结：在智能体遵循某个策略长时间运行后，智能体处于任何状态的概率都可以用这种分布来描述。

### 8.2.2 优化算法
虽然我们有了目标函数，但下一步是优化它。
为了最小化目标函数 $J(w)$，我们可以使用梯度下降算法：
$$
w_{k+1} = w_k - \alpha_k \nabla_w J(w_k)
$$
真实梯度是
$$
\begin{aligned}
\nabla_w J(w) &= \nabla_w \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2] \\
&= \mathbb{E}[\nabla_w (v_{\pi}(S) - \hat{v}(S, w))^2] \\
&= \mathbb{E}[2(v_{\pi}(S) - \hat{v}(S, w))(-\nabla_w \hat{v}(S, w))] \\
&= -2\mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))\nabla_w \hat{v}(S, w)]
\end{aligned}
$$
上面的真实梯度涉及期望的计算。

我们可以使用随机梯度来代替真实梯度：
$$
w_{t+1} = w_t + \alpha_t(v_{\pi}(s_t) - \hat{v}(s_t, w_t))\nabla_w \hat{v}(s_t, w_t),
$$
其中 $s_t$ 是 $S$ 的一个样本。这里， $2\alpha_k$ 合并为 $\alpha_k$。

该算法不可实现，因为它需要真实状态值 $v_{\pi}$，而这是待估计的未知量。
我们可以用一个近似值来替换 $v_{\pi}(s_t)$，这样算法就变得可实现。

特别是，
基于函数近似的蒙特卡洛学习：
设 $g_t$ 为该情节中从 $s_t$ 开始的折扣回报。那么，$g_t$ 可以用来近似 $v_{\pi}(s_t)$。算法变为
$$
w_{t+1} = w_t + \alpha_t(g_t - \hat{v}(s_t, w_t))\nabla_w \hat{v}(s_t, w_t).
$$
基于函数近似的时序差分学习：
根据时序差分学习的精神，$r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t)$ 可以被视为 $v_{\pi}(s_t)$ 的一个近似。那么，算法变为
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t).
$$

**伪代码：基于函数近似的时序差分学习**
初始化：一个关于 $w$ 可微的函数 $\hat{v}(s, w)$。初始参数 $w_0$。
目标：近似给定策略 $\pi$ 的真实状态值。
对于遵循策略 $\pi$ 生成的每个情节，执行
  对于每个步骤 $(s_t, r_{t+1}, s_{t+1})$，执行
    在一般情况下，
	    $w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t)$
    在线性情况下，
	    $w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t \right] \phi(s_t)$

它只能估计给定策略的状态值，但理解稍后介绍的其他算法很重要。

### 8.2.3 函数近似器的选取
一个尚未回答的重要问题：如何选择函数 $\hat{v}(s, w)$？
* 第一种方法，在以前广泛使用**，是使用线性函数$$
  \hat{v}(s, w) = \phi^T(s)w
  $$这里，$\phi(s)$ 是特征向量，可以是多项式基、傅里叶基，...。我们已经在启发性示例中看到，稍后将在说明性示例中再次看到。
* 第二种方法，如今广泛使用，是使用神经网络作为非线性函数近似器。
  神经网络的输入是状态，输出是 $\hat{v}(s, w)$，网络参数是 $w$。

线性函数近似：
在线性情况下，其中 $\hat{v}(s, w) = \phi^T(s)w$，我们有
$$
\nabla_w \hat{v}(s, w) = \phi(s).
$$
将梯度代入时序差分算法
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t)
$$
得到
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t \right] \phi(s_t),
$$
这就是基于线性函数近似的时序差分学习算法，简称其为 TD-Linear。

线性函数近似的缺点：
- 难以选择合适的特征向量。
线性函数近似的优点：
- 线性情况下TD算法的理论性质比非线性情况下更容易理解。
- 线性函数近似仍然很强大，因为表格表示仅仅是线性函数近似的一个特例。

我们接下来展示表格表示是线性函数近似的一个特例。
首先，考虑状态 $s$ 的特殊特征向量：
$$
\phi(s) = e_s \in \mathbb{R}^{|S|},
$$
其中 $e_s$ 是一个向量，其第 $s$ 个分量为 1，其余分量为 0。
在这种情况下，
$$
\hat{v}(s, w) = e_s^T w = w(s),
$$
其中 $w(s)$ 是 $w$ 的第 $s$ 个分量。

回顾 TD-Linear 算法是
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t \right] \phi(s_t),
$$
当 $\phi(s_t) = e_{s_t}$ 时，上述算法变为
$$
w_{t+1} = w_t + \alpha_t (r_{t+1} + \gamma w_t(s_{t+1}) - w_t(s_t)) e_{s_t}.
$$
这是一个向量方程，仅仅更新 $w_t$ 的 $s_t$ 分量。
方程两边乘以 $e_{s_t}^T$ 得到
$$
w_{t+1}(s_t) = w_t(s_t) + \alpha_t (r_{t+1} + \gamma w_t(s_{t+1}) - w_t(s_t)),
$$
这正是表格时序差分算法。

### 8.2.4 总结
到目前为止，我们已经完成了基于价值函数近似的时序差分学习的故事。
这个故事从目标函数开始：
$$
J(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2].
$$
目标函数表明这是一个策略评估问题。

梯度下降算法是
$$
w_{t+1} = w_t + \alpha_t(v_{\pi}(s_t) - \hat{v}(s_t, w_t))\nabla_w \hat{v}(s_t, w_t).
$$
算法中未知的真实价值函数被一个近似值取代，从而得到算法：
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t).
$$
尽管这个故事有助于理解基本思想，但它不是数学上严谨的。

该算法
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t)
$$
没有最小化以下目标函数：
$$
J(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2]
$$

### 8.2.5 进一步分析
#### 8.2.5.1 收敛性分析
为了研究基于函数近似的时序差分学习：
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t) \right] \nabla_w \hat{v}(s_t, w_t).
$$
的收敛性质，我们首先考虑以下确定性算法：
$$
w_{t+1} = w_t + \alpha_t\mathbb{E}\left[(r_{t+1} + \gamma\phi^T(s_{t+1})w_t - \phi^T(s_t)w_t)\phi(s_t)\right], \quad (a)
$$
其中期望是根据随机变量 $s_t, s_{t+1}, r_{t+1}$ 计算的。$s_t$ 的分布假定为平稳分布 $d_\pi$。算法 (a) 是确定性的，因为在计算期望后，随机变量 $s_t, s_{t+1}, r_{t+1}$ 都消失了。

我们为什么要考虑这个确定性算法？首先，这个确定性算法的收敛性更容易（尽管并非微不足道）分析。其次，更重要的是，这个确定性算法的收敛性意味着基于函数近似的时序差分学习的收敛性。这是因为基于函数近似的时序差分学习可以被看作是 (a) 的随机梯度下降 (SGD) 实现。因此，我们只需要研究确定性算法的收敛性质。

尽管 (a) 的表达式乍看起来可能很复杂，但它可以大大简化。为此，请注意：
$$
\Phi = \begin{bmatrix} \vdots \\ \phi^T(s) \\ \vdots \end{bmatrix} \in \mathbb{R}^{n \times m}, \quad D = \begin{bmatrix} \ddots \\ & d_\pi(s) \\ & & \ddots \end{bmatrix} \in \mathbb{R}^{n \times n}, \quad (b)
$$
其中 $\Phi$ 是包含所有特征向量的矩阵，$D$ 是一个对角矩阵，其对角线元素是平稳分布。这两个矩阵将经常使用。

**引理**
(a) 中的期望可以重写为
$$
\mathbb{E}\left[(r_{t+1} + \gamma\phi^T(s_{t+1})w_t - \phi^T(s_t)w_t)\phi(s_t)\right] = b - Aw_t,
$$
其中
$$
\begin{aligned}
A &\equiv \Phi^T D(I - \gamma P_\pi)\Phi \in \mathbb{R}^{m \times m}, \\
b &\equiv \Phi^T D r_\pi \in \mathbb{R}^m.
\end{aligned} \quad (c)
$$
这里，$P_\pi, r_\pi$ 是贝尔曼方程 $v_\pi = r_\pi + \gamma P_\pi v_\pi$ 中的两个项，$I$ 是具有适当维度的单位矩阵。

结合引理的表达式，确定性算法 (a) 可以重写为
$$
w_{t+1} = w_t + \alpha_t(b - Aw_t), \quad (d)
$$
这是一个简单的确定性过程。其收敛性分析如下。

首先，收敛的 $w_t$ 值是什么？假设 $w_t$ 随着 $t \to \infty$ 收敛到一个常数值 $w^*$，那么 (d) 暗示 $w^* = w^* + \alpha_\infty(b - Aw^*)$，这表明 $b - Aw^* = 0$，因此
$$
w^* = A^{-1}b.
$$
下面给出关于这个收敛值的一些注意：
 - $A$ 是否可逆？答案是肯定的。事实上 $A$ 不仅可逆，而且是正定的。也就是说，对于任何非零向量 $x$，都有 $x^T A x > 0$。
 - 对 $w^* = A^{-1}b$ 的解释是什么？它实际上是最小化投影贝尔曼误差的最优解。
 - 表格法是一个特例。一个有趣的结论是，当维度 $w$ 等于 $|S|$ 且 $\phi(s) = [0, \dots, 1, \dots, 0]^T$（其中对应于 $s$ 的分量为 1）时，我们有$$
w^* = A^{-1}b = v_\pi. \quad (e)
$$这个方程表明要学习的参数向量实际上是真实的状态值。这个结论与 TD 算法是 TD-linear 算法的一个特例这一事实是一致的。定理 e 的证明在下面给出。可以验证 $A = \Phi^T D(I - \gamma P_\pi)\Phi$ 和 $b = \Phi^T D r_\pi$ 在这种情况下意味着 $\Phi = I$。因此，$w^* = A^{-1}b = (I - \gamma P_\pi)^{-1} D^{-1} D r_\pi = (I - \gamma P_\pi)^{-1} r_\pi = v_\pi$。

其次，我们证明 $w_t$ 在 (d) 中收敛到 $w^* = A^{-1}b$ 当 $t \to \infty$。由于 (d) 是一个简单的确定性过程，它可以以多种方式证明。我们提出以下两种证明方式。

证明 1：定义收敛误差 $\delta_t \equiv w_t - w^*$。我们只需要证明 $\delta_t$ 收敛到零。为此，将 $w_t = \delta_t + w^*$ 代入 (d) 得到
$$
\delta_{t+1} = \delta_t - \alpha_t A\delta_t = (I - \alpha_t A)\delta_t.
$$
由此可知
$$
\delta_{t+1} = (I - \alpha_t A) \cdots (I - \alpha_0 A)\delta_0.
$$
考虑 $\alpha_t = \alpha$ 对所有 $t$ 成立的简单情况。那么我们有
$$
\|\delta_{t+1}\|_2 \le \|I - \alpha A\|^{t+1}\|\delta_0\|_2.
$$
当 $\alpha > 0$ 足够小时，我们有 $\|I - \alpha A\|_2 < 1$，因此 $\delta_t \to 0$ 当 $t \to \infty$。保持 $\|I - \alpha A\|_2 < 1$ 的原因是 $A$ 是正定且对于任何 $x$ 有 $x^T A x > 0$。

证明 2：考虑函数 $g(w) \equiv b - Aw$。由于 $w^*$ 是 $g(w) = 0$ 的根，任务实际上是一个寻根问题。算法 (d) 实际上是 Robbins-Monro (RM) 算法。尽管最初的 RM 算法是为随机过程设计的，但它也可以应用于确定性情况。RM 算法的收敛性可以被研究为 $w_t = w_t + \alpha_t(b - Aw_t)$。也就是说，当 $\sum_t \alpha_t = \infty$ 和 $\sum_t \alpha_t^2 < \infty$ 时，$w_t$ 收敛到 $w^*$。


**引理的证明**
利用全期望定律，我们有
$$
\begin{aligned}
& \mathbb{E}\left[r_{t+1}\phi(s_t) + \phi(s_t)(\gamma\phi^T(s_{t+1}) - \phi^T(s_t))w_t\right] \\ 
& = \sum_{s \in \mathcal{S}} d_\pi(s)\mathbb{E}\left[r_{t+1}\phi(s_t) + \phi(s_t)(\gamma\phi^T(s_{t+1}) - \phi^T(s_t))w_t \middle| s_t = s\right] \\
& = \sum_{s \in \mathcal{S}} d_\pi(s)\mathbb{E}\left[r_{t+1}\phi(s_t) \middle| s_t = s\right] + \sum_{s \in \mathcal{S}} d_\pi(s)\mathbb{E}\left[\phi(s_t)(\gamma\phi^T(s_{t+1}) - \phi^T(s_t))w_t \middle| s_t = s\right].
\end{aligned} \quad (f)
$$
这里，假定 $s_t$ 服从平稳分布 $d_\pi$。首先观察 (f) 中的第一项
$$
\mathbb{E}\left[r_{t+1}\phi(s_t) \middle| s_t = s\right] = \phi(s)\mathbb{E}\left[r_{t+1} \middle| s_t = s\right] = \phi(s)r_\pi(s),
$$
其中 $r_\pi(s) = \sum_a \pi(a|s)\sum_{r,p}r p(r, s, a)$。那么，(f) 中的第一项可以重写为
$$
\sum_{s \in \mathcal{S}} d_\pi(s)\mathbb{E}\left[r_{t+1}\phi(s_t) \middle| s_t = s\right] = \sum_{s \in \mathcal{S}} d_\pi(s)\phi(s)r_\pi(s) = \Phi^T D r_\pi, \quad (g)
$$
其中 $r_\pi = [\dots, r_\pi(s), \dots]^T \in \mathbb{R}^n$。

其次，考虑 (f) 中的第二项。由于
$$
\begin{aligned}
& \mathbb{E}\left[\phi(s_t)(\gamma\phi^T(s_{t+1}) - \phi^T(s_t))w_t \middle| s_t = s\right] \\
& = \mathbb{E}\left[-\phi(s)\phi^T(s)w_t \middle| s_t = s\right] + \mathbb{E}\left[\gamma\phi(s)\phi^T(s_{t+1})w_t \middle| s_t = s\right] \\
& = -\phi(s)\phi^T(s)w_t + \gamma\phi(s)\mathbb{E}\left[\phi^T(s_{t+1}) \middle| s_t = s\right] w_t \\
& = -\phi(s)\phi^T(s)w_t + \gamma\phi(s) \sum_{s' \in \mathcal{S}} p(s'|s)\phi^T(s')w_t,
\end{aligned}
$$
(f) 中的第二项变为
$$
\begin{aligned}
& \sum_{s \in \mathcal{S}} d_\pi(s)\mathbb{E}\left[\phi(s_t)(\gamma\phi^T(s_{t+1}) - \phi^T(s_t))w_t \middle| s_t = s\right] \\
& = \sum_{s \in \mathcal{S}} d_\pi(s)\left[-\phi(s)\phi^T(s)w_t + \gamma\phi(s) \sum_{s' \in \mathcal{S}} p(s'|s)\phi^T(s')w_t\right] \\
& = \sum_{s \in \mathcal{S}} d_\pi(s)\phi(s)[-\phi^T(s) + \gamma\sum_{s' \in \mathcal{S}} p(s'|s)\phi^T(s')]w_t \\
& = \Phi^T D(-\Phi + \gamma P_\pi\Phi)w_t \\
& = -\Phi^T D(I - \gamma P_\pi)\Phi w_t.
\end{aligned} \quad (h)
$$
结合 (g) 和 (h) 得到
$$
\begin{aligned}
\mathbb{E}\left[r_{t+1} + \gamma\phi^T(s_{t+1})w_t - \phi^T(s_t)w_t)\phi(s_t)\right] &= \Phi^T D r_\pi - \Phi^T D(I - \gamma P_\pi)\Phi w_t \\
&= b - Aw_t,
\end{aligned} \quad (j)
$$
其中 $b \equiv \Phi^T D r_\pi$ 和 $A \equiv \Phi^T D(I - \gamma P_\pi)\Phi$。

**证明 $A = \Phi^T D(I - \gamma P_\pi)\Phi$ 可逆且正定**
矩阵 $A$ 是正定的，如果对于任何非零向量 $x$ 都有 $x^T A x > 0$。如果 $A$ 是正定（或负定）的，则表示为 $A > 0$（或 $A < 0$）。这里，$>$ 和 $<$ 应该与逐元素比较区分开来。注意 $A$ 可能不是对称的。尽管是正定矩阵通常指对称矩阵，但非对称矩阵也可以是正定的。

我们接下来证明 $A > 0$，因此 $A$ 是可逆的。证明 $A > 0$ 的想法是证明
$$
D(I - \gamma P_\pi) \equiv M > 0. \quad (k)
$$
很明显，$M > 0$ 意味着 $\Phi^T M \Phi > 0$，因为 $\Phi$ 是一个具有列满秩的高矩阵（假设特征向量是线性独立的）。注意
$$
M = \frac{M + M^T}{2} + \frac{M - M^T}{2}.
$$
由于 $M - M^T$ 是斜对称的，并且对于任何 $x$，我们知道 $x^T(M - M^T)x = 0$。为了证明 $M > 0$ 当且仅当 $M + M^T > 0$，我们应用严格对角占优矩阵是正定的事实。

首先，它成立
$$
(M + M^T)\mathbf{1}_n > 0, \quad (l)
$$
其中 $\mathbf{1}_n = [1, \dots, 1]^T \in \mathbb{R}^n$。(l) 的证明如下。由于 $P_\pi \mathbf{1}_n = \mathbf{1}_n$，我们有 $M \mathbf{1}_n = D(I - \gamma P_\pi)\mathbf{1}_n = D(\mathbf{1}_n - \gamma\mathbf{1}_n) = (1 - \gamma)d_\pi$。此外，$M^T \mathbf{1}_n = (I - \gamma P_\pi)^T D \mathbf{1}_n = (I - \gamma P_\pi)^T d_\pi$，其中最后一个等式成立是因为 $P_\pi^T d_\pi = d_\pi$。总之，我们有
$$
(M + M^T)\mathbf{1}_n = 2(1 - \gamma)d_\pi.
$$
由于 $d_\pi$ 的所有分量都是正的，我们有 $(M + M^T)\mathbf{1}_n > 0$。

其次，(l) 的逐元素形式是
$$
\sum_{j=1}^n [M + M^T]_{ij} > 0, \quad i = 1, \dots, n,
$$
这可以进一步写成
$$
[M + M^T]_{ii} + \sum_{j \ne i} [M + M^T]_{ij} > 0.
$$
通过检查 (k) 中 $M$ 的表达式可以验证，对角线元素是正的，非对角线元素是非正的。因此，上述不等式可以重写为
$$
|[M + M^T]_{ii}| \ge \sum_{j \ne i} |[M + M^T]_{ij}|.
$$
上述不等式表明 $M + M^T$ 中第 $i$ 个对角线元素的绝对值大于同一行中非对角线元素的绝对值之和。因此，$M + M^T$ 是严格对角占优的，证明完成。

#### 8.2.5.2 使投影贝尔曼误差最小化
我们已经证明了 TD-Linear 算法收敛到 $w^* = A^{-1}b$，接下来我们证明 $w^*$ 是使投影贝尔曼误差最小化的最优解。为此，我们回顾三个目标函数。

第一个目标函数是
$$
J_E(w) = \mathbb{E}[(v_{\pi}(S) - \hat{v}(S, w))^2],
$$
已在 (8.3) 中介绍。根据期望的定义，$J_E(w)$ 可以用矩阵形式重新表示为
$$
J_E(w) = \|\hat{v}(w) - v_{\pi}\|_D^2,
$$
其中 $v_{\pi}$ 是真实状态值，$\hat{v}(w)$ 是近似值。这里，$\|\cdot\|_D^2$ 是一个加权范数：
$$
\|x\|_D^2 = x^T D x = \|D^{1/2}x\|_2^2
$$
其中 $D$ 在 (b) 中给出。
这是我们可以想象到的最简单的目标函数，当谈到函数近似时。然而，它依赖于真实的、未知的值。为了获得一个可实现的算法，我们必须考虑其他目标函数，例如贝尔曼误差和投影贝尔曼误差。

第二个目标函数是贝尔曼误差。特别是，由于 $v_{\pi}$ 满足贝尔曼方程 $v_{\pi} = r_{\pi} + \gamma P_{\pi}v_{\pi}$，因此期望估计值 $\hat{v}(w)$ 也应在最大程度上满足此方程。因此，贝尔曼误差是
$$
J_{BE}(w) = \|\hat{v}(w) - (r_{\pi} + \gamma P_{\pi}\hat{v}(w))\|_D^2 \doteq \|\hat{v}(w) - T_{\pi}(\hat{v}(w))\|_D^2. \quad (m)
$$
这里，$T_{\pi}(\cdot)$ 是贝尔曼算子。特别是，对于任何向量 $x \in \mathbb{R}^n$，贝尔曼算子定义为
$$
T_{\pi}(x) \doteq r_{\pi} + \gamma P_{\pi}x.
$$
最小化贝尔曼误差是一个标准的最小二乘问题。这里省略了解决方案的细节。

第三，值得注意的是，$J_{BE}(w)$ 在 (m) 中可能无法最小化为零，因为近似器的近似能力有限。相比之下，可以最小化为零的目标函数是投影贝尔曼误差：
$$
J_{PBE}(w) = \|\hat{v}(w) - MT_{\pi}(\hat{v}(w))\|_D^2,
$$
其中 $M \in \mathbb{R}^{n \times n}$ 是正交投影矩阵，它将任何向量几何投影到所有近似值的空间中。

实际上，TD 学习算法在 (8.13) 中旨在最小化投影贝尔曼误差 $J_{PBE}$，而不是 $J_E$ 或 $J_{BE}$。原因如下。为了简单起见，考虑线性情况，其中 $\hat{v}(w) = \Phi w$。这里，$\Phi$ 在 (b) 中定义。$\Phi$ 的值域空间是所有可能的线性近似的集合。那么，
$$
M = \Phi(\Phi^T D \Phi)^{-1}\Phi^T D \in \mathbb{R}^{n \times n}
$$
是投影矩阵，它将任何向量几何投影到值域空间 $\Phi$ 上。由于 $\hat{v}(w)$ 在 $\Phi$ 的值域空间中，我们总是可以找到一个 $w$ 值，使得 $J_{PBE}(w)$ 最小化为零。可以证明，最小化 $J_{PBE}(w)$ 的解是 $w^* = A^{-1}b$。也就是说
$$
w^* = A^{-1}b = \arg \min_w J_{PBE}(w),
$$

**我们接下来证明 $w^* = A^{-1}b$ 是最小化 $J_{PBE}(w)$ 的最优解。**
由于 $J_{PBE}(w) = 0 \Leftrightarrow \hat{v}(w) - MT_{\pi}(\hat{v}(w)) = 0$，我们只需要研究方程
$$
\hat{v}(w) = MT_{\pi}(\hat{v}(w)).
$$
在线性情况下，将 $\hat{v}(w) = \Phi w$ 和 $M$ 的表达式代入上述方程得到
$$
\Phi w = \Phi(\Phi^T D \Phi)^{-1}\Phi^T D(r_{\pi} + \gamma P_{\pi}\Phi w). \quad (n)
$$
由于 $\Phi$ 具有满列秩，我们有 $\Phi x = \Phi y \Leftrightarrow x = y$ 对于任意 $x, y$。因此，(n) 意味着
$$
\begin{aligned}
w &= (\Phi^T D \Phi)^{-1}\Phi^T D(r_{\pi} + \gamma P_{\pi}\Phi w) \\
& \Leftrightarrow \Phi^T D(r_{\pi} + \gamma P_{\pi}\Phi w) = (\Phi^T D \Phi)w \\
& \Leftrightarrow \Phi^T D r_{\pi} + \gamma \Phi^T D P_{\pi}\Phi w = (\Phi^T D \Phi)w \\
& \Leftrightarrow \Phi^T D r_{\pi} = (\Phi^T D \Phi - \gamma \Phi^T D P_{\pi}\Phi)w \\
& \Leftrightarrow w = (\Phi^T D(I - \gamma P_{\pi})\Phi)^{-1}\Phi^T D r_{\pi} = A^{-1}b,
\end{aligned}
$$
其中 $A, b$ 在 (c) 中给出。因此，$w^* = A^{-1}b$ 是最小化 $J_{PBE}(w)$ 的最优解。
由于 TD 算法旨在最小化 $J_{PBE}$ 而不是 $J_E$，因此自然会问估计值 $\hat{v}(w)$ 与真实状态值 $v_{\pi}$ 的接近程度。在线性情况下，最小化投影贝尔曼误差的估计值是 $\hat{v}(w^*) = \Phi w^*$。其与真实状态值 $v_{\pi}$ 的偏差满足
$$
\|\hat{v}(w^*) - v_{\pi}\|_D = \|\Phi w^* - v_{\pi}\|_D \leq \frac{1}{1 - \gamma} \min_w \|\hat{v}(w) - v_{\pi}\|_D = \frac{1}{1 - \gamma} \min_w \sqrt{J_E(w)}. \quad (o)
$$
这个不等式的证明在方框 8.6 中给出。不等式 (o) 表明 $\Phi w^*$ 和 $v_{\pi}$ 之间的差异以上述 $J_E(w)$ 的最小值作为上界。然而，这个界是松散的，特别是当 $\gamma$ 接近 1 时。因此，它主要具有理论价值。

**误差界 (o) 的证明**
注意到
$$
\begin{aligned}
\|\Phi w^* - v_{\pi}\|_D &= \|\Phi w^* - Mv_{\pi} + Mv_{\pi} - v_{\pi}\|_D \\
&\leq \|\Phi w^* - Mv_{\pi}\|_D + \|Mv_{\pi} - v_{\pi}\|_D \\
&= \|MT_{\pi}(\Phi w^*) - MT_{\pi}(v_{\pi})\|_D + \|Mv_{\pi} - v_{\pi}\|_D, \quad (p)
\end{aligned}
$$
其中最后一个等式是由于 $\Phi w^* = MT_{\pi}(\Phi w^*)$ 和 $v_{\pi} = T_{\pi}(v_{\pi})$。代入
$$
MT_{\pi}(\Phi w^*) - MT_{\pi}(v_{\pi}) = M(r_{\pi} + \gamma P_{\pi}\Phi w^*) - M(r_{\pi} + \gamma P_{\pi}v_{\pi}) = \gamma M P_{\pi}(\Phi w^* - v_{\pi})
$$
代入 (p) 得到
$$
\begin{aligned}
\|\Phi w^* - v_{\pi}\|_D &\leq \|\gamma M P_{\pi}(\Phi w^* - v_{\pi})\|_D + \|Mv_{\pi} - v_{\pi}\|_D \\
&\leq \gamma \|M\|_D \|P_{\pi}(\Phi w^* - v_{\pi})\|_D + \|Mv_{\pi} - v_{\pi}\|_D \\
&= \gamma \|\Phi w^* - v_{\pi}\|_D + \|Mv_{\pi} - v_{\pi}\|_D \quad (\text{因为 } \|M\|_D = 1) \\
&\leq \gamma \|\Phi w^* - v_{\pi}\|_D + \|Mv_{\pi} - v_{\pi}\|_D. \quad (\text{因为 } \|P_{\pi}x\|_D \leq \|x\|_D \text{ 对于所有 } x)
\end{aligned}
$$
$\|M\|_D = 1$ 和 $\|P_{\pi}x\|_D \leq \|x\|_D$ 的证明推迟到方框末尾。识别上述不等式得到
$$
\begin{aligned}
\|\Phi w^* - v_{\pi}\|_D &\leq \frac{1}{1 - \gamma} \|Mv_{\pi} - v_{\pi}\|_D \\
&= \frac{1}{1 - \gamma} \min_w \|\hat{v}(w) - v_{\pi}\|_D.
\end{aligned}
$$
其中最后一个等式是因为 $\|Mv_{\pi} - v_{\pi}\|_D$ 是 $v_{\pi}$ 与所有可能近似空间的正交投影之间的误差。因此，它是 $v_{\pi}$ 与 $\hat{v}(w)$ 之间误差的最小值。

我们接下来证明一些有用的事实，这些事实已经在此证明中使用。
加权范数的性质。根据定义，
$$
\|x\|_D = \sqrt{x^T D x} = \|D^{1/2}x\|_2,
$$
诱导矩阵范数是
$$
\|A\|_D = \max_{x \neq 0} \|Ax\|_D / \|x\|_D = \|D^{1/2}AD^{-1/2}\|_2.
$$
对于具有适当维度的矩阵 $A, B$，我们有
$$
\|ABx\|_D \leq \|A\|_D \|B\|_D \|x\|_D.
$$
为了验证这一点，
$$
\begin{aligned}
\|ABx\|_D 
&= \|D^{1/2}ABx\|_2 \\
&= \|D^{1/2}AD^{-1/2}D^{1/2}BD^{-1/2}D^{1/2}x\|_2 \\
&\leq \|D^{1/2}AD^{-1/2}\|_2 \|D^{1/2}BD^{-1/2}\|_2 \|D^{1/2}x\|_2 \\
&= \|A\|_D \|B\|_D \|x\|_D. \\
\end{aligned}
$$

证明 $\|M\|_D = 1$。这对于所有 $x, y$ 都是有效的。
$$
\|M\|_D = \|\Phi(\Phi^T D \Phi)^{-1}\Phi^T D\|_D = \|D^{1/2}\Phi(\Phi^T D \Phi)^{-1}\Phi^T D^{-1/2}\|_2 = 1.
$$
其中等式是由于矩阵在 $L_2$ 范数下是正交投影矩阵，并且任何正交投影矩阵的 $L_2$ 范数等于 1。
证明 $\|P_{\pi}x\|_D \leq \|x\|_D$ 对于所有 $x \in \mathbb{R}^n$。首先，
$$
\|P_{\pi}x\|_D^2 = x^T P_{\pi}^T D P_{\pi}x = \sum_{i,j} x_i [\Phi^T D P_{\pi}]_{ij} x_j = \sum_{i,j} x_i \left( \sum_k [P_{\pi}^T D]_{ik} [P_{\pi}]_{kj} \right) x_j,
$$
重新整理上述方程得到
$$
\begin{aligned}
\|P_{\pi}x\|_D^2 &= \sum_k [D]_{kk} \left( \sum_i [P_{\pi}]_{ki}x_i \right)^2 \\
&\leq \sum_k [D]_{kk} \left( \sum_i [P_{\pi}]_{ki}^2 x_i^2 \right) \quad (\text{由于 Jensen 不等式 }) \\
&= \sum_i \left( \sum_k [D]_{kk} [P_{\pi}]_{ki}^2 \right) x_i^2 \\
&= \sum_i [D]_{ii} x_i^2 \quad (\text{由于 } d_{\pi}^T P_{\pi} = d_{\pi}^T) \\
&= \|x\|_D^2.
\end{aligned}
$$
#### 8.2.5.3 最小二乘时序差分法
我们接下来介绍一种称为最小二乘 TD (LSTD) 的算法。与 TD-Linear 算法类似，LSTD 旨在最小化投影贝尔曼误差。然而，它比 TD-Linear 算法有一些优势。

回想一下投影贝尔曼误差最优参数 $w^*$ 的最小值，即 $w^* = A^{-1}b$，其中 $A = \Phi^T D(I - \gamma P_\pi)\Phi$ 且 $b = \Phi^T D r_\pi$。事实上，由 (j) 可知 $A$ 和 $b$ 也可以写为
$$
\begin{aligned}
A &= \mathbb{E}\left[\phi(s_t)(\phi(s_t) - \gamma\phi(s_{t+1}))^T\right], \\
b &= \mathbb{E}\left[r_{t+1}\phi(s_t)\right].
\end{aligned}
$$
上面两个方程表明 $A$ 和 $b$ 是 $s_t, s_{t+1}, r_{t+1}$ 的期望。LSTD 的思想很简单：如果我们能直接从随机样本中获得 $A$ 和 $b$ 的估计，即 $\hat{A}$ 和 $\hat{b}$，那么最优参数就可以直接估计为 $w^* \approx \hat{A}^{-1}\hat{b}$。

特别地，假设 $(s_0, r_1, s_1, \dots, s_t, r_{t+1}, s_{t+1}, \dots)$ 是由给定策略 $\pi$ 产生的轨迹。令 $\hat{A}_t$ 和 $\hat{b}_t$ 分别为在时刻 $t$ 时 $A$ 和 $b$ 的估计。它们通过样本的平均值计算：
$$
\begin{aligned}
\hat{A}_t &= \sum_{k=0}^{t-1} \phi(s_k)(\phi(s_k) - \gamma\phi(s_{k+1}))^T, \\
\hat{b}_t &= \sum_{k=0}^{t-1} r_{k+1}\phi(s_k).
\end{aligned} \quad (q)
$$
然后，估计的参数是
$$
\hat{w}_t = \hat{A}_t^{-1}\hat{b}_t.
$$
右侧 (q) 中是否缺少一个 $1/t$ 系数？事实上，为了简单起见，当省略它时，$w_t$ 的值仍然保持不变。由于 $\hat{A}$ 在 $t$ 很小时可能不是特别稳定，$\hat{A}_t$ 通常会通过一个小的常数矩阵 $\sigma I$ 偏置，其中 $I$ 是单位矩阵，$\sigma$ 是一个小的正数。

LSTD 的优点是它能更有效地利用经验样本，并且收敛速度快于 TD 方法。这是因为该算法是专门基于对最优解表达式的了解而设计的。我们对问题理解得越好，算法的设计就越好。

LSTD 的缺点如下。首先，它只能估计状态值。相比之下，TD 算法可以扩展到估计动作值，如下一节所示。此外，TD 算法允许非线性近似，而 LSTD 不允许。这是因为该算法是专门基于 $w^*$ 的表达式而设计的。其次，LSTD 的计算成本高于 TD，因为 TD 每次更新都更新一个 $m \times m$ 矩阵，而 TD 更新一个 $m$ 维向量。更重要的是，在每一步中，LSTD 都需要计算 $\hat{A}_t$ 的逆，其计算复杂度为 $O(m^3)$。解决这个问题的常用方法是直接更新 $\hat{A}_t$ 的逆，而不是更新 $\hat{A}_t$。特别地，$\hat{A}_{t+1}$ 可以递归地计算如下：
$$
\begin{aligned}
\hat{A}_{t+1} &= \sum_{k=0}^{t} \phi(s_k)(\phi(s_k) - \gamma\phi(s_{k+1}))^T \\
&= \sum_{k=0}^{t-1} \phi(s_k)(\phi(s_k) - \gamma\phi(s_{k+1}))^T + \phi(s_t)(\phi(s_t) - \gamma\phi(s_{t+1}))^T \\
&= \hat{A}_t + \phi(s_t)(\phi(s_t) - \gamma\phi(s_{t+1}))^T.
\end{aligned}
$$
上述表达式将 $\hat{A}_{t+1}$ 分解为两个矩阵之和。它的逆可以根据计算为
$$
\begin{aligned}
\hat{A}_{t+1}^{-1} &= \left(\hat{A}_t + \phi(s_t)(\phi(s_t) - \gamma\phi(s_{t+1}))^T\right)^{-1} \\
&= \hat{A}_t^{-1} - \frac{\hat{A}_t^{-1}\phi(s_t)(\phi(s_t) - \gamma\phi(s_{t+1}))^T\hat{A}_t^{-1}}{1 + (\phi(s_t) - \gamma\phi(s_{t+1}))^T\hat{A}_t^{-1}\phi(s_t)}.
\end{aligned}
$$
因此，我们可以直接存储和更新 $\hat{A}_t^{-1}$，避免计算矩阵逆。该递归算法不需要步长。然而，它需要设置 $\hat{A}_0^{-1}$ 的初始值，初始值可以选择为 $\hat{A}_0^{-1} = \sigma I$。

## 8.3 结合 Sarsa 进行函数近似
到目前为止，我们仅仅考虑了状态值估计问题。也就是说，我们希望
$$
\hat{v} \approx v_{\pi}
$$
为了寻找最优策略，我们需要估计动作值。

基于函数近似的 Sarsa 算法是
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t).
$$
这与我们前面介绍的算法相同，只是 $\hat{v}$ 被 $\hat{q}$ 替换了。


为了寻找最优策略，我们可以结合策略评估和策略改进。
**伪代码：基于函数近似的Sarsa算法**
目标：寻找一个能够将智能体从初始状态-动作对 $(s_0, a_0)$ 引导到目标的策略。
对于每个轮次，执行
如果当前 $s_t$ 不是目标状态，执行
	按照 $\pi_t(s_t)$ 选择动作 $a_t$，生成 $r_{t+1}, s_{t+1}$，然后按照 $\pi_t(s_{t+1})$ 选择动作 $a_{t+1}$
	价值更新（参数更新）：
		$w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)$
    策略更新：$$
    \begin{aligned}
    \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\mathcal{A}(s_t)|} - 1 \quad \text{if} \quad a = \underset{a \in \mathcal{A}(s_t)}{\arg \max} \hat{q}(s_t, a, w_{t+1}) \\
    \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\mathcal{A}(s_t)|} \quad \text{otherwise}
    \end{aligned}
    $$


## 8.4 结合 Q-learning 进行函数近似
类似于 Sarsa，表格 Q-learning 也可以扩展到价值函数近似的情况。

$q$ 值更新规则是
$$
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t),
$$
这与 Sarsa 相同，只是 $\hat{q}(s_{t+1}, a_{t+1}, w_t)$ 被 $\max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t)$ 替换了。

**伪代码：基于函数近似的 Q-learning（on-policy 版本）**
初始化：初始参数向量 $w_0$。初始策略 $\pi_0$。小 $\epsilon > 0$。
目标：寻找一个能够将智能体从初始状态-动作对 $(s_0, a_0)$ 引导到目标的良好策略。
对于每个轮次，执行
如果当前 $s_t$ 不是目标状态，执行
	按照 $\pi_t(s_t)$ 选择动作 $a_t$，并生成 $r_{t+1}, s_{t+1}$
    价值更新（参数更新）：$w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)$
    策略更新：$$
    \begin{aligned}
    \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\mathcal{A}(s_t)|} - 1 \quad \text{if} \quad a = \underset{a \in \mathcal{A}(s_t)}{\arg \max} \hat{q}(s_t, a, w_{t+1}) \\
    \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\mathcal{A}(s_t)|} \quad \text{otherwise}
    \end{aligned}
    $$

## 8.5 深度 Q-learning
#### 8.5.1 算法陈述
深度 Q-learning 或深度 Q-网络 (DQN)：
* 最早且最成功的将深度神经网络引入强化学习的算法之一。
* 神经网络的作用是作为非线性函数近似器。
* 与以下算法不同：$$
  w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)
  $$因为训练网络的方式不同。

深度 Q-learning 旨在最小化目标函数/损失函数：
$$
J(w) = \mathbb{E}\left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w) \right)^2 \right],
$$
其中 $(S, A, R, S')$ 是随机变量。

这实际上是贝尔曼最优误差。那是因为
$$
q(s, a) = \mathbb{E}\left[ R_{t+1} + \gamma \max_{a \in \mathcal{A}(S_{t+1})} q(S_{t+1}, a) \Big| S_t = s, A_t = a \right], \quad \forall s, a
$$
在期望意义上，$R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w)$ 的值应该为零。

如何最小化目标函数？梯度下降！
如何计算目标函数的梯度？很棘手。
那是因为，在这个目标函数中
$$
J(w) = \mathbb{E}\left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w) \right)^2 \right],
$$
参数 $w$ 不仅出现在 $\hat{q}(S, A, w)$ 中，还出现在
$$
y \doteq R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w)
$$
为了简单起见，我们在计算梯度时可以假设 $y$ 中的 $w$ 是固定的（至少暂时固定）。

#### 8.5.2 梯度下降技巧
为了做到这一点，我们可以引入两个网络。
* 一个是主网络，表示 $\hat{q}(s, a, w)$
* 另一个是目标网络 $\hat{q}(s, a, w_T)$。

在这种情况下，目标函数退化为
$$
J = \mathbb{E}\left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w) \right)^2 \right],
$$
其中 $w_T$ 是目标网络参数。

当 $w_T$ 固定时，$J$ 的梯度可以很容易地得到
$$
\nabla_w J = \mathbb{E}\left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w) \right) \nabla_w \hat{q}(S, A, w) \right].
$$
深度 Q-learning 的基本思想是使用梯度下降算法来最小化目标函数。然而，这样的优化过程涉及到一些值得特别注意的重要技术。

第一种技术：两个网络，一个主网络和一个目标网络。
为什么使用它？数学原因已在我们计算梯度时解释。
实现细节：
* 令 $w$ 和 $w_T$ 分别表示主网络和目标网络的参数。它们最初被设置为相同。
* 在每次迭代中，我们从回放缓冲区（稍后解释）中抽取一个小批量样本 $\{ (s, a, r, s') \}$。
* 网络的输入包括状态 $s$ 和动作 $a$。目标输出是 $y_T \doteq r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$。然后，我们直接最小化 TD 误差或称为损失函数 $(y_T - \hat{q}(s, a, w))^2$ 在小批量 $\{ (s, a, y_T) \}$ 上。

另一种技术：经验回放
问题：什么是经验回放？
回答：
* 在我们收集了一些经验样本后，我们不按照它们被收集的顺序使用这些样本。
* 相反，我们将它们存储在一个集合中，称为回放缓冲区 $\mathcal{B} \doteq \{ (s, a, r, s') \}$。
* 每次我们训练神经网络时，我们都可以从回放缓冲区中抽取一个随机样本的小批量。
* 样本的抽取，或称为经验回放，应该遵循均匀分布（为什么？）。

问题：为什么经验回放在深度 Q-learning 中是必要的？为什么回放必须遵循均匀分布？
回答：答案在于目标函数。
$$
J = \mathbb{E}\left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w) \right)^2 \right]
$$
* $(S, A) \sim d$: $(S, A)$ 是一个索引，被视为一个单一的随机变量。
* $R \sim p(R|S, A), S' \sim p(S'|S, A)$: $R$ 和 $S'$ 由系统模型决定。
* 状态-动作对 $(S, A)$ 的分布被假定为均匀的。
* 然而，样本不是均匀收集的，因为它们是由特定策略连续生成的。
* 为了打破连续样本之间的相关性，我们可以使用经验回放技术，从回放缓冲区中均匀抽取样本。
* 这就是为什么经验回放是必要的以及为什么经验回放必须是均匀的数学原因。

重新审视表格情况：
- 问题：为什么表格 Q-learning 不需要经验回放？回答：没有均匀分布要求。
- 问题：为什么深度 Q-learning 涉及分布？回答：深度情况下的目标函数是所有 $(S, A)$ 上的标量平均值。表格情况不涉及 $S$ 或 $A$ 的任何分布。表格情况下的算法旨在解决所有 $(s, a)$ 的一组方程（贝尔曼最优方程）。
- 问题：我们可以在表格 Q-learning 中使用经验回放吗？回答：是的，我们可以。并且更样本高效。

#### 8.5.3 算法总结
**伪代码：深度 Q-learning（off-policy 版本）**
目标：从行为策略 $\pi_b$ 生成的经验样本中学习一个最优目标网络，以近似最优动作值。
将由 $\pi_b$ 生成的经验样本存储在回放缓冲区 $\mathcal{B} = \{ (s, a, r, s') \}$ 中
对于每次迭代，执行
	从 $\mathcal{B}$ 中均匀抽取一个小批量样本
	对于每个样本 $(s, a, r, s')$，计算目标值 $y_T = r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$，其中 $w_T$ 是目标网络的参数
	使用小批量 $\{ (s, a, y_T) \}$ 更新主网络以最小化 $(y_T - \hat{q}(s, a, w))^2$
	每 $C$ 次迭代设置 $w_T = w$

# 9 策略梯度方法
## 9.1 策略梯度的基本思想
此前，策略已由表格表示：
所有状态的行动概率都存储在表格 $\pi(a|s)$ 中。表格的每个条目都通过状态和行动进行索引。

|          | $a_1$           | $a_2$           | $a_3$           | $a_4$           | $a_5$           |
| -------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| $s_1$    | $\pi(a_1, s_1)$ | $\pi(a_1, s_2)$ | $\pi(a_1, s_3)$ | $\pi(a_1, s_4)$ | $\pi(a_1, s_5)$ |
| $\vdots$ | $\vdots$        | $\vdots$        | $\vdots$        | $\vdots$        | $\vdots$        |
| $s_9$    | $\pi(a_9, s_1)$ | $\pi(a_9, s_2)$ | $\pi(a_9, s_3)$ | $\pi(a_9, s_4)$ | $\pi(a_9, s_5)$ |
我们可以直接访问或更改表格中的值。

策略现在可以用参数化函数表示：
$$
\pi(a|s, \theta)
$$
其中 $\theta \in \mathbb{R}^m$ 是一个参数向量。
* 该函数可以是一个神经网络，例如，其输入是 $s$，输出是采取每个行动的概率，参数是 $\theta$。
* 优势：当状态空间很大时，表格表示在存储和泛化方面效率低下。
* 函数表示有时也写作 $\pi(a, s, \theta)$，$\pi_{\theta}(a|s)$，或 $\pi_{\theta}(a, s)$。

表格表示和函数表示之间的区别：
* 首先，如何定义最优策略？
    * 当以表格形式表示时，策略 $\pi$ 是最优的，如果它可以最大化每个状态值。
    * 当以函数形式表示时，策略 $\pi$ 是最优的，如果它可以最大化某些标量度量。
* 其次，如何获取一个动作的概率？
    * 在表格情况下，可以通过查阅表格策略来直接获取在 $s$ 采取行动 $a$ 的概率。
    * 在函数表示情况下，我们需要根据函数结构和参数计算 $\pi(a|s, \theta)$ 的值。
* 第三，如何更新策略？
    * 当以表格形式表示时，策略 $\pi$ 可以通过直接改变表格中的条目来更新。
    * 当以参数化函数表示时，策略 $\pi$ 不能再以这种方式更新。相反，它只能通过改变参数 $\theta$ 来更新。

策略梯度的基本思想很简单：
* 首先，定义最优策略的度量（或目标函数）：$J(\theta)$，它可以定义最优策略。
* 其次，基于梯度的优化算法来搜索最优策略：$$\theta_{t+1} = \theta_t + \alpha \nabla_{\theta}J(\theta_t)$$

## 9.2 确定最优策略的度量
尽管思想简单，但当我们试图回答以下问题时，复杂性就出现了。
* 应该使用哪些合适的度量？
* 如何计算这些度量的梯度？

### 9.2.1 平均值度量
第一个度量是平均状态值或简称为平均值。具体来说，该度量定义为
$$
\bar{v}_{\pi} = \sum_{s \in \mathcal{S}} d(s)v_{\pi}(s)
$$
$\bar{v}_{\pi}$ 是状态值的加权平均。$d(s) \ge 0$ 是状态 $s$ 的权重。由于 $\sum_{s \in \mathcal{S}} d(s) = 1$，我们可以将 $d(s)$ 解释为概率分布。那么，该度量可以写为
$$
\bar{v}_{\pi} = \mathbb{E}[v_{\pi}(S)]
$$其中 $S \sim d$。

如何选择分布 $d$？有两种情况。
第一种情况是 $d$ 独立于策略 $\pi$。
* 这种情况相对简单，因为度量的梯度更容易计算。
* 在这种情况下，我们特地将 $d$ 记为 $d_0$，将 $\bar{v}_{\pi}$ 记为 $\bar{v}_{\pi}^0$。
* 如何选择 $d_0$？
    * 一种简单的方法是将所有状态视为同等重要，因此选择 $d_0(s) = 1/|\mathcal{S}|$。
    * 另一种重要情况是，我们只对特定状态 $s_0$ 感兴趣。例如，某些任务中的片段总是从相同的状态 $s_0$ 开始。那么，我们只关心从 $s_0$ 开始的长期回报。在这种情况下，$$
d_0(s_0) = 1, \quad d_0(s \ne s_0) = 0.
$$

第二种情况是 $d$ 依赖于策略 $\pi$。
* 一种常见的选择 $d$ 的方式是 $d_{\pi}(s)$，它是策略 $\pi$ 下的平稳分布。
    * $d_{\pi}$ 的一个基本性质是它满足$$
d_{\pi}^{\text{T}} P_{\pi} = d_{\pi}^{\text{T}},
$$其中 $P_{\pi}$ 是状态转移概率矩阵。
* 选择 $d_{\pi}$ 的解释如下。
    * 如果一个状态在长期运行中被频繁访问，那么它更重要，应该获得更多的权重。
    * 如果一个状态很少被访问，那么我们给它较少的权重。

一个重要的等价表达式：
$$
J(\theta) = \lim_{n \to \infty} \mathbb{E}\left[ \sum_{t=0}^n \gamma^t R_{t+1} \right] = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \right].
$$
问题：它与我们刚才介绍的度量有什么关系？
回答：它们是相同的。这是因为
$$
\begin{aligned}
J(\theta) &= \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \right] \\
&= \sum_{s \in \mathcal{S}} d(s)\mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0 = s \right] \\
&= \sum_{s \in \mathcal{S}} d(s)v_{\pi}(s) \\
&= \bar{v}_{\pi}
\end{aligned}
$$

### 9.2.2 平均奖励度量
第二个度量是平均一步奖励或简称为平均奖励。具体来说，该度量是
$$
\bar{r}_{\pi} \doteq \sum_{s \in \mathcal{S}} d_{\pi}(s)r_{\pi}(s) = \mathbb{E}[r_{\pi}(S)],
$$
其中 $S \sim d_{\pi}$。这里，
$$
r_{\pi}(s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s)r(s, a)
$$
是开始于状态 $s$ 可以获得的一步即时奖励的平均值，并且
$$
r(s, a) = \mathbb{E}[R|s, a] = \sum_r rp(r|s, a)
$$
权重 $d_{\pi}$ 是平稳分布。顾名思义，$\bar{r}_{\pi}$ 简单地就是一步即时奖励的加权平均。

$\bar{r}_{\pi}$ 的一个等价的定义：
假设一个智能体遵循给定策略并生成一个轨迹，其奖励为 $(R_{t+1}, R_{t+2}, \dots)$。
沿此轨迹的平均单步奖励是
$$
\begin{aligned}
& \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ R_{t+1} + R_{t+2} + \dots + R_{t+n} \Big| S_t = s_0 \right] \\
={}& \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{k=1}^n R_{t+k} \Big| S_t = s_0 \right]
\end{aligned}
$$
其中 $s_0$ 是轨迹的起始状态。

一个重要的性质是
$$
\begin{aligned}
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{k=1}^n R_{t+k} \Big| S_t = s_0 \right] &= \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{k=1}^n R_{t+k} \right] \\
&= \sum_s d_{\pi}(s)r_{\pi}(s) \\
&= \bar{r}_{\pi}
\end{aligned}
$$
注意：起始状态 $s_0$ 不重要。

**证明**
步骤 1：我们首先证明以下等式对任意起始状态 $s_0 \in \mathcal{S}$ 有效：
$$
\bar{r}_{\pi} = \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} | S_0 = s_0 \right].
\quad (a)
$$
为此，我们注意到
$$
\begin{aligned}
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} | S_0 = s_0 \right] &= \lim_{n \to \infty} \frac{1}{n} \sum_{t=0}^{n-1} \mathbb{E}\left[ R_{t+1} | S_0 = s_0 \right] \\
&= \lim_{t \to \infty} \mathbb{E}\left[ R_{t+1} | S_0 = s_0 \right],
\quad (b)
\end{aligned}
$$
其中最后一个等式是由于 Cesaro 和（也称为 Cesaro 均值）的性质。特别是，如果 $\{a_k\}_{k=1}^{\infty}$ 是一个收敛序列，使得 $\lim_{k \to \infty} a_k$ 存在，那么 $\{1/n \sum_{k=1}^n a_k\}_{n=1}^{\infty}$ 也是一个收敛序列，使得 $\lim_{n \to \infty} 1/n \sum_{k=1}^n a_k = \lim_{k \to \infty} a_k$。

接下来，我们更仔细地检查 (b) 中的 $\mathbb{E}[R_{t+1} | S_0 = s_0]$。根据全期望定律，我们有
$$
\begin{aligned}
\mathbb{E}[R_{t+1} | S_0 = s_0] &= \sum_{s \in \mathcal{S}} \mathbb{E}[R_{t+1} | S_t = s, S_0 = s_0] p^{(t)}(s|s_0) \\
&= \sum_{s \in \mathcal{S}} \mathbb{E}[R_{t+1} | S_t = s] p^{(t)}(s|s_0) \\
&= \sum_{s \in \mathcal{S}} r_{\pi}(s) p^{(t)}(s|s_0),
\end{aligned}
$$
其中 $p^{(t)}(s|s_0)$ 表示从 $s_0$ 经过恰好 $t$ 步转移到 $s$ 的概率。上述等式中的第二个等式是由于马尔可夫无记忆性：下一个时间步获得的奖励仅取决于当前状态，而不是之前的状态。
注意
$$
\lim_{t \to \infty} p^{(t)}(s|s_0) = d_{\pi}(s)
$$
根据平稳分布的定义。因此，起始状态 $s_0$ 不重要。那么，我们有
$$
\lim_{t \to \infty} \mathbb{E}[R_{t+1}|S_0 = s_0] = \lim_{t \to \infty} \sum_{s \in \mathcal{S}} r_{\pi}(s)p^{(t)}(s|s_0) = \sum_{s \in \mathcal{S}} r_{\pi}(s)d_{\pi}(s) = \bar{r}_{\pi}.
$$
将上述等式代入 (b) 得到 (a)。

步骤 2：考虑任意状态分布 $d$。根据全期望定律，我们有
$$
\begin{aligned}
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} \right] &= \lim_{n \to \infty} \frac{1}{n} \sum_{s \in \mathcal{S}} d(s)\mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} | S_0 = s \right] \\
&= \sum_{s \in \mathcal{S}} d(s) \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} | S_0 = s \right].
\end{aligned}
$$
由于 (a) 对任何起始状态都有效，将 (a) 代入上述等式得到
$$
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} \right] = \sum_{s \in \mathcal{S}} d(s)\bar{r}_{\pi} = \bar{r}_{\pi}.
$$
证明完成。

### 9.2.3 总结
度量：$\bar{v}_{\pi}$
表达式 1：
$$
\sum_{s \in \mathcal{S}} d(s)v_{\pi}(s)
$$
表达式 2：
$$
\mathbb{E}_{S \sim d}[v_{\pi}(S)]
$$
表达式 3：
$$
\lim_{n \to \infty} \mathbb{E}\left[\sum_{t=0}^n \gamma^t R_{t+1}\right]
$$

度量：$\bar{r}_{\pi}$
表达式 1：
$$
\sum_{s \in \mathcal{S}} d_{\pi}(s)r_{\pi}(s)
$$
表达式 2：
$$
\mathbb{E}_{S \sim d_{\pi}}[r_{\pi}(S)]
$$
表达式 3：$$\lim_{n \to \infty} \frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1} R_{t+1}\right]$$

关于度量的重要说明：
* 所有这些度量都是 $\pi$ 的函数。
* 由于 $\pi$ 由 $\theta$ 参数化，这些度量是 $\theta$ 的函数。
* 换句话说，$\theta$ 的不同值可以生成不同的度量值。
* 因此，我们可以搜索 $\theta$ 的最优值来最大化这些度量。
* 一个复杂之处在于，度量可以定义在折扣情况（其中 $\gamma \in [0, 1)$）或无折扣情况（其中 $\gamma = 1$）下。
* 本书中到目前为止我们只考虑了折扣情况。有关无折扣情况的详细信息，请参阅本书。
* 直观上，$\bar{r}_{\pi}$ 更短视，因为它只考虑即时奖励，而 $\bar{v}_{\pi}$ 考虑所有步骤的总奖励。
* 然而，这两个度量是相互等价的。在折扣情况下，当 $\gamma < 1$ 时，成立以下关系：$$
\bar{r}_{\pi} = (1 - \gamma)\bar{v}_{\pi}.
$$证明请参见书中。


## 9.3 度量的梯度
给定上一节中介绍的度量，我们可以使用基于梯度的方法来最大化它们。为此，我们首先需要计算这些度量的梯度。

梯度计算是策略梯度方法中最复杂的部分之一。这是因为
* 首先，我们需要区分不同的度量 $\bar{v}_{\pi}$, $\bar{r}_{\pi}$, $\bar{v}_{\pi}^0$
* 其次，我们需要区分折扣和无折扣情况。

#### 9.3.1 总览
**定理（策略梯度定理）**
$J(\theta)$ 的梯度是
$$
\nabla_{\theta} J(\theta) = \sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a),
\quad (c)
$$
其中 $\eta$ 是一个状态分布，$\nabla_{\theta}\pi$ 是 $\pi$ 关于 $\theta$ 的梯度。此外，(c) 有一个紧凑的期望形式：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{S \sim \eta, A \sim \pi(S, \theta)}\left[ \nabla_{\theta} \ln \pi(A|S, \theta)q_{\pi}(S, A) \right].
\quad (d)
$$
其中 $\ln$ 是自然对数。

说明：
* 应该指出，定理（策略梯度定理）是定理（折扣情况下的 $\bar{v}_{\pi}$ 梯度）、定理（折扣情况下 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度）和定理（无折扣情况下 $\bar{r}_{\pi}$ 的梯度）中结果的总结。这三个定理解决了涉及不同度量和折扣/无折扣情况的不同场景。这些场景中的梯度都具有相似的表达式，因此在定理（策略梯度定理）中进行了总结。$J(\theta)$ 和 $\eta$ 的具体表达式未在定理（策略梯度定理）中给出，可以在定理（折扣情况下的 $\bar{v}_{\pi}$ 梯度）、定理（折扣情况下 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度）和定理（无折扣情况下 $\bar{r}_{\pi}$ 的梯度）中找到。特别是，$J(\theta)$ 可以是 $\bar{v}_{\pi}$、$\bar{r}_{\pi}$ 或 $\bar{v}_{\pi}^0$。等式 (c) 中的等号可能是严格相等或近似。分布 $\eta$ 在不同场景中也不同。
* 表达式 (d) 比 (c) 更受青睐，因为它可以表示为随机梯度。

为什么 (c) 可以表示为 (d)？证明如下。根据期望的定义，(c) 可以改写为
$$
\begin{aligned}
\nabla_{\theta} J(\theta) &= \sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \mathbb{E}_{S \sim \eta}\left[ \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(A|S, \theta)q_{\pi}(S, A) \right].
\quad (e)
\end{aligned}
$$
此外，$\ln \pi(a|s, \theta)$ 的梯度是
$$
\nabla_{\theta} \ln \pi(a|s, \theta) = \frac{\nabla_{\theta}\pi(a|s, \theta)}{\pi(a|s, \theta)}
$$
因此
$$
\nabla_{\theta}\pi(a|s, \theta) = \pi(a|s, \theta)\nabla_{\theta} \ln \pi(a|s, \theta).
\quad (f)
$$
将 (f) 代入 (e) 得到
$$
\begin{aligned}
\nabla_{\theta} J(\theta) &= \mathbb{E}\left[ \sum_{a \in \mathcal{A}} \pi(a|S, \theta)\nabla_{\theta} \ln \pi(a|S, \theta)q_{\pi}(S, A) \right] \\
&= \mathbb{E}_{S \sim \eta, A \sim \pi(S, \theta)}\left[ \nabla_{\theta} \ln \pi(A|S, \theta)q_{\pi}(S, A) \right].
\end{aligned}
$$

值得注意的是，为了确保 $\ln \pi(a|s, \theta)$ 有效，$\pi(a|s, \theta)$ 必须对所有 $(s, a)$ 为正。这可以通过使用 softmax 函数来实现：
$$
\pi(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_{a' \in \mathcal{A}} e^{h(s, a', \theta)}}, \quad a \in \mathcal{A},
\quad (g)
$$
其中 $h(s, a, \theta)$ 是一个指示选择行动 $a$ 的函数。策略 (g) 满足 $\pi(a|s, \theta) \in (0, 1)$ 且对任意 $s \in \mathcal{S}$ 都有 $\sum_{a \in \mathcal{A}} \pi(a|s, \theta) = 1$。这个策略可以通过神经网络实现。网络的输入是 $s$。输出层是 softmax 层，因此网络输出 $\pi(a|s, \theta)$ 对所有 $a$ 都有效，并且它们的和等于 $1$。

由于对所有 $a$ 都有 $\pi(a|s, \theta) > 0$，因此参数化策略是随机的，因此是探索性的。该策略不直接告诉行动采取什么行动。相反，行动应该根据概率分布生成。

### 9.3.2 折扣情况下的梯度推导
接下来我们推导折扣情况下 $\gamma \in (0, 1)$ 的度量的梯度。折扣情况下的状态值和行动值定义为
$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s], \\
q_{\pi}(s, a) &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s, A_t = a].
\end{aligned}
$$
成立 $v_{\pi}(s) = \sum_{a \in \mathcal{A}}\pi(a|s, \theta)q_{\pi}(s, a)$，且状态值满足贝尔曼方程。

首先，我们证明 $\bar{v}_{\pi}(\theta)$ 和 $\bar{r}_{\pi}(\theta)$ 是等价的度量。
**引理（等价性 $\bar{v}_{\pi}(\theta)$ 和 $\bar{r}_{\pi}(\theta)$）**
在折扣情况下，当 $\gamma \in (0, 1)$ 时，成立
$$
\bar{r}_{\pi} = (1 - \gamma)\bar{v}_{\pi}.
\quad (h)
$$

**引理（等价性 $\bar{v}_{\pi}(\theta)$ 和 $\bar{r}_{\pi}(\theta)$）证明**
请注意 $\bar{v}_{\pi}(\theta) = d_{\pi}^{\text{T}} v_{\pi}$，其中 $v_{\pi}$ 和 $r_{\pi}$ 满足贝尔曼方程 $v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi}$。将 $d_{\pi}^{\text{T}}$ 乘以贝尔曼方程两边得到
$$
\begin{aligned}
d_{\pi}^{\text{T}} v_{\pi} &= d_{\pi}^{\text{T}} r_{\pi} + \gamma d_{\pi}^{\text{T}} P_{\pi} v_{\pi} \\
&= d_{\pi}^{\text{T}} r_{\pi} + \gamma d_{\pi}^{\text{T}} v_{\pi},
\end{aligned}
$$
这意味着 (h)。


其次，以下引理给出了任意 $s$ 的 $v_{\pi}(s)$ 的梯度。
**引理 （梯度 $v_{\pi}(s)$）**
在折扣情况下，对于任意 $s \in \mathcal{S}$，成立
$$
\nabla_{\theta}v_{\pi}(s) = \sum_{s' \in \mathcal{S}} \text{Pr}_{\pi}(s'|s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s', \theta)q_{\pi}(s', a),
\quad (i)
$$
其中
$$\text{Pr}_{\pi}(s'|s) = \sum_{k=0}^{\infty} \gamma^k [P_{\pi}^k]_{ss'} = [(I_n - \gamma P_{\pi})^{-1}]_{ss'}$$
是策略 $\pi$ 下从状态 $s$ 到 $s'$ 的总折扣概率。这里，$[X]_{ss'}$ 表示矩阵 $X$ 的第 $s$ 行第 $s'$ 列的元素，$[P_{\pi}^k]_{ss'}$是在策略 $\pi$ 下使用恰好 $k$ 步从 $s$ 到 $s'$ 转移的概率。

**引理 （梯度 $v_{\pi}(s)$）证明**
首先，对于任意 $s \in \mathcal{S}$，成立
$$
\begin{aligned}
\nabla_{\theta}v_{\pi}(s) &= \nabla_{\theta}\left[ \sum_{a \in \mathcal{A}} \pi(a|s, \theta)q_{\pi}(s, a) \right] \\
&= \sum_{a \in \mathcal{A}} \left[ \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) + \pi(a|s, \theta)\nabla_{\theta}q_{\pi}(s, a) \right],
\quad (j)
\end{aligned}
$$
其中 $q_{\pi}(s, a)$ 是由下式给出的行动值
$$
q_{\pi}(s, a) = r(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s, a)v_{\pi}(s').
$$
由于 $r(s, a) = \sum_r rp(r|s, a)$ 独立于 $\theta$，我们有
$$
\nabla_{\theta}q_{\pi}(s, a) = 0 + \gamma \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s').
$$
将此结果代入 (j) 得到
$$
\begin{aligned}
\nabla_{\theta}v_{\pi}(s) &= \sum_{a \in \mathcal{A}} \left[ \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) + \pi(a|s, \theta)\gamma \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s') \right] \\
&= \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) + \gamma \sum_{a \in \mathcal{A}} \pi(a|s, \theta) \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s').
\quad (k)
\end{aligned}
$$
值得注意的是，$\nabla_{\theta}v_{\pi}$ 出现在上述等式的两边。一种计算它的方法是使用展开技术。在这里，我们使用另一种基于矩阵-向量形式的方法，我们认为它更直观。特别是，令
$$
u(s) \doteq \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a).
$$
由于
$$
\sum_{a \in \mathcal{A}} \pi(a|s, \theta) \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s') = \sum_{s' \in \mathcal{S}} p(s'|s)\nabla_{\theta}v_{\pi}(s') = \sum_{s' \in \mathcal{S}} [P_{\pi}]_{ss'}\nabla_{\theta}v_{\pi}(s'),
$$
方程 (k) 可以写成矩阵-向量形式
$$
\begin{bmatrix}
\vdots \\
\nabla_{\theta}v_{\pi}(s) \\
\vdots
\end{bmatrix}_{\nabla_{\theta}v_{\pi} \in \mathbb{R}^{mn}} = \begin{bmatrix}
\vdots \\
u(s) \\
\vdots
\end{bmatrix}_{u \in \mathbb{R}^{mn}} +\gamma(P_{\pi} \otimes I_m) \begin{bmatrix}
\vdots \\
\nabla_{\theta}v_{\pi}(s') \\
\vdots
\end{bmatrix}_{\nabla_{\theta}v_{\pi} \in \mathbb{R}^{mn}},
$$
可以简洁地写成
$$
\nabla_{\theta} v_{\pi} = u + \gamma (P_{\pi} \otimes I_m)\nabla_{\theta} v_{\pi}.
$$
这里，$n = |\mathcal{S}|$，$m$ 是参数向量 $\theta$ 的维度。方程中出现 Kronecker 积的原因是 $\nabla_{\theta} v_{\pi}(s)$ 是一个向量。上述等式是 $\nabla_{\theta} v_{\pi}$ 的线性方程，可以解为
$$
\begin{aligned}
\nabla_{\theta} v_{\pi} &= (I_{nm} - \gamma P_{\pi} \otimes I_m)^{-1} u \\
&= (I_n \otimes I_m - \gamma P_{\pi} \otimes I_m)^{-1} u \\
&= [(I_n - \gamma P_{\pi})^{-1} \otimes I_m] u.
\quad (l)
\end{aligned}
$$
对于任何状态 $s$，由 (l) 可知
$$
\begin{aligned}
\nabla_{\theta} v_{\pi}(s) &= \sum_{s' \in \mathcal{S}} [(I_n - \gamma P_{\pi})^{-1}]_{ss'} u(s') \\
&= \sum_{s' \in \mathcal{S}} [(I_n - \gamma P_{\pi})^{-1}]_{ss'} \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s', \theta)q_{\pi}(s', a).
\quad (m)
\end{aligned}
$$
量 $[(I_n - \gamma P_{\pi})^{-1}]_{ss'}$ 具有概率解释。特别是，由于 $(I_n - \gamma P_{\pi})^{-1} = I_n + \gamma P_{\pi} + \gamma^2 P_{\pi}^2 + \dots$，我们有
$$
[(I_n - \gamma P_{\pi})^{-1}]_{ss'} = [I_n]_{ss'} + \gamma[P_{\pi}]_{ss'} + \gamma^2[P_{\pi}^2]_{ss'} + \dots = \sum_{k=0}^{\infty} \gamma^k [P_{\pi}^k]_{ss'}.
$$
请注意 $[P_{\pi}^k]_{ss'}$ 是从 $s$ 到 $s'$ 经过恰好 $k$ 步的转移概率。因此，它 $[(I_n - \gamma P_{\pi})^{-1}]_{ss'}$ 是从 $s$ 到 $s'$ 经过任意步数的总折扣转移概率。通过将 $\text{Pr}_{\pi}(s'|s) \doteq [(I_n - \gamma P_{\pi})^{-1}]_{ss'}$，方程 (m) 变为 (i)。


结合引理 （梯度 $v_{\pi}(s)$）的结果，我们准备推导 $\nabla_{\theta}\bar{v}_{\pi}$ 的梯度。
**定理（折扣情况下的 $\bar{v}_{\pi}$ 梯度）**
在折扣情况下，当 $\gamma \in (0, 1)$ 时，$\bar{v}_{\pi}$ 的梯度是
$$
\nabla_{\theta}\bar{v}_{\pi}^0 = \mathbb{E}_{S \sim \rho, A \sim \pi(S, A)}\left[ \nabla_{\theta} \ln \pi(A|S, \theta) q_{\pi}(S, A) \right],
$$
其中 $S \sim \rho$ 且 $A \sim \pi(S, \theta)$。这里，状态分布 $\rho$ 是
$$
\rho_{\pi}(s) = \sum_{s_0 \in \mathcal{S}} d_0(s_0)\text{Pr}_{\pi}(s|s_0), \quad s \in \mathcal{S},
\quad (n)
$$
其中 $\text{Pr}_{\pi}(s|s') = \sum_{k=0}^{\infty} \gamma^k [P_{\pi}^k]_{s's} = [(I - \gamma P_{\pi})^{-1}]_{s's}$ 是从在策略 $\pi$下 $s_0$ 到 $s$ 的总折扣概率。

**定理（折扣情况下的 $\bar{v}_{\pi}$ 梯度）证明**
由于 $d_0(s)$ 独立于 $\pi$，我们有
$$
\nabla_{\theta}\bar{v}_{\pi}^0 = \nabla_{\theta}\sum_{s \in \mathcal{S}} d_0(s)v_{\pi}(s) = \sum_{s \in \mathcal{S}} d_0(s)\nabla_{\theta}v_{\pi}(s).
$$
将引理 （梯度 $v_{\pi}(s)$）中给出的 $\nabla_{\theta}v_{\pi}(s)$ 表达式代入上述等式得到
$$
\begin{aligned}
\nabla_{\theta}\bar{v}_{\pi}^0 &= \sum_{s \in \mathcal{S}} d_0(s)\nabla_{\theta}v_{\pi}(s) \\
&= \sum_{s \in \mathcal{S}} d_0(s)\sum_{s' \in \mathcal{S}} \text{Pr}_{\pi}(s'|s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s', \theta)q_{\pi}(s', a) \\
&= \sum_{s' \in \mathcal{S}} \left(\sum_{s \in \mathcal{S}} d_0(s)\text{Pr}_{\pi}(s'|s)\right) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s', \theta)q_{\pi}(s', a) \\
&\doteq \sum_{s' \in \mathcal{S}} \rho_{\pi}(s') \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s', \theta)q_{\pi}(s', a) \\
&= \sum_{s \in \mathcal{S}} \rho_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) \quad (\text{将 } s' \text{ 更改为 } s) \\
&= \sum_{s \in \mathcal{S}} \rho_{\pi}(s) \sum_{a \in \mathcal{A}} \pi(a|s, \theta)\nabla_{\theta}\ln\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \mathbb{E}[ \nabla_{\theta}\ln\pi(A|S, \theta)q_{\pi}(S, A) ],
\end{aligned}
$$
其中 $S \sim \rho_{\pi}$ 且 $A \sim \pi(S, \theta)$。证明完成。


结合引理（等价性 $\bar{v}_{\pi}(\theta)$ 和 $\bar{r}_{\pi}(\theta)$）和引理 （梯度 $v_{\pi}(s)$），我们可以推导出 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度。
**定理（折扣情况下 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度）**
在折扣情况下，当 $\gamma \in (0, 1)$ 时，$\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度是
$$
\begin{aligned}
\nabla_{\theta}\bar{r}_{\pi} &= (1 - \gamma)\nabla_{\theta}\bar{v}_{\pi} \\
&\approx \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \mathbb{E}[\nabla_{\theta}\ln\pi(A|S, \theta)q_{\pi}(S, A)],
\end{aligned}
$$
其中 $S \sim d_{\pi}$ 且 $A \sim \pi(S, \theta)$。这里，当 $\gamma$ 接近 $1$ 时，近似更准确。

**定理（折扣情况下 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度）证明**
根据 $\bar{v}_{\pi}$ 的定义可知
$$
\begin{aligned}
\nabla_{\theta}\bar{v}_{\pi} &= \nabla_{\theta}\sum_{s \in \mathcal{S}} d_{\pi}(s)v_{\pi}(s) \\
&= \sum_{s \in \mathcal{S}} \nabla_{\theta}d_{\pi}(s)v_{\pi}(s) + \sum_{s \in \mathcal{S}} d_{\pi}(s)\nabla_{\theta}v_{\pi}(s).
\quad (o)
\end{aligned}
$$
该等式包含两项。一方面，将 (l) 中 $\nabla_{\theta}v_{\pi}$ 的表达式代入第二项得到
$$
\begin{aligned}
\sum_{s \in \mathcal{S}} d_{\pi}(s)\nabla_{\theta}v_{\pi}(s) &= (d_{\pi}^{\text{T}} \otimes I_m)\nabla_{\theta}v_{\pi} \\
&= (d_{\pi}^{\text{T}} \otimes I_m) [(I_n - \gamma P_{\pi})^{-1} \otimes I_m] u \\
&= [d_{\pi}^{\text{T}}(I_n - \gamma P_{\pi})^{-1}] \otimes I_m u.
\quad (p)
\end{aligned}
$$
注意到
$$
d_{\pi}^{\text{T}}(I_n - \gamma P_{\pi})^{-1} = \frac{1}{1 - \gamma}d_{\pi}^{\text{T}},
$$
这可以通过在等式两边乘以 $(I_n - \gamma P_{\pi})$ 轻松验证。因此，(p) 变为
$$
\sum_{s \in \mathcal{S}} d_{\pi}(s)\nabla_{\theta}v_{\pi}(s) = \frac{1}{1 - \gamma} d_{\pi}^{\text{T}} \otimes I_m u = \frac{1}{1 - \gamma} \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a).
$$
另一方面，(o) 的第一项涉及 $\nabla_{\theta}d_{\pi}$。然而，由于第二项包含 $\frac{1}{1-\gamma}$，当 $\gamma \to 1$ 时，第二项变为主导项，第一项变得可以忽略不计。因此，
$$
\nabla_{\theta}\bar{v}_{\pi} \approx \frac{1}{1 - \gamma} \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a),
$$
此外，根据 $\bar{r}_{\pi} = (1 - \gamma)\bar{v}_{\pi}$ 可知
$$
\begin{aligned}
\nabla_{\theta}\bar{r}_{\pi} &= (1 - \gamma)\nabla_{\theta}\bar{v}_{\pi} \\
&\approx \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \pi(a|s, \theta)\nabla_{\theta}\ln\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \mathbb{E}[\nabla_{\theta}\ln\pi(A|S, \theta)q_{\pi}(S, A)].
\end{aligned}
$$
上述等式中的近似要求当 $\gamma \to 1$ 时第一项不趋于无穷。

### 9.3.3 无折扣情况下的梯度推导
接下来我们展示如何在无折扣情况下计算度量的梯度，即 $\gamma = 1$。读者可能会想，为什么我们突然开始考虑无折扣情况，而本书到目前为止我们只考虑了折扣情况。事实上，平均奖励 $\bar{r}_{\pi}$ 的定义对折扣和无折扣情况都有效。虽然 $\bar{r}_{\pi}$ 在折扣情况下的梯度是近似的，但我们将看到它在无折扣情况下的梯度更优美。

在无折扣情况下，需要重新定义状态值和行动值。由于无折扣的总和 $\mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \dots | S_t = s]$ 可能会发散，状态值和行动值以一种特殊的方式定义：
$$
\begin{aligned}
v_{\pi}(s) &\doteq \mathbb{E}[R_{t+1} - \bar{r}_{\pi} + (R_{t+2} - \bar{r}_{\pi}) + (R_{t+3} - \bar{r}_{\pi}) + \dots | S_t = s], \\
q_{\pi}(s, a) &\doteq \mathbb{E}[R_{t+1} - \bar{r}_{\pi} + (R_{t+2} - \bar{r}_{\pi}) + (R_{t+3} - \bar{r}_{\pi}) + \dots | S_t = s, A_t = a],
\end{aligned}
$$
其中 $\bar{r}_{\pi}$ 是平均奖励，其确定方式已在 $\pi$ 给定时给出。在文献中，对 $v_{\pi}(s)$ 有不同的名称，例如差分奖励或偏差。可以验证上述状态值满足以下贝尔曼方程：
$$
v_{\pi}(s) = \sum_{a}\pi(a|s, \theta)\left[ \sum_{s'}p(s'|s, a)(r(s, a) - \bar{r}_{\pi}) + \sum_{s'}p(s'|s, a)v_{\pi}(s') \right].
\quad (q)
$$
由于 $v_{\pi}(s) = \sum_{a \in \mathcal{A}}\pi(a|s, \theta)q_{\pi}(s, a)$，成立 $q_{\pi}(s, a) = r(s, a) - \bar{r}_{\pi} + \sum_{s'}p(s'|s, a)v_{\pi}(s')$。方程 (q) 的矩阵-向量形式是
$$
v_{\pi} = r_{\pi} - \bar{r}_{\pi}\mathbf{1}_n + P_{\pi}v_{\pi},
\quad (r)
$$
其中 $\mathbf{1}_n = [1, 1, \dots, 1]^{\text{T}} \in \mathbb{R}^n$。方程 (r) 类似于贝尔曼方程，并且有一个特定名称，称为**泊松方程**。
如何从泊松方程中解出 $v_{\pi}$？答案在以下定理中给出。
**定理（泊松方程的解）**
令
$$
v_{\pi}^* = (I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}})^{-1} r_{\pi}.
\quad (s)
$$
那么，$v_{\pi}^*$ 是泊松方程 (r) 的一个解。此外，泊松方程的任何解都具有以下形式：
$$v_{\pi} = v_{\pi}^* + c\mathbf{1}_n,$$
其中 $c \in \mathbb{R}$。
该定理表明泊松方程的解可能不是唯一的。

**定理（泊松方程的解）证明**
我们分三步证明。
步骤 1：证明 (s) 中的 $v_{\pi}^*$ 是泊松方程的一个解。
为了简化，令
$$A \doteq I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}.$$
那么，$v_{\pi}^* = A^{-1}r_{\pi}$。$A$ 是可逆矩阵的事实将在步骤 3 中证明。将 $v_{\pi}^* = A^{-1}r_{\pi}$ 代入 (r) 得到
$$A^{-1}r_{\pi} = r_{\pi} - \bar{r}_{\pi}\mathbf{1}_n + P_{\pi}A^{-1}r_{\pi}.$$
这个等式在下面证明是有效的。识别这个等式得到 $(-A^{-1} + I_n - \mathbf{1}_n d_{\pi}^{\text{T}} + P_{\pi}A^{-1})r_{\pi} = 0$，因此，
$$(-I_n + A - \mathbf{1}_n d_{\pi}^{\text{T}} A + P_{\pi})A^{-1}r_{\pi} = 0.$$
上面等式中括号内的项为零，因为
$$
-I_n + A - \mathbf{1}_n d_{\pi}^{\text{T}} A + P_{\pi} = -I_n + (I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}) - \mathbf{1}_n d_{\pi}^{\text{T}}(I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}) + P_{\pi} = 0.
$$
因此，(s) 中的 $v_{\pi}^*$ 是一个解。

步骤 2：解的通用表达式。
将 $\bar{r}_{\pi} = d_{\pi}^{\text{T}}r_{\pi}$ 代入 (r) 得到
$$
v_{\pi} = r_{\pi} - d_{\pi}^{\text{T}}r_{\pi}\mathbf{1}_n + P_{\pi}v_{\pi}
\quad (t)
$$
因此
$$
(I_n - P_{\pi})v_{\pi} = (I_n - \mathbf{1}_n d_{\pi}^{\text{T}})r_{\pi}.
\quad (u)
$$
注意到 $I_n - P_{\pi}$ 是奇异的，因为 $(I_n - P_{\pi})\mathbf{1}_n = 0$。因此，方程 (u) 的解不是唯一的：如果 $v_{\pi}^*$ 是一个解，那么 $v_{\pi}^* + x$ 也是任意 $x \in \text{Null}(I_n - P_{\pi})$ 的解。当 $P_{\pi}$ 是不可约的，$\text{Null}(I_n - P_{\pi}) = \text{span}\{\mathbf{1}_n\}$。那么，泊松方程的任何解都可以表示为 $v_{\pi}^* + c\mathbf{1}_n$，其中 $c \in \mathbb{R}$。

步骤 3：证明 $A = I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}$ 是可逆的。
由于 $v_{\pi}^*$ 涉及 $A^{-1}$，因此有必要证明 $A$ 是可逆的。分析总结在以下引理中。
**引理**
矩阵 $I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}$ 是可逆的，其逆是
$$[I_n - (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})]^{-1} = \sum_{k=1}^{\infty} (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k + I_n.$$
证明。
首先，我们不加证明地陈述一些初步事实。令 $\rho(M)$ 表示矩阵 $M$ 的谱半径。$I_n - M$ 是可逆的，当且仅当 $\rho(M) < 1$。此外，$\rho(M) < 1$ 当且仅当 $\lim_{k \to \infty} M^k = 0$。
基于上述事实，我们接下来证明 $\lim_{k \to \infty} (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k \to 0$，然后证明 $I_n - (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})$ 的可逆性。为此，我们注意到
$$
(P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k = P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}}, \quad k \ge 1,
\quad (v)
$$
这可以通过归纳法证明。例如，当 $k = 1$ 时，等式成立。当 $k = 2$ 时，我们有
$$
\begin{aligned}
(P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^2 &= (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})(P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}}) \\
&= P_{\pi}^2 - P_{\pi}\mathbf{1}_n d_{\pi}^{\text{T}} - \mathbf{1}_n d_{\pi}^{\text{T}} P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}}\mathbf{1}_n d_{\pi}^{\text{T}} \\
&= P_{\pi}^2 - \mathbf{1}_n d_{\pi}^{\text{T}},
\end{aligned}
$$
其中最后一个等式是由于 $P_{\pi}\mathbf{1}_n = \mathbf{1}_n$, $d_{\pi}^{\text{T}}P_{\pi} = d_{\pi}^{\text{T}}$, 和 $d_{\pi}^{\text{T}}\mathbf{1}_n = 1$。当 $k \ge 3$ 时可以类似地证明。
由于 $d_{\pi}$ 是状态的平稳分布，成立 $\lim_{k \to \infty} P_{\pi}^k = \mathbf{1}_n d_{\pi}^{\text{T}}$。因此，(v) 意味着
$$\lim_{k \to \infty} (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k = \lim_{k \to \infty} P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}} = 0.$$
结果是，$\rho(P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}}) < 1$，因此 $I_n - (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})$ 是可逆的。此外，该矩阵的逆由下式给出
$$
\begin{aligned}
(I_n - (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}}))^{-1} &= \sum_{k=0}^{\infty} (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k \\
&= I_n + \sum_{k=1}^{\infty} (P_{\pi} - \mathbf{1}_n d_{\pi}^{\text{T}})^k \\
&= I_n + \sum_{k=1}^{\infty} (P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}}) \\
&= \sum_{k=0}^{\infty} (P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}}) + \mathbf{1}_n d_{\pi}^{\text{T}}.
\end{aligned}
$$
证明完成。

然而，上面方程中给出的结果 $(I_n - P_{\pi} + \mathbf{1}_n d_{\pi}^{\text{T}})^{-1} = \sum_{k=0}^{\infty} (P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}})$ 是不准确的，因为 $\sum_{k=0}^{\infty} (P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}})$ 是奇异的，因为 $\sum_{k=0}^{\infty} (P_{\pi}^k - \mathbf{1}_n d_{\pi}^{\text{T}})\mathbf{1}_n = 0$。引理纠正了这一不准确之处。

尽管在无折扣情况下 $v_{\pi}$ 的值不是唯一的，如定理（泊松方程的解）所示，但 $\bar{r}_{\pi}$ 的值是唯一的。特别是，根据泊松方程可知
$$
\begin{aligned}
\bar{r}_{\pi}\mathbf{1}_n &= r_{\pi} + (P_{\pi} - I_n)v_{\pi} \\
&= r_{\pi} + (P_{\pi} - I_n)(v_{\pi}^* + c\mathbf{1}_n) \\
&= r_{\pi} + (P_{\pi} - I_n)v_{\pi}^*.
\end{aligned}
$$
值得注意的是，未确定的常数 $c$ 被抵消了，因此 $\bar{r}_{\pi}$ 是唯一的。因此，我们可以计算无折扣情况下 $\bar{r}_{\pi}$ 的梯度。此外，由于 $v_{\pi}$ 不是唯一的，$\bar{v}_{\pi}$ 也不是唯一的。我们不讨论无折扣情况下 $\bar{v}_{\pi}$ 的梯度。对于感兴趣的读者，值得一提的是我们可以添加更多约束来唯一确定泊松方程中的 $v_{\pi}$。例如，假设存在一个经常性状态，该经常性状态的值可以确定。

无折扣情况下梯度的推导如下。
**定理（无折扣情况下 $\bar{r}_{\pi}$ 的梯度）**
在无折扣情况下，平均奖励 $\bar{r}_{\pi}$ 的梯度是
$$
\begin{aligned}
\nabla_{\theta}\bar{r}_{\pi} &= \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) \\
&= \mathbb{E}[\nabla_{\theta}\ln\pi(A|S, \theta)q_{\pi}(S, A)],
\quad (w)
\end{aligned}
$$
其中 $S \sim d_{\pi}$ 且 $A \sim \pi(S, \theta)$。
与定理（折扣情况下 $\bar{r}_{\pi}$ 和 $\bar{v}_{\pi}$ 的梯度）中所示的折扣情况相比，无折扣情况下 $\bar{r}_{\pi}$ 的梯度更为优雅，因为 (w) 是严格有效的，并且 $S$ 服从平稳分布。

**定理（无折扣情况下 $\bar{r}_{\pi}$ 的梯度）证明**
首先，根据 $v_{\pi}(s) = \sum_{a \in \mathcal{A}}\pi(a|s, \theta)q_{\pi}(s, a)$ 可知
$$
\begin{aligned}
\nabla_{\theta}v_{\pi}(s) &= \nabla_{\theta}\left[ \sum_{a \in \mathcal{A}} \pi(a|s, \theta)q_{\pi}(s, a) \right] \\
&= \sum_{a \in \mathcal{A}} \left[ \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) + \pi(a|s, \theta)\nabla_{\theta}q_{\pi}(s, a) \right],
\quad (x)
\end{aligned}
$$
其中 $q_{\pi}(s, a)$ 是满足以下条件的行动值
$$
\begin{aligned}
q_{\pi}(s, a) &= \sum_r p(r|s, a)(r - \bar{r}_{\pi}) + \sum_{s'} p(s'|s, a)v_{\pi}(s') \\
&= r(s, a) - \bar{r}_{\pi} + \sum_{s'} p(s'|s, a)v_{\pi}(s').
\end{aligned}
$$
由于 $r(s, a) = \sum_r rp(r|s, a)$ 独立于 $\theta$，我们有
$$\nabla_{\theta}q_{\pi}(s, a) = 0 - \nabla_{\theta}\bar{r}_{\pi} + \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s').$$
将此结果代入 (x) 得到
$$
\begin{aligned}
\nabla_{\theta}v_{\pi}(s) &= \sum_{a \in \mathcal{A}} \left[ \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) + \pi(a|s, \theta)\left(-\nabla_{\theta}\bar{r}_{\pi} + \sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s')\right) \right] \\
&= \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a) - \nabla_{\theta}\bar{r}_{\pi} + \sum_{a \in \mathcal{A}} \pi(a|s, \theta)\sum_{s' \in \mathcal{S}} p(s'|s, a)\nabla_{\theta}v_{\pi}(s').
\quad (y)
\end{aligned}
$$
令
$$u(s) \doteq \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a).$$
由于 $\sum_{a \in \mathcal{A}} \pi(a|s, \theta)\sum_{s' \in \mathcal{S}}p(s'|s, a)\nabla_{\theta}v_{\pi}(s') = \sum_{s' \in \mathcal{S}}p(s'|s)\nabla_{\theta}v_{\pi}(s')$，方程 (y) 可以写成矩阵-向量形式
$$
\begin{bmatrix}
\vdots \\
\nabla_{\theta}v_{\pi}(s) \\
\vdots
\end{bmatrix}_{\nabla_{\theta}v_{\pi} \in \mathbb{R}^{mn}} = \begin{bmatrix}
\vdots \\
u(s) \\
\vdots
\end{bmatrix}_{u \in \mathbb{R}^{mn}} -\mathbf{1}_n \otimes \nabla_{\theta}\bar{r}_{\pi} + (P_{\pi} \otimes I_m) \begin{bmatrix}
\vdots \\
\nabla_{\theta}v_{\pi}(s') \\
\vdots
\end{bmatrix}_{\nabla_{\theta}v_{\pi} \in \mathbb{R}^{mn}},
$$
其中 $n = |\mathcal{S}|$, $m$ 是 $\theta$ 的维度，$\otimes$ 是 Kronecker 积。上述等式可以简洁地写成
$$\nabla_{\theta}v_{\pi} = u - \mathbf{1}_n \otimes \nabla_{\theta}\bar{r}_{\pi} + (P_{\pi} \otimes I_m)\nabla_{\theta}v_{\pi},$$
因此
$$\mathbf{1}_n \otimes \nabla_{\theta}\bar{r}_{\pi} = u + (P_{\pi} \otimes I_m)\nabla_{\theta}v_{\pi} - \nabla_{\theta}v_{\pi}.$$
在上述等式两边乘以 $d_{\pi}^{\text{T}} \otimes I_m$ 得到
$$
\begin{aligned}
(d_{\pi}^{\text{T}} \mathbf{1}_n) \otimes \nabla_{\theta}\bar{r}_{\pi} &= d_{\pi}^{\text{T}} \otimes I_m u + (d_{\pi}^{\text{T}} P_{\pi}) \otimes I_m \nabla_{\theta}v_{\pi} - d_{\pi}^{\text{T}} \otimes I_m \nabla_{\theta}v_{\pi} \\
&= d_{\pi}^{\text{T}} \otimes I_m u,
\end{aligned}
$$
这意味着
$$ \begin{aligned} \nabla_{\theta}\bar{r}_{\pi} &= d_{\pi}^{\text{T}} \otimes I_m u \\ &= \sum_{s \in \mathcal{S}} d_{\pi}(s)u(s) \\ &= \sum_{s \in \mathcal{S}} d_{\pi}(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta)q_{\pi}(s, a). \end{aligned}
$$

## 9.4 策略梯度算法（REINFORCE）
### 9.4.1 算法陈述
现在，我们准备介绍第一个策略梯度算法来寻找最优策略。

最大化 $J(\theta)$ 的梯度上升算法是
$$
\begin{aligned}
\theta_{t+1} &= \theta_t + \alpha \nabla_{\theta}J(\theta) \\
&= \theta_t + \alpha \mathbb{E}\left[ \nabla_{\theta} \ln \pi(A|S, \theta_t)q_{\pi}(S, A) \right]
\end{aligned}
$$
真实的梯度可以被随机梯度替换：
$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_{\pi}(s_t, a_t)
$$
此外，由于 $q_{\pi}$ 未知，它可以被近似：
$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_{\pi}(s_t, a_t)
$$
有不同的方法来近似 $q_{\pi}(s_t, a_t)$：基于蒙特卡洛的方法，REINFORCE，TD 方法及更多内容。

如何进行采样？
$$
\mathbb{E}_{S \sim d, A \sim \pi}\left[ \nabla_{\theta} \ln \pi(A|S, \theta_t)q_{\pi}(S, A) \right] \longrightarrow \nabla_{\theta} \ln \pi(a|s, \theta_t)q_{\pi}(s, a)
$$
* 如何采样 $S$？
    * $S \sim d$，其中分布 $d$ 是策略 $\pi$ 下的长期行为。
* 如何采样 $A$？
    * $A \sim \pi(A|S, \theta)$。因此，$a_t$ 应该按照 $\pi(\theta_t)$ 在 $s_t$ 处采样。
    * 因此，策略梯度方法是 on-policy 的。

### 9.4.2 算法解释
由于
$$
\nabla_{\theta} \ln \pi(a_t|s_t, \theta_t) = \frac{\nabla_{\theta}\pi(a_t|s_t, \theta_t)}{\pi(a_t|s_t, \theta_t)}
$$
该算法可以改写为
$$
\begin{aligned}
\theta_{t+1} &= \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_{\pi}(s_t, a_t) \\
&= \theta_t + \alpha \underbrace{\left( \frac{q_t(s_t, a_t)}{\pi(a_t|s_t, \theta_t)} \right)}_{\beta_t} \nabla_{\theta}\pi(a_t|s_t, \theta_t).
\end{aligned}
$$
因此，我们得到算法的重要表达式：
$$
\theta_{t+1} = \theta_t + \alpha \beta_t \nabla_{\theta}\pi(a_t|s_t, \theta_t)
$$


这是一个最大化 $\pi(a_t|s_t, \theta)$ 的梯度上升算法：

$$
\theta_{t+1} = \theta_t + \alpha \beta_t \nabla_{\theta}\pi(a_t|s_t, \theta_t)
$$
直觉：当 $\alpha \beta_t$ 足够小时
如果 $\beta_t > 0$，选择 $(s_t, a_t)$ 的概率增加：
$$
\pi(a_t|s_t, \theta_{t+1}) > \pi(a_t|s_t, \theta_t)
$$

$\beta_t$ 越大，增强越强。
如果 $\beta_t < 0$，那么
$$
\pi(a_t|s_t, \theta_{t+1}) < \pi(a_t|s_t, \theta_t).
$$
当 $\theta_{t+1} - \theta_t$ 足够小时，我们有
$$
\begin{aligned}
\pi(a_t|s_t, \theta_{t+1}) &\approx \pi(a_t|s_t, \theta_t) + (\nabla_{\theta}\pi(a_t|s_t, \theta_t))^{\text{T}} (\theta_{t+1} - \theta_t) \\
&= \pi(a_t|s_t, \theta_t) + \alpha \beta_t (\nabla_{\theta}\pi(a_t|s_t, \theta_t))^{\text{T}} (\nabla_{\theta}\pi(a_t|s_t, \theta_t)) \\
&= \pi(a_t|s_t, \theta_t) + \alpha \beta_t \|\nabla_{\theta}\pi(a_t|s_t, \theta_t)\|^2
\end{aligned}
$$

对于
$$
\theta_{t+1} = \theta_t + \alpha \underbrace{\left( \frac{q_t(s_t, a_t)}{\pi(a_t|s_t, \theta_t)} \right)}_{\beta_t} \nabla_{\theta}\pi(a_t|s_t, \theta_t)
$$
系数 $\beta_t$ 可以很好地平衡探索和利用。
* 首先，$\beta_t$ 与 $q_t(s_t, a_t)$ 成正比。
    * 如果 $q_t(s_t, a_t)$ 很大，那么 $\beta_t$ 很大。
    * 因此，该算法旨在增强具有更高价值的行动。
* 其次，$\beta_t$ 与 $\pi(a_t|s_t, \theta_t)$ 成反比。
    * 如果 $\pi(a_t|s_t, \theta_t)$ 很小，那么 $\beta_t$ 很大。
    * 因此，该算法旨在探索那些概率较低的行动。

回顾一下
$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_{\pi}(s_t, a_t)
$$
被替换为

$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_t(s_t, a_t)
$$
其中 $q_t(s_t, a_t)$ 是 $q_{\pi}(s_t, a_t)$ 的近似。
* 如果 $q_{\pi}(s_t, a_t)$ 通过蒙特卡洛估计近似，该算法有一个特定名称，即 **REINFORCE**。
* REINFORCE 是最早和最简单的策略梯度算法之一。
* 许多其他策略梯度算法，例如 actor-critic 方法，可以通过扩展 REINFORCE 获得。

### 9.4.3 算法总结 
**伪代码：蒙特卡洛策略梯度 (REINFORCE)**
初始化：一个参数化函数 $\pi(a|s, \theta)$, $\gamma \in (0, 1)$, 且 $\alpha > 0$。
目标：搜索一个最大化 $J(\theta)$ 的最优策略。
对于第 $k$ 次迭代，执行
	选择 $s_0$ 并按照 $\pi(\theta_k)$ 生成一个片段。假设该片段是 $\{s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T\}$。
	对于 $t = 0, 1, \dots, T-1$，执行
	    值更新： $q_t(s_t, a_t) = \sum_{k=t+1}^T \gamma^{k-t-1} r_k$
	    策略更新： $\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)q_t(s_t, a_t)$
	$\theta_k = \theta_T$

# 10 演员-评论家方法

## 10.1 最简单的演员-评论家：QAC


## 10.2 优势演员-评论家：A2C


## 10.3 Off-policy 的演员-评论家


## 10.4 确定性演员-评论家：DPG