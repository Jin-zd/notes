# Lecture 1: Introduction to Natural Language Processing
自然语言是人类为通用交流而口头说出、书写或用手语表达的语言。

自然语言：多媒体。文本形式：纯文本、RTF 文本、图像文本。音频形式：– 语音。视频形式。混合形式。
自然语言：多生物形式。书面语言 - 印刷 / 书写的文本。口头语言 - 语音信号 / 口语。手语 - 为听力障碍者使用的肢体语言。盲文 - 为盲人使用的语言

自然语言处理（NLP）是对自动处理、理解和生成人类自然语言等问题的研究。
自然语言处理的应用有：机器翻译；信息提取；文本转语音系统；语音识别；信息检索；问答系统；对话系统；情感分析

自然与语言多层次的歧义：在声学层面（语音识别）；中文中的词汇层面；在句法层面；在语义（含义）层面；在语篇（多子句）层面
主要挑战：普遍存在的不确定性，词汇、句法、语义、语用和语音层面。未知语言现象的不可预测性；数据不足；语言知识表达的复杂性。
自然语言处理中的知识瓶颈：语言知识；关于世界的知识。
解决方法：符号方法，统计方法

# Lecture 2: Linguistics Foundations
汉字是一种非字母符号。
汉字是表意文字系统中的语素音节文字。
字形向量表示：使用历史脚本；用于字形的田字格卷积神经网络结构；图像分类作为辅助目标
多种字符编码标准：GB；GBK；GB18030；Big 5；Unicode

基于字符的语言处理：字本位
语素是基本的形态单位，即最小的有意义的元素，不能再进一步分析为更小的部分且仍保留意义  
一种概括观点，即一个音节是一个由一个汉字表示的语素。

汉语构词法 —— 双音节复合词  
偏正型复合词：名词前有描述性修饰语，整体含义可能不同于各部分含义的简单相加。如 小人热心 水手。
偏正型动词复合词，动词前有修饰语，指明动词动作执行的方式 。如 寄生飞驰。
并列型复合词：同义复合词，两个组成语素具有相似或相同的含义，似乎没有对应的英语表达。如 报告 声音。
反义复合词：组成部分具有相反或相对的意义，意义上有明显差异，整体的词性可能不同。如 买卖 左右 大小 开关。
动宾复合词：既可以是动词性的，也可以是名词性的。如 放心 鼓掌 司机。 
动补复合词（英语中没有）：动词性语素后面跟着一个补语，表示动作的方向或结果。如 进来 进去。
主谓复合词：一个主语和一个谓语。如 地震 心疼 民主。 
名量补语复合词：语素不呈现任何常见的语法关系，表示名词所属的一个通用类别。如 人口 羊群 书本。

三音节复合词
层次结构的可能性  
在由 ABC 组成的字符串中，AB 或 BC 之间可能存在更紧密的组合。  
音节数量较多会导致分析出现歧义的可能性。  

四字复合词
修饰词 - 名词：情人节 大学生
动宾式三音节复合词：开玩笑 吹牛皮 
主谓宾：胆结石 鬼画符。这些包含三个语法成分的复合词自然至少要有三个音节才有可能。
描述性 + 名词：乒乓球 棒棒糖。其重叠式或拟声双音节词位于中心名词之前。三音节形式所特有的。

四字复合词  
由同义双音节复合词成对组合而成：骄傲自满 艰难困苦  
同义词交织成对：花言巧语 油腔滑调  
反义词交织成对：大同小异 口是心非  
同义词与反义词交织成对：大惊小怪 生离死别  
方向语素与同义词成对组合：南腔北调 东奔西跑
成对的方向语素与反义词：南来北往  
成对的数字与同义词交织 ：一干二净 四平八稳  
成对的数字与反义词交织：七上八下  
成对的与同义词交织：全心全意 称王称霸  
重叠与反义词交织：自生自灭 不破不立  
重叠与数字交织：不三不四

汉语构词法 - 词缀法：通过在词根上添加词缀可以构成派生词。
汉语构词法 - 重叠：单音节或双音节词根语素可以被复制，不使用于英语
汉语构词法 - 词的离析：一种形态句法现象，词的组成语素相互分离的过程

单词组成短语，进而构成句子。  
短语结构语法告诉我们如何从单词的含义来确定句子的含义
解析（语法分析）：句法是语法的主要组成部分之一；是短语或句子中单词的正确顺序；是用于撰写符合语法规范句子的一种工具；以某种语言为母语的人在不知不觉中就学会了正确的句法。
句法树：
![[Pasted image 20250610212929.png|400]]

句法结构由词汇项组成，这些词汇项通过称为依存关系的二元不对称关系相连接。
![[Pasted image 20250610213020.png|400]]

语篇是一组搭配合理且连贯的句子。  语篇理论研究的是超出单个句子范围的语言现象。语篇分析 / 处理是一系列自然语言处理任务，旨在从多个句子组成的文本中揭示不同层面的语言结构，这可以支持许多 “下游” 自然语言处理应用。

建模连贯结构
语法性：区分结构良好的句子与随机的单词序列。模型被定义为单词之间的分组和关系。
连贯性：在多句子层面起着相同的作用。模型也被定义为句子（或多句单位）之间的分组和关系。

语篇 / 连贯关系：指明句子或从句之间的关系。由于这些关系，相邻的两个句子看起来会连贯。

语料库是一个庞大且有组织的文本集合。
用于进行统计分析和假设检验，检查特定语言范围内的出现情况或验证语言规则。一个语料库可能包含单一语言的文本（单语语料库），也可能包含多种语言的文本数据（多语语料库）。
经过特殊格式化以进行并排比较的多语言语料库被称为对齐平行语料库。

语料库语言学是对语言的研究，这种研究通过 “现实世界” 文本样本（语料库）来体现。  
这种方法代表了一种融会贯通的方式，即推导出一套抽象规则，自然语言正是由这些规则支配，或与其他语言相关联。语料库最初是手动完成的，现在大多通过自动化流程获得。

# Lecture3: Machine Learning Foundations
机器学习是对计算机算法的研究，这些算法通过经验和数据的使用自动改进。
一个预测函数接收输入并产生输出。  
一个机器学习算法：  
输入：训练数据  
从训练数据中 “学习”。  
输出：一个 “预测函数”，在给定输入时产生输出。  

每个机器学习算法都有三个组成部分：  
表示：如何对输入与输出之间的关系进行建模。  
假设空间：所有可能的条件概率分布或决策函数。  
评估：评估候选假设函数的方式。损失函数、经验风险最小化。  
优化：确定假设函数中参数的算法。  

机器学习算法类型：监督学习，无监督学习，强化学习

线性判别函数
给定一个样本 $\mathbf{x} = [x_1, ..., x_d]^{\text{T}}$，令 $f(\mathbf{x}, \mathbf{w})$ 为
$$
\begin{align}
f(\mathbf{x}, \mathbf{w}) &= \mathbf{w}^{\text{T}} \mathbf{x} + b \\
&= w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b
\end{align}
$$
$\mathbf{w} = [w_1, ..., w_d]^{\text{T}}$ 是一个权重向量，$b$ 是一个偏置项。
线性判别函数是一个超平面，构成一个决策边界。

逻辑回归中的收益：
为了确定模型的参数，即权重和偏置
我们希望学习的参数能使估计值对于每个训练样本，偏差尽可能接近真实值  
损失函数：系统输出与标准输出之间的距离  
梯度下降：一种优化算法，用于迭代更新参数，以使损失函数最小化

这是一个关于二分类问题中函数输出的解释。
对于二分类问题， $y \in \{0, 1\}$，但是 $f(\mathbf{x}, \mathbf{w}) \in \mathbb{R}$。
逻辑函数或称为 sigmoid 函数：$\sigma(z) = \frac{1}{1 + \exp(-z)}$

交叉熵损失函数
给定 $N$ 个训练样本 $\{ (\mathbf{x}^{(n)}, y^{(n)}) \}_{n=1}^{N}$
令 $\hat{y} = p(y=1|\mathbf{x}) = \sigma(\mathbf{w}^{\text{T}}\mathbf{x} + b)$
$$
L(\mathbf{w}, b) = -\frac{1}{N} \sum_{n=1}^{N} \left( y^{(n)} \log \hat{y}^{(n)} + (1 - y^{(n)}) \log(1 - \hat{y}^{(n)}) \right).
$$
梯度下降：
偏导数：$\frac{\partial L}{\partial \mathbf{w}} = -\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}^{(n)}(y^{(n)} - \hat{y}^{(n)})$，$\frac{\partial L}{\partial b} = -\frac{1}{N}\sum_{n=1}^{N}(y^{(n)} - \hat{y}^{(n)})$
更新 w, b（沿着负梯度方向更新参数）$\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t} - \alpha\frac{\partial L}{\partial \mathbf{w}}$，$b_{t+1} \leftarrow b_{t} - \alpha\frac{\partial L}{\partial b}$
α 是步长（学习率）。

如果权重学习得过于完美，以至于模型完全匹配训练数据，那些仅仅偶然与类别相关的噪声因素  
也会被建模。这个问题被称为过拟合。
避免过拟合的一种方法是在目标函数中添加一个正则化项，以惩罚较大的权重。
设 $\theta = [w_1, w_2, ..., w_d, b]^\text{T}$
L1 正则化 $R(\theta) = ||\theta||_1 = \sum_{i=1}^{d+1} |\theta_i|$
L2 正则化 $R(\theta) = \frac{1}{2}||\theta||_2^2 = \frac{1}{2}\sum_{i=1}^{d+1} \theta_i^2$

支持向量机：
目标函数$$
\begin{aligned} 
&\max_{\mathbf{w}, b} \frac{1}{2}||\mathbf{w}||^{2} \\ 
&\text{s.t.} \quad y^{(n)} (\mathbf{w}^\text{T} \mathbf{x}^{(n)} + b) \ge 1, \forall n \end{aligned}$$为处理线性不可分问题，有必要通过核函数将样本从原始特征空间映射到更高维空间。
![[Pasted image 20250611110843.png|400]]

结构化学习：分类器预测输入样本属于哪个类别。结构化学习的输出不再是离散的标签，而是结构化对象，例如序列、树或图。

隐马尔可夫链：给定一个模型和一些观测结果，估计不可观测的状态序列。
观察是可见的，状态是不可见的。
两个基本假设
观测独立性假设：每个时刻的 $x_t$ 只依赖于它的标签 $z_t$。
齐次马尔可夫性质：$z_t$ 依赖于前一时刻的 $z_{t-1}$。
$$
\begin{aligned}
p(\mathbf{z}|\mathbf{x}) 
&= \frac{p(\mathbf{x}|\mathbf{z})p(\mathbf{z})}{p(\mathbf{x})} \\
& \propto p(\mathbf{x}|\mathbf{z})p(\mathbf{z}) \\
& = \underbrace{\left[\prod_{t=1}^{T} p(x_t|z_t)\right]}_{\text{观测概率 / 放射概率}} \underbrace{p(z_1)}_{\text{初始状态概率}} \underbrace{\left[\prod_{t=2}^{T} p(z_t|z_{t-1})\right]}_{\text{状态转移概率}}
\end{aligned}
$$
隐马尔可夫链的参数：$\lambda = (A, B, \pi)$
$A$ 是状态转移矩阵，$B$ 是观测概率矩阵，$\pi$ 是初始状态概率的向量。
$A$ 和 $\pi$ 决定如何生成隐藏状态序列。$B$ 决定如何从状态生成观测值。它们共同决定如何生成观测序列。

三个基本 HMM 问题：
评估：给定观测序列 $\mathbf{x} = (x_1, x_2, ..., x_T)$ 和 HMM 模型 $\lambda = (A, B, \pi)$，如何计算观测序列的概率 $P(\mathbf{x}|\lambda)$? 
解码：给定观测序列 $\mathbf{x} = (x_1, x_2, ..., x_T)$ 和 HMM 模型 $\lambda = (A, B, \pi)$，如何找到最能解释观测的状态序列 $P(\mathbf{z}|\mathbf{x}, \lambda)$?
学习：如何调整模型参数 $\lambda = (A, B, \pi)$ 以最大化 $P(\mathbf{x}|\lambda)$? 

评估问题：
直接计算 $P(\mathbf{x}|\lambda) = \sum_{\mathbf{z}} P(\mathbf{x}|\mathbf{z}, \lambda)P(\mathbf{z}|\lambda)$
其中 $P(\mathbf{x}|\mathbf{z}, \lambda) = \prod_{t=1}^{T} b_{z_t}(x_t)$，$P(\mathbf{z}|\lambda) = \pi_{z_1} \prod_{t=2}^{T} a_{z_{t-1} z_t}$
有 $N^T$ 种可能的状态序列，计算复杂度为 $O(TN^T)$。
$N$ 是可能状态的数量，$T$ 是序列的长度。
解决方案是动态规划。

前向算法
1.  初始化：$\alpha_1(i) = \pi_i b_i(x_1)$
2.  前向递归：对 $t = 1, 2, \dots, T-1$, $\alpha_{t+1}(i) = \left[\sum_{j=1}^{N} \alpha_t(j)a_{ji}\right] b_i(x_{t+1})$
3.  终止：$P(\mathbf{x}|\lambda) = \sum_{i=1}^{N} \alpha_T(i)$
复杂度 $O(N^2T)$

反向算法：
1.  初始化：$\beta_T(i) = 1$
2.  后向递归：对 $t = T-1, T-2, \dots, 1$，$\beta_t(i) = \sum_{j=1}^{N} a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)$
3.  终止：$P(\mathbf{x}|\lambda) = \sum_{i=1}^{N} b_i(x_1) \beta_1(i) \pi_i$

解码问题：
维特比算法：定义维特比变量 $\delta_t(i)$ 为生成观测序列 $x_1, x_2, \dots, x_t$ 并在沿任何隐藏状态序列 $z_1, z_2, \dots, z_{t-1}$ 移动并进入 $z_t = s_i$ 时的最大概率：
$\delta_t(i) = \max_{z_1, z_2, \dots, z_{t-1}} P(x_1, x_2, \dots, x_t, z_1, z_2, \dots, z_{t-1}, z_t = s_i|\lambda)$
1. 初始化：$\delta_1(i) = \pi_i b_i(x_1)$，$\Psi_1(i) = 0$
2.  前向递归：对 $t = 1, 2, \dots, T-1$，$\delta_{t+1}(i) = \max_{1 \le j \le N} [\delta_t(j)a_{ji}] b_i(x_{t+1})$，$\Psi_{t+1}(i) = \arg \max_{1 \le j \le N} [\delta_t(j)a_{ji}]$
3.  终止：$P^* = \max_{1 \le i \le N} \delta_T(i)$，$z_T^* = \arg \max_{1 \le i \le N} \delta_T(i)$
4.  回溯 (Backtrack)：对 $t = T-1, \dots, 2, 1$，$z_t^* = \Psi_{t+1}(z_{t+1}^*)$

学习问题：
Baum-Welch 算法：
给定 $\left\{\mathbf{X}^{(n)}\right\}_{n=1}^{N}$，初始化参数
重复：
E 步骤：通过前向-后向算法估计观测 $\mathbf{x}^{(n)}$ 的状态 $\hat{\mathbf{z}}^{(n)}$：
$$\begin{aligned} \alpha_t(i) &= P(x_1, x_2, \dots, x_t, y_t = s_i|\lambda) \\ \beta_{t+1}(j) &= P(x_{t+2}, x_{t+3}, \dots, x_T|z_{t+1} = s_j, \lambda) \\ \xi_t(i,j) &= P(z_t = s_i, z_{t+1} = s_j | \mathbf{x}) \\ &= \frac{p(z_t = s_i, z_{t+1} = s_j, \mathbf{x})}{p(\mathbf{x})} \\ &= \frac{\alpha_t(i) a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_t(i) a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)} \end{aligned}$$
M 步骤：通过 $\mathbf{x}^{(n)}$, $\hat{\mathbf{z}}^{(n)}$ 更新参数：
$$\begin{aligned} a_{ij} &= \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)} \\ b_j(k) &= \frac{\sum_{t=1}^{T} \gamma_t(j) \cdot \mathbf{1}(x_t = o_k)}{\sum_{t=1}^{T} \gamma_t(j)} \\ \pi_i &= \gamma_1(i) \end{aligned}$$

HMM 是一种生成模型，它对观测序列的生成过程进行建模：$$\begin{aligned}
    \hat{\mathbf{z}} &= \arg \max_{\mathbf{z}} p(\mathbf{z}|\mathbf{x}) \\
    &= \arg \max_{\mathbf{z}} p(\mathbf{x}|\mathbf{z})p(\mathbf{z})
    \end{aligned}$$CRF（条件随机场）是一种判别模型，它直接对输入到输出的映射关系进行建模：$$\hat{\mathbf{z}} = \arg \max_{\mathbf{z}} F(\mathbf{z}|\mathbf{x})$$CRF 是一种概率无向图模型，设 CRF 中最大的团集为 C，则条件概率为：
$$p(\mathbf{z}|\mathbf{x}, \theta) = \frac{1}{Z(\mathbf{x}, \theta)}\exp\left(\sum_{c \in C} \theta_c^\text{T} f_c(\mathbf{x}, \mathbf{z}_c)\right)$$
其中，$Z(\mathbf{x}, \theta)$ 是归一化因子，也称为配分函数：$Z(\mathbf{x}, \theta) = \sum_{\mathbf{z}} \exp\left(\sum_{c \in C} \theta_c^\text{T} f_c(\mathbf{x}, \mathbf{z}_c)\right)$

线性链 CRF：
$$p(\mathbf{z}|\mathbf{x}, \theta) = \frac{1}{Z(\mathbf{x}, \theta)}\exp\left(\sum_{t=1}^{T} \theta_1^\text{T} \underbrace{f_1(x_t, y_t)}_{\text{状态特征}} + \sum_{t=2}^{T} \theta_2^\text{T} \underbrace{f_2(y_{t-1}, y_t)}_{\substack{\text{转移特征}}}\right)$$
配分函数的计算通常采用动态规划方法。  
参数一般通过最大条件对数似然函数进行学习。  
解码过程与隐马尔可夫模型类似，通过维特比算法可快速计算。

HMM 和 CRF对比：

|         | 速度  | 判别 vs. 生成 | 归一化 |
| ------- | --- | --------- | --- |
| HMM | 非常快 | 生成式       | 局部  |
| CRF | 比较慢 | 判别式       | 全局  |

# Lecture 4: Deep Learning Foundation
机器学习：预处理->特征提取->特征转换->预测
深度学习：表示：神经网络。评估：交叉熵损失等。优化：随机梯度下降。
人工神经网络（ANN）基于一组有向连接的神经元。
前馈神经网络是一种人工神经网络，其中节点之间的连接不会形成循环。
有单个隐藏层且包含有限数量神经元的前馈网络，在温和假设下，基于激活函数，能够逼近$R^n$紧子集上的连续函数。一个两层神经网络可以模拟任何函数。

RNN（循环神经网络）：
![[Pasted image 20250611111433.png|400]]
可以处理任意长度的输入；步骤计算（理论上）可以使用多步之前的信息；对于更长的输入，模型大小不会增加；在每个时间步应用相同的权重，因此在处理输入时具有对称性。

LSTM（长短期记忆网络）：
LSTM 可以从单元中擦除、写入和读取信息.信息的擦除/写入/读取选择由三个相应的门控制。
计算：
![[Pasted image 20250611111623.png|400]]
网络结构：
![[Pasted image 20250611111738.png|400]]

GRU（门控循环单元）：
![[Pasted image 20250611111826.png|400]]

Bidirectional RNN（双向循环神经网络）
计算：
![[Pasted image 20250611112013.png|400]]
网络结构：
![[Pasted image 20250611112004.png|400]]

CNN（卷积神经网络）：
卷积运算、填充、步长。
TextCNN：
![[Pasted image 20250611112218.png|400]]
Encoder-Decoder（编码器-解码器网络）：
结构：
![[Pasted image 20250611112317.png|400]]
该网络的主要缺点之一是它无法从较长的语义句子中提取强上下文关系，也就是说，如果某段较长的文本在其子字符串内存在某些上下文或关系，那么基本的序列到序列模型无法识别这些上下文，因此在一定程度上会降低模型的性能，最终导致准确率下降。

注意力（K-Q-V）：
给定查询 $q$ 和键 $\mathbf{x}_{1:N}$
$$\begin{aligned}
\alpha_i &= p(z = i|\mathbf{x}_{1:N}, q) \\
&= \text{softmax}\left(s(\mathbf{x}_i, q)\right) \\
&= \frac{\exp\left(s(\mathbf{x}_i, q)\right)}{\sum_{j=1}^{N} \exp\left(s(\mathbf{x}_j, q)\right)}
\end{aligned}$$
 $s(\mathbf{x}_i, q)$ 是评分函数：$s(\mathbf{x}_i, q) = \mathbf{x}_i^\text{T} q$，$s(\mathbf{x}_i, q) = \mathbf{x}_i^\text{T} W q$
给定值 $\mathbf{v}_{1:N}$，$a = \sum_i \alpha_i \mathbf{v}_i$，$a$ 是输出。

RNN中的注意力：全局注意力；局部注意力；双向注意力；自注意力；键值注意力。 
自注意力：
![[Pasted image 20250611112831.png|400]]

# Lecture 5: Language Model
语言建模：给定一系列单词，计算下一个单词的概率分布。
应用：机器翻译，手写识别，上下文敏感的拼写纠正，中文分词。

N-gram：
估计给定历史 $h$ 的单词 $w$ 的概率 $p(w|h)$。
简化假设：
无需根据一个词的完整历史来计算其概率，而是可以仅通过最后几个词来近似其历史。  
马尔可夫假设：一个词的概率仅取决于前一个词。

Bigram 回顾前一个词；Trigram：回顾前两个词；N-gram：回顾前 n - 1 个词。

Bigram
估计二元语法概率：计数并相除
$$P(w_t|w_{t-1}, w_{t-2}, \dots, w_1) \approx P(w_t|w_{t-1}) = \frac{C(w_{t-1} w_t)}{C(w_{t-1})}$$
词语序列的概率：
为了有一个一致的概率模型，在每个句子中附加一个唯一的开始和结束符号，并将它们视为额外的词。$$P(<s>, w_1, w_2, \dots, w_T, </s>) \approx P(w_1|<s>)P(w_2|w_1) \dots P(w_T|w_{T-1})P(</s>|w_T)$$语言模型必须在大量文本语料库上进行训练，以估计出良好的参数值。模型可以根据其对不相交（留出）测试语料库预测高概率的能力来进行评估（在训练语料库上进行测试会给出一个乐观的有偏估计）。

如何处理测试语料库中在训练数据中未出现的词，即不在词汇表中的词？  
训练一个包含未知词明确符号（\<UNK>）的模型：预先选择一个词汇表，并用\<UNK>替换训练语料库中的其他词；用\<UNK>替换训练数据中每个词的首次出现。

生成：
词序列：$w_1^n = w_1 \dots w_n$
概率链式法则：$P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2) \dots P(w_n|w_1^{n-1}) = \prod_{k=1}^{n} P(w_k|w_1^{k-1})$
N-gram 近似：$P(w_1^n) = \prod_{k=1}^{n} P(w_k|w_{k-N+1}^{k-1})$
随着我们增加 N 的值，N 元语法在对训练语料库进行建模方面的表现越来越好。
很多时候，局部上下文并不能提供最有用的预测线索，相反，这些线索是由长距离依赖关系提供的。

为什么仅使用 Bigram 和 Trigram？
马尔可夫近似仍然代价高昂。
为了减少参数数量，我们可以：进行词干提取（使用词干而非单词类型）；将单词分组为语义类；只出现过一次的词 --> 与未出现过的词相同。

远距离二元语法 (Distant Bi-gram)
应用带距离的二元语法来估计 N-gram
$$P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2) \dots P(w_n|w_1^{n-1}) = \prod_{\substack{k=1 \\ d=1,\text{windowsize}}}^{n} P(w_k|w_{k-d}, d)$$
参数空间线性增加；优于二元语法；略优于三元语法；但低于 N-gram。

语言建模评估：
困惑度 (Perplexity)：
衡量模型“拟合”测试数据的好坏程度
对于一个句子 $S = w_1 w_2 \dots w_T$
$$\begin{aligned}
PP(S) &= P(w_1 w_2 \dots w_N)^{-\frac{1}{N}} \\
&= \sqrt[N]{\frac{1}{P(w_1 w_2 \dots w_N)}} \\
&= \sqrt[N]{\prod_{i=1}^{N} \frac{1}{p(w_i|w_1 w_2 \dots w_{i-1})}}
\end{aligned}$$
最小化困惑度等同于最大化概率。

平滑（Smoothing）：
罕见（但并非不可能）的组合在训练中从未出现过，将被赋值为零。
如果在测试过程中出现新的组合，其概率被设为零，整个序列的概率也会变为零。
需要考虑到可能会看到在训练中未出现过的事件。 
降低之前见过的事件的概率，为之前未见过的事件留出一点概率。

拉普拉斯 (加一) 平滑 (Laplace (Add-One) Smoothing)
假设看到的每个词都比实际多一次
仅仅在所有计数上加一
$$P(w_t | w_{t-1} \cdots w_{t-n-1}) = \frac{C(w_t w_{t-1} \cdots w_{t-n-1})}{C(w_t)}$$

$$P(w_t | w_{t-1} \cdots w_{t-n-1}) = \frac{C(w_t w_{t-1} \cdots w_{t-n-1}) + 1}{C(w_t) + V^{n-1}}$$
折扣：折扣计数与原始计数的比率：$c_i^* = (c_i + 1) \frac{N}{N+V}$，$d_c = \frac{c^*}{c}$
类似的，有加 K 平滑法。$$P(w_t | w_{t-1} \cdots w_{t-n-1}) = \frac{C(w_t w_{t-1} \cdots w_{t-n-1}) + k}{C(w_t) + kV^{n-1}}$$
退避：如果有充分证据则使用三元语法，否则使用二元语法，再不然就使用一元语法  
插值：混合使用一元语法、二元语法和三元语法

退避：
仅当高阶模型的数据不可用时（即计数为零），才使用低阶模型。
递归地回退到较弱的模型，直到数据可用。
$$P_{katz}(w_n | w_{n-N+1}^{n-1}) = \begin{cases} P^*(w_n | w_{n-N+1}^{n-1}) & \text{if } C(w_{n-N+1}^{n-1}) > 1 \\ \alpha(w_{n-N+1}^{n-1}) P_{katz}(w_n | w_{n-N+2}^{n-1}) & \text{otherwise} \end{cases}$$
其中 $P^*$ 是一个折算后的概率估计，用于为未见事件保留质量，而 $\alpha$ 是回退权重。

线性插值
简单插值 $\hat{P}(w_t|w_{t-1}w_{t-2}) = \lambda_1 P(w_t|w_{t-1}w_{t-2}) + \lambda_2 P(w_t|w_{t-1}) + \lambda_3 P(w_t)$，$\sum_i \lambda_i = 1$
基于上下文的 Lambda 值：$\hat{P}(w_t|w_{t-1}w_{t-2}) = \lambda_1(w_{t-1}w_{t-2}) P(w_t|w_{t-1}w_{t-2}) + \lambda_2(w_{t-1}w_{t-2}) P(w_t|w_{t-1}) + \lambda_3(w_{t-1}w_{t-2}) P(w_t)$
使用一个预留语料库：训练数据，预留数据，测试数据
选择 $\lambda$ 以最大化预留数据的概率：确定 Ngram 概率（基于训练数据），然后寻找能使预留数据集概率最大的 $\lambda$。
 
使用 0/1 仅强调了出现的单词与未出现的单词之间的差异（在某些短文本分类中可能表现良好），但没有反映出出现的单词之间的差异。  
在频繁使用时，一些常见词会出现在许多文章中，例如 “我，是，因为”。这些高频词对文档主题的贡献不大。相反，一些低频词，如 “蜜蜂，农业，机床”，能更好地反映文档的主题。  
因此，给词频（TF）赋予权重，即逆文档频率（IDF），以获得 TF - IDF 值。一个词对一篇文章的重要性越高，其 TF - IDF 值就越大。
TF-IDF：
词频 (TF)：$TF_{w,D_i} = \frac{\text{count}(w)}{|D_i|}$
逆文档频率(IDF)： $IDF_w = \log \frac{N}{\sum_{i=1}^{N} I(w,D_i)}$
为避免分母为 0，平滑 IDF：$IDF_w = \log \frac{N}{1+\sum_{i=1}^{N} I(w,D_i)}$
TF-IDF 值可以通过将 TF 和 IDF 相乘得到：$TF-IDF_{w,D_i} = TF_{w,D_i} * IDF_w$

NNLM（神经网络语言模型）：
![[Pasted image 20250611121711.png|400]]
词向量出现在两个地方 $C \in \mathbb{R}^{D \times V}$，$U \in \mathbb{R}^{|h| \times V}$，其中 $|h|$ 是隐藏层大小。
不仅有左上下文，还有右上下文

Word2vec：
Skip-gram：从随机词向量开始，遍历整个语料库中的每个词，尝试使用词向量预测周围的词：
$$P(o|c) = \frac{\exp(u_o^\text{T} v_c)}{\sum_{w \in V} \exp(u_w^\text{T} v_c)}$$
更新向量以获得更好的预测。

优化目标：
CBOW：$P(w_t|C_t) = \frac{\exp(v_{C_t} \cdot v_{w_t}')}{\sum_{w' \in V} \exp(v_{C_t} \cdot v_{w'}')}$
Skip-Gram：$P(c|w_t) = \frac{\exp(v_{w_t} \cdot v_c')}{\sum_{w' \in V} \exp(v_{w_t} \cdot v_{w'}')}$
词和上下文使用不同的向量矩阵。
当词汇量大且计算资源有限时，概率（归一化）计算效率低。

负采样 (Negative Sampling)：
以两种方式分类（词，上下文）：1 表示在给定上下文中共现，0 表示不共现；通过“改变词”来构建（词，上下文）负例：$\log \sigma(v_{w_t} \cdot v_{w_{t+j}}') + \sum_{i=1}^{K} \log \sigma(-v_{w_t} \cdot v_{\tilde{w}_i}')$，负采样分布 $\tilde{w}_i \sim P_n(w)$


GPT（Generative Pre-Training）：
计算给定文本序列的最大似然估计 (无监督预训练)：
$$\mathcal{L}^\text{PT}(x) = \sum_i \log P(x_i|x_{i-k} \dots x_{i-1}; \theta)$$
GPT 使用多层 Transformer 作为模型的基本结构
$$
\begin{aligned}
&\mathbf{h}^{[0]} = e_x \mathbf{W}^\text{e} + \mathbf{W}^\text{p} \\
&\mathbf{h}^{[l]} = \text{Transformer-Block}(\mathbf{h}^{[l-1]}), \quad \forall l \in \{1, 2, \dots, L\} \\
& P(x) = \text{Softmax}(\mathbf{h}^{[L]} \mathbf{W}^{e^T})
\end{aligned}
$$
使用 GPT 最后一层的表示来完成相关的预测任务
$$P(y|x_1 \dots x_n) = \text{Softmax}(\mathbf{h}^{[L]} \mathbf{W}^\text{y})$$
$$\mathcal{L}^\text{FT}(C) = \sum_{(x,y)} \log P(y|x_1 \dots x_n)$$
在某些情况下，添加额外的预训练损失可以进一步提高性能
$$\mathcal{L}(C) = \mathcal{L}^\text{FT}(C) + \lambda \mathcal{L}^\text{PT}(C)$$

Transformer解码器：
![[Pasted image 20250611123121.png|400]]


BERT：
掩码输入词：掩码掉输入词的 \( k\% \)，然后预测这些被掩码的词，通常使用 \( k = 15\% \)
掩码过少：训练成本过高；掩码过多：上下文不足

掩码标记在微调时从未见过：在预训练阶段使用的掩码标记（如 `[MASK]`）在微调阶段可能没有出现过，这可能导致模型在实际应用中表现不佳。
解决：预测15%的词：在输入文本中随机选择15%的词进行预测，而不是每次都用 `[MASK]` 替换。
80%的时间，用 `[MASK]` 替换选定的词；10%的时间，用一个随机词替换选定的词；10%的时间，保持选定的词不变。

BERT：掩码语言模型：
输入层：$X = \text{[CLS]} x_1' x_2' \dots x_n' \text{[SEP]}$，$v = \text{InputRepresentation}(X)$
BERT 编码层：$\mathbf{h} = \text{Transformer}(v)$
输出层：$P_i = \text{Softmax}(\mathbf{h}_i^\text{m} \mathbf{W}^{\text{t}^\text{T}} + \mathbf{b}^\text{o})$

BERT：下一句预测：
输入层：$X = \text{[CLS]} x_1^{(1)} x_2^{(1)} \dots x_n^{(1)} \text{[SEP]} x_1^{(2)} x_2^{(2)} \dots x_m^{(2)} \text{[SEP]}$，$v = \text{InputRepresentation}(X)$
BERT 编码层：$\mathbf{h} = \text{Transformer}(v)$
输出层：$P = \text{Softmax}(\mathbf{h}_0 \mathbf{W}^\text{P} + \mathbf{b}^\text{o})$

模型压缩：
剪枝：去除模型中冗余的部分  
蒸馏：使用小模型模仿大模型  
量化：使用低比特表示，例如 float32 -> int8

上下文学习：输入：任务描述 + 示例 + 提示

# Lecture 6: Word Processing
句子 $S$ 是一个中文字符序列 $(C_1, C_2, ..., C_M)$。一个 n-字符的词元（即词），从 $S$ 的第 i 个字符开始，是一个有序的 n-元组形式 $(C_i, C_{i+1}, ..., C_{i+n-1})$。
句子 $W$ 的一个切分将其所有字符切分成各种长度的非重叠词元 $(C_1, C_2, ..., C_i)$, $(C_i, C_i+1, ..., C_{i_2-1})$, $(C_{i_N-1}, C_{i_N+1}, ..., C_M)$。
分别将 $W$ 的元素命名为 $(W_1, W_2, ..., W_N)$。

基于单词的方法，基于词典：
正向最大匹配法（FMM）：贪心搜索算法 ，从句子中的起始点开始，寻找与单词表中匹配的最长字符串。  
逆向最大匹配法（BMM）：贪心搜索算法，与 FMM 类似，但方向相反，更准确。
优化的最大匹配：根据频率优化字典中的词条，快速匹配，无性能提升  
双向最大匹配方法：先应用正向最大匹配（FMM），然后应用逆向最大匹配（BMM），比较两种分词结果以解决任何不一致和歧义问题。
基于字典的方法难以处理组合歧义问题

基于统计：
一种基于 EM 算法的方法。

机器学习方法：
基于变换的学习算法：需要一个预先分词的参考语料库和一个初始分词器，学习算法将初始分词与参考语料库进行比较，识别出纠正分词错误的规则，更新初始分词以输出一组排序后的规则。
基于词格的方法：应用不同的基于机器学习的算法来识别词格中最有可能的路径

基于单个和多个字符的方法。
基于条件随机场的方法。

基于神经网络的方法：序列标注方法；基于注意力的方法；跨域相关方法；通用方法

Benchmark：
标准：北京大学标准；中央研究院标准；宾夕法尼亚大学标准
汉字处理评测基准（SigHan Bakeoff）通过五项指标进行评估 —— 整体分词的召回率、精确率和 F 值，以及未登录词和已登录词的召回率。

未知词检测与识别：
频率 / 相对频率：挑选新候选词的起点，低频新词很难识别  
词内概率：反映汉语的形态属性，使用词内概率在初始分词后组合相邻的单个字符  
构词模式：描述一个字符在一个词内特定位置出现的可能性  
互信息：估计字符 n 元组成分之间的内部关联强度
左 / 右熵以及上下文依赖性：描述当前项（字符序列）对其上下文的依赖强度，当前项对其上下文的依赖强度会随着该项成为中文单词的概率增加而降低。  
独立词概率：单个字符或一串字符的属性，此字符在文本中作为独立词出现的可能性  
非词列表：用于排除不良候选词的功能字符列表，单个字符的介词、副词和连词等。
新词与词典词之间的相似性：如果两个字符在相同的词模式中出现的次数更多，那么它们之间的类比就更可靠。
“重叠多样性”：一个字符 n 元语法成为一个有意义的中文单词的可能性是通过 “重叠多样性” 来衡量的，这是一个标准，它将候选词的优劣与与候选词重叠的字符串的优劣一同考虑。
最大化子串：一种简单却有效的提取特定类型频繁子串的方法，能很好地估计未知词边界。  
一种统一模型：从域外语料库和域内未标注文本中学习。跨域学习功能可以基于域相似性学习域外信息。半监督学习功能可以通过自训练学习域内未标注信息。

基于神经网络的词汇外词向量方法：
少样本学习中的词汇外单词；从字形中学习中文词汇外词嵌入

中文人名识别：
利用上下文信息进行姓名识别；部首是汉字的主要组成部分。

外国名字翻译：
外国名字通常是根据其原语言的发音音译而来；长度不限

中文组织名称识别
组织名称的构成很复杂，唯一明确的线索是组织类型；长度不可预测；包含许多其他类型的名称；组织名称中包含的词汇多种多样；许多组织名称有缩写。
中文的组织机构名称通常由两个主要部分组成：第一部分是专有名称：前缀词；第二部分是组织类型：组织机构名称特征词，第二部分包含指导确定正确边界的主要线索词 - 确定左边界对于组织机构名称识别至关重要。
组织名称构成的特征：字词在组织名称内 / 外出现的概率  →组织字词列表；著名组织名称及其缩写 潜在左边界的特征：地点名称；组织名称；特殊词汇
组织名称构成的约束条件：一些 “停用词”，如 “失败” 和 “其它”，永远不会被选作名称的一部分；基于规则的方法；基于概率的方法；基于角色的标记方法；基于网络的方法

中文地名识别：
地名词典及词典查询-中国地名集；在一篇真实文本中，大约 30% 的地名无法找到。
结合语料库统计数据和上下文规则；地名后缀是一种有用的特征；基于语料库的方法：估计一个字符成为中文地名一部分的可能性，用作起始、中间和结尾字符，捕捉一个字符构成中文地名的能力，为定位地名候选词提供一种好方法。
基于规则的候选确认；基于规则的候选消除。

缩写：
简化缩写；消除缩写；通用缩写。

扩展简化缩写：
使用映射表生成扩展内容：假设简化缩写中的每个字符或单词都必须与完整形式中的某个单词匹配；字符或短单词与完整单词的对比
消歧作为 N 元语法解码：扩展解码旨在从格中搜索长词组合作为最佳路径，以最大化N元语法概率；如有候选集，则从其中选择合适的扩展；使用二元语法分词；主要思想是：对于给定缩写的每个扩展候选词，使用二元语法语言模型将其分割成一系列单词。分割得分最高的候选词将被确定为扩展输出。
使用缩写词典生成扩展形式；将每个未简化的缩写映射到来自缩写：扩展语料库的一组完整形式。
扩展候选词的最终消歧：
比较它们各自的 n 元语法分数；分数较高的那个被选为最终的扩展结果。

标题的自动缩写：
缩写只是标题中所有单词首字母的组合；选择组合标题中某些单词的前缀；使用一些外来词

描述分析：词法分析（Lex）；语法分析（Syn）；语义分析（Sem）
候选生成：指标特征；前缀得分；候选得分

缩写排名：
与现有单词相同或相似的缩写通常是不错的选择，表示缩写的语言模型得分。
# Lecture 7: Word Meaning and Word Sense Disambiguation
在语言学中，词义是一个词的多种含义之一。  
在每个句子中，我们根据句子其余部分的提示，为所观察的词赋予不同的含义。
单词的含义通过字典中的描述字符串来表达。

词义的计算机表达：
由义元描述：义元是意义的最小单位，不可再分
由定义描述
由带注释的同义词集描述：WordNet，CiLin

同音异义词：一组拼写和发音相同但含义不同的词之一  
多义词：具有多个相关含义的单词或短语 
同义词：具有相似或相同含义的不同单词
同义词集：一个或多个同义词的集合
反义词：具有相反或对立意义的不同词汇
上位词：表示处于上级或属于更高等级或类别的语义关系 
下位词：表示处于下级或属于更低等级或类别的语义关系
并列词：如果 X 和 Y 共享一个上位词，那么 Y 是 X 的并列词
整体关系词：一个定义了表示整体的术语与表示整体的一部分或成员的术语之间关系的词。
部分关系词：一个表示较大整体的一部分的词。
转喻：一种修辞手法，用与某个概念密切相关的事物的名称来指代该概念。
从属关系方式：如果活动 Y 是以某种方式进行 X，那么动词 Y 是动词 X 的从属词
蕴含关系：如果做 X 就必然在做 Y，那么动词 Y 被 X 所蕴含  

WordNet 是一个英语词汇数据库。它将英语单词分组为同义词集，提供简短、通用的定义，并记录这些同义词集之间的各种语义关系。  目的：制作一个更便于直观使用的词典和同义词词典的组合，支持自动文本分析和人工智能应用。

根据词义而非词形来组织词汇知识；WordNet 中的描述目标 - 复合词 - 短语动词 - 搭配词 - 成语 - 单词；单词是最小单位，没有词素，没有框架。
词汇知识：保存在词典中；世界知识保存在百科全书中。WordNet 专注于词汇知识，在某些条目中，结合了词汇知识和世界知识。

WordNet 区分名词、动词、形容词和副词，遵循不同的语法规则；每个同义词集都包含一组同义词或搭配；一个词的不同语义存在于不同的同义词集中；同义词集的含义通过简短的定义性注释进一步阐明；同义词集通过多种语义关系与其他同义词集相连。

名词词义关系：
上位词：如果每个 X 都是（某种）Y，那么 Y 就是 X 的上位词。 
下位词：如果每一个 Y 都是一种 X，那么 Y 就是 X 的下位词。
同类词：如果 X 和 Y 共享一个上位词，那么 Y 就是 X 的同类词。  
整体词：如果 X 是 Y 的一部分，那么 Y 就是 X 的整体词。
部分词：如果 Y 是 X 的一部分，那么 Y 就是 X 的部分词。

名词和动词都被组织成层次结构，由上位词定义。  
在最高层级，这些层次结构被组织成基本类型 —— 名词有 25 个原始组，动词有 15 个。 

动词词义关系：
上位词：如果活动 X 是一种 Y，那么动词 Y 就是动词 X 的上位词。
方式词：如果活动 Y 是以某种方式做 X，那么动词 Y 就是动词 X 的方式词。 
蕴涵：如果做 X 就必然在做 Y，那么动词 Y 就被 X 所蕴涵。
并列词：那些共享一个共同上位词的动词。

形容词词义关系：
被描述为 N 维空间  
形容词同义词集：形容词簇  
形容词簇通过反义词关系相连  
与 “近似” 关系类似；直接反义词；间接反义词

WordNet 是基于同义词集（Synset）组织的。  同义词集并非概念的唯一表达方式；同义词集通过语义关系相连，如上位词 - 下位词、并列词、整体词 - 部分词等等。

WordNet中词汇的相似度计算：
基本思想：在 WordNet 中，两个更接近或具有共同层次结构的词（同义词集）更为相似；在 WordNet 层次结构中，路径越短，相似度越高。
使用最近公共祖先节点的信息内容来衡量相似性；最近公共祖先节点越低，相似性越高。

CILIN：同义词词林，为翻译和写作目的开发，以三级层次树形结构对常用汉语词汇进行分类，这种层次结构反映了词汇之间的语义关系。

HowNet：是一个超语言常识知识系统，用于人类语言技术中的语义计算。  揭示其汉英词典中共同标注的概念之间的概念间关系和属性间关系。基于概念间关系和属性间关系构建其知识库的图结构。与树状结构的词汇数据库不同，它揭示了相同词性类别内以及跨词性类别的概念关系，尤其是名词和动词之间的语义角色关系。该表示基于中文和英文单词及表达所表示的概念。HowNet 假设所有概念都可以简化为相关的义原，定义一组封闭的义原，由此可以构成一组开放的概念。
分类体系：
义原按层次结构分类，称为分类体系  
分类体系主要提供概念的上下位关系  

关系是 HowNet 的灵魂。  
HowNet 中的关系分为显性关系和隐性关系。
知网中有 11 种显性关系：(1) 同义关系（），(2) 同类关系，(3) 反义关系，(4) 逆反关系，(5)  
上位关系，(6) 下位关系，(7) 部分 - 整体关系，(8) 值 - 属性关系，(9)  属性 - 主体关系，(10) 同源角色框架关系，以及 (11) 语义角色 - 事件关系。
最后四种类型是跨词性的关系。

HowNet 中词汇相似度计算：
对于 HowNet 中的两个中文词汇 $W_1$ 和 $W_2$，如果 $W_1$ 具有概念 $S_{11}, S_{12}, ..., S_{1n}$，$W_2$ 具有概念 $S_{21}, S_{22}, ..., S_{2m}$，则 $W_1$ 和 $W_2$ 之间的相似度等于两个概念之间的最大相似度：
$$\text{Sim}(W_1, W_2) = \max_{i=1 \dots n, j=1 \dots m} \text{Sim}(S_{1i}, S_{2j})$$
与传统语义词典不同，在知网中每个概念不能映射到一个概念树节点，而是由一系列义原进行描述。
两个义原之间的语义距离定义为：$$\text{Sim}(p_1, p_2) = \frac{\alpha}{d+\alpha}$$ $d$ 是 HowNet 原语层次结构中 $p_1$ 和 $p_2$ 之间路径的长度。
 $\alpha$ 是一个可调参数。


CCD：中文概念词典
一个遵循 WordNet 框架的汉英双语类 WordNet 词典，在结构上与 WordNet 兼容
CCD 中描述的关系比 WordNet 更精细。同义词关系分为等同关系和相似关系；反义词关系分为基本反义词和非基本反义词。引入了三种新的部分 - 整体关系类型。搭配示例从真实语料库中选取，并带有定量描述。

语义资源的应用：词义消歧；词语相似度估计；句子极性判定；本体构建；语义场构建；常识推理。

词义消歧：
基于知识的方法：
基于重叠的方法：需要一个机器可读词典；找出目标词不同义项的特征（义项包）与上下文单词特征（上下文包）之间的重叠部分；这些特征可以是义项定义、例句、上位词等；这些特征也可以被赋予权重；重叠度最大的义项被选为上下文合适的义项。

莱斯克算法：词义包：包含歧义词某个候选词义定义中的单词。上下文包：包含每个上下文单词的每个词义定义中的单词。

沃克基于词库的算法：
步骤 1：针对目标词的每个义项，找到该义项所属的词库类别。  
步骤 2：利用上下文单词为每个义项计算得分。如果某个上下文单词的词库类别与某个义项的词库类别匹配，那么该上下文单词会使该义项的得分加 1。

基于总体概念密度的词义消歧  
根据词的某个义项与上下文的相关性来选择义项；相关性通过概念距离来衡量；这种方法使用结构化的层次语义网络（WordNet）来计算概念距离；概念距离越小，概念密度就越高。  

基于知识的方法：总结
使用选择限制进行词义消歧的缺点：需要详尽的知识库。  
基于重叠方法的缺点：词典定义通常非常简短；词典条目很少考虑不同词义的分布限制；存在匹配稀疏的问题；机读词典中不存在专有名词。因此，这些方法无法捕捉专有名词提供的有力线索。

基于机器学习的方法：
每个搭配一种语义：相邻词汇根据相对距离、顺序和句法关系，为目标词的语义提供有力且一致的线索
每个语篇一种语义：在任何给定文档中，目标词的语义高度一致。对于与主题相关的词汇适用，对于动词不适用

如果两个词具有相似的上下文，那么它们就是相似的。

词项 - 文档矩阵：
每个单元格：词项 t 在文档 d 中的出现次数。
每个文档都是一个计数向量，如果两个文档的向量相似，那么这两个文档就是相似的。
每个词也是一个计数向量，如果单词的向量相似，那么这些单词也相似。

朴素贝叶斯 (Naïve Bayes)
$$\hat{s} = \operatorname{argmax}_{s \in \text{senses}} \text{Pr}(s|V_w)$$
其中 $V_w$ 是一个特征向量，包含：
$w$ 的词性 (POS)；
$w$ 的语义和句法特征；
搭配向量 (围绕它的词集) $\rightarrow$ 通常由下一个词 (+1)、下下一个词 (+2)、-2、-1 及其词性组成；
共现向量(词 $w$ 在周围词袋中出现的次数)；
应用贝叶斯规则和朴素独立性假设：
$$\hat{s} = \operatorname{argmax}_{s \in \text{senses}} \text{Pr}(s) \cdot \prod_{i=1}^{n} \text{Pr}(V_w^i|s)$$
决策列表
基于“每种搭配一种意义”的属性：附近的词为目标词的意义提供了强大而一致的线索。
收集大量模糊词的搭配。
计算所有此类搭配的词义概率分布。
计算对数似然比：
$$\text{Log}\left(\frac{\text{Pr}(\text{Sense-A}|\text{Collocation}_i)}{\text{Pr}(\text{Sense-B}|\text{Collocation}_i)}\right)$$
假设该词只有两种意义。当然，这可以很容易地扩展到 'k' 种意义。
更高的对数似然 = 更多的预测证据
搭配按决策列表排序，预测性最强的搭配排名最高
创建决策列表：
对于每个特征，找到
$$\text{sense}(\text{feature}) = \operatorname{argmax}_{\text{senses}} P(\text{sense}|\text{feature})$$
创建一个规则 $\text{feature} \rightarrow \text{sense}(\text{feature})$，其权重为 $P(\text{sense}(\text{feature})|\text{feature})$。
应用决策列表：采用列表中适用于某个示例的第一条规则

基于支持向量机的方法：
支持向量机（SVM）是一种二分类器，它寻找具有最大间隔的超平面来分隔训练样本。  
针对单词的每个义项构建一个单独的分类器。  
训练阶段：使用已标注的语料库，对于单词的每个义项，训练一个支持向量机。同时考虑单词的词性以及相邻单词的词性 - 局部搭配、共现向量、基于句法关系的特征（例如中心词、中心词的词性、中心词的语态等）。  
测试阶段：给定一个测试句子，使用上述特征构建一个测试样本，并将其作为输入提供给每个二分类器。  
根据每个分类器返回的标签选择正确的义项。  

基于范例的词义消歧（k 近邻算法）：
为每个需要消除歧义的单词构建一个基于范例的分类器。  
步骤 1：从每个包含歧义词且已标注词义的句子中，利用以下内容构建一个训练示例： - 歧义词的词性以及相邻单词的词性。 - 局部搭配 - 共现向量 - 形态特征 - 主谓句法依存关系  
步骤 2：对于给定的包含歧义词的测试句子，以类似方式构建一个测试示例。  
步骤 3：然后将测试示例与所有训练示例进行比较，并选择 k 个最相近的训练示例。  
步骤 4：在这 “k” 个示例中出现频率最高的词义，将被选为正确的词义。

半监督决策列表算法：  
基于亚罗夫斯基（Yarowsky）使用决策列表的监督算法。  
目标：从已标记的训练数据的小子集开始
手动标记一些训练示例；手动为每个类别选择一个特征；使用词典定义中的单词

迭代自举法：
步骤 1：确定多义词出现的所有上下文；对于每个可能的语义，使用种子搭配来识别相对少量的代表该语义的训练示例。
步骤 2：在种子数据上训练决策列表算法；使用训练好的分类器对整个样本集进行分类；通过添加那些被高概率标记为意义 A 或意义 B 的成员来创建新的种子数据；使用新的种子数据重新训练分类器；这些新增内容将贡献可靠指示这两种意义的新搭配。

无监督方法： 
利用罗热同义词词典类别  
基于三个观察结果：不同概念类别的单词往往出现在可识别的不同语境中；不同的词义属于不同的概念类别；基于语境的概念类别判别器可作为这些类别成员的基于语境的判别器。  
识别同义词词典类别总体语境中的显著单词并适当加权。

混合方法：
一种迭代方法：使用来自 WordNet 的语义关系（同义关系和上下位关系）。  
从 WordNet（释义）和少量已标注数据中提取搭配和上下文信息。  
语境中的单义词作为已消歧单词的种子集。  
在每次迭代中，根据新词与已消歧单词的语义距离对新词进行消歧。

使用 HowNet 挖掘特征的期望最大化算法：
步骤 1：特征选择。使用 HowNet 获取一个词的核心字符。对词语进行分组：具有相同字符的词被归为同一类。在 HowNet 同义词集描述中进行搜索，去除描述中没有字符的词。
步骤 2：使用 EM 算法进行词聚类。  
聚类步骤：根据词的语义特征将所有词分组为几个簇。得到与语义相对应的词簇。词义消歧：计算每个词与不同词簇之间的关联。关联越强表明这些词描述该语义的概率越高。选择排名靠前的词作为语义特征词。

克服知识瓶颈：
使用搜索引擎：使用单义词和从同义词集注释中提取的短语构建搜索查询；将这些查询输入到搜索引擎中；从检索到的文档中提取包含搜索查询的句子。  
使用等效伪词：  使用属于歧义词每个义项的单义词； 将这些词在语料库中的出现情况作为歧义词的训练示例。
# Lecture 8: Parsing I

# Lecture 9: Parsing II

# Lecture 10: Text Catergorization
文本有 $N$ 个类别 $C = \{c_1, \dots, c_N\}$，给定的一个实例的文本描述 $x$，希望得到一个判别器 $c(x) \in C$
常见分类：情感（消极/积极/中性）；文档关系（二/多分类、多标签分类）。

文本分类的框架：
![[Pasted image 20250611171650.png|400]]

文档分类：有监督学习，输入一个文件 $d$、一个类别集合 $C$、训练集 $\{ (d_i, c_i) \}$，期望获得一个分类器 $\gamma: d \to c$。对无监督学习，可以使用聚类等。但需要先有一些特征才能聚类，即文本表示，一般地，可以将一个文档表示为 $d$ 的表示 = $\{$特征 $A$, 权值$\}$，$\{$特征 $B$, 权值$\}$...$\}$
预处理：停用词过滤（去掉a/the这种高频但低信息量的词）、提取词干（把各种时态的词当作一个）。进阶方法：高阶统计特征、短语识别技术（语法驱动：如识别名词短语等复合结构；统计驱动：通过其现频率识别常用短语）
权值计算：一种常用的权值是 $\text{tf (term frequency)}$，即词频
$$\text{tf}(t,d) = \frac{\text{词在文档}d\text{中出现的次数}}{\text{文档}d\text{的总词数}}$$
TF-IDF（词频-逆文档频率）更常用
$$TF-IDF(t,d) = TF(t,d) \times IDF(t)$$
$$IDF(t) = \log\frac{\text{总文档数}}{\text{包含词}t\text{的文档数}+1}$$
IDF的修正作用：当 $t$ 在所有文档都出现，那么 TF-IDF 接近零，都出现就说明这个词没什么信息含量。典型应用是TF-IDF值提取关键词和文本向量化（提取特征）。若要喂给神经网络，则需归一化TF-IDF，计算公式为：
$$\text{归一化} TF-IDF(t,d) = TF-IDF(t,d) / \sqrt{\sum_{t \in D} (TF-IDF(t,d))^2}$$

特征降维：去除冗余和无关特征。
局部策略：对每个特定类别选择最优特征子集，如类别内某种指标(如卡方统计量、互信息、信息增益、信息比率、几率)最高的几个特征/词项。

全局策略：如用PCA、LDA、t-SNE(基于KL散度)等寻找最优的特征。

特征生成：无监督特征生成有分布聚类和潜在语义索引(LSI/LSA)；有监督的则有监督聚类、显式语义分析(ESI/ESA)。

LSI：使用TF-IDF构造词-文档矩阵 $A_{m \times n}$，表示 $m$ 个词，$n$ 个文档。
进行奇异值分解，$A = U \Sigma V^\text{T}$，对角矩阵 $\Sigma$ 中会有很小的特征值，把它们去掉，假设保留了 $k$ 个大的特征值。那么有 $A \approx U_{m \times k} \Sigma_{k \times k} V_{k \times n}^\text{T}$，其中 $k$ 是预设主题数，$U,V$ 分别表示词项-主题关联度，文档-主题关联度。计算查询向量 $q$ 和文档的相似度：$(q^\text{T} U \Sigma^{-1}) \cdot d$， $d$ 为文档向量。优点是自动发现潜在语义关系，缓解一词多义(因为一个词可以对应多个主题)，无监督，缺点是计算量大，需手动设置，没有概率解释性。

ESA：通过向量空间模型将概念 $C$ 表示为 Wikipedia 文章TF-IDF向量 $d_i$ 的线性组合：
$$c = \sum_i w_i d_i$$
其中 $w_i$ 为关联权重。其核心思想是利用显式知识库（如Wikipedia）的语义结构进行词义消歧和相似度计算。

特征选择与生成在于：针对特定任务（如中文分词、情感分析等）动态构建最优特征集，需考虑粒度（字/词/标点）、领域模式（如《红楼梦》作者鉴别）及任务特性（如情感词筛选）

文本分类方法：
基于规则：通过预定义关键词组合（如"coffee and machine*"）直接匹配文本来实现分类。

1980年代末的专家系统：因规则集膨胀导致维护耗时、复杂性高且一致性难以保证。

数据驱动的机器学习方法：
Rocchio 算法：计算各类别训练文档 TF-IDF 向量的质心作为原型向量，计算测试文档与原型向量的余弦相似度。4 个特点：（分类边界）非一致性假设、构建类别原型向量、原型向量长度无关性、基于相似度。

训练算法. 
假设有$N$类，初始化$N$个全零的原型向量$p_i$。对训练集中的每个样本$(x, c(x))$， $c(x)$为文档$x$的类别，计算$x$的归一化TF-IDF向量$d$， $p_{c(x)} += d$。

判别算法：输入文档$x$，计算归一化TF-IDF向量$d$，则$c(x) = \arg\max_i \cos(d, p_i)$。

贝叶斯分类：贝叶斯分类器即最大化后验概率，即$$c(d) = \arg \max_c p(c|d) = \arg \max_c p(c)p(d|c)$$
$p(c)$ 用频数估计概率，$p(d|c)$ 可用独立性假设，注意到文档 $d$ 是由一个词 $t_1, \dots, t_n$ 构成，因此 $p(d|c) \approx \prod_i p(t_i|c)$，这就是朴素贝叶斯分类器。

决策树：优势在于可解释性强、可视化直观、预处理需求少且支持缺失值预测等特征工程，但对噪声敏感。

CNN：把文档 $d \in \mathbb{R}^{|D| \times k}$ 当做图片，进行卷积、池化、正则化。（$|D|$ 是词数，$k$ 是特征数）。

最近邻/k-近邻分类：两个点的距离一般用余弦相似度计算。找相似度最高的 $k$ 个点，这 $k$ 个点里面类别最多的那个那个就是分类结果。
kNN算法的核心要素包括：相似度度量函数（如余弦/欧氏距离）、近邻数量k的选取、邻居权重分配策略（加权或均等）以及基于多数表决/阈值规则分类决策机制。
kNN算法作为简单高效的惰性学习器，兼具非参数化、局部估计和非线性分类优势（计算复杂度与数据规模线性相关），但存在在线响应慢、概率输出未标准化及可解释性弱等局限。
加权求和评分通过累加待分类点与k近邻中同类样本的相似度得分
$$\text{分数}(c|x) = \sum_{d_i \in k \text{的近邻}} \text{相似度}(x,d) \times I(d,c)$$
来计算类别置信度。$I(d,c) = 1$ 如果 $d$ 属于类别 $c$，否则为零。

支持向量机：直接使用特征向量分类即可。SVM的核心优势在于其凸优化特性保证全局最优解、擅长高维数据处理且理论完备，但受限于大矩阵计算复杂度（$O(m^3)$）和原生二分类限制（需OVR/OVO扩展多分类）。

预训练模型：BERT通过掩码语言建模和下一句预测进行预训练，微调时通过分层学习率缓解灾难性遗忘，顶层特征对文本分类最有效，领域内持续预训练和小数据任务提升显著。RoBERTa通过动态掩码优化、移除NSP任务、扩大训练数据与轮次，证明单纯增加训练轮次（同数据）或数据规模均能显著提升BERT性能，实现更鲁棒的预训练优化。

 评估指标 (混淆矩阵)
 ![[Pasted image 20250611191255.png|400]]
F 度量 (F measure): 一个结合 P 和 R 的单一数字：
$$F_\beta = \frac{(\beta^2+1)PR}{\beta^2P+R}$$
几乎总是使用平衡的 F1 (即 $\beta=1$)
$$F_1 = \frac{2PR}{P+R}$$
宏平均(Macro-averaging)和微平均(Micro-averaging)：
![[Pasted image 20250611191454.png||400]]


# Lecture 11: Text Clustering and Topic Model
文档聚类方法：
理想的聚类算法：应具备可扩展性、多数据类型兼容性、低参数依赖、抗噪性、输入顺序无关性、约束条件兼容性以及结果可解释性。

核心挑战：需通过内部指标（如簇内紧密度/簇间分离度）或外部基准（如已知标签对比）评估质量，同时优化算法收敛速度、局部最优规避以及重叠簇处理能力。

两种聚类：
层次聚类（相对比分聚类，多子聚类的过程与结构）、剖分聚类。
迭代使用剖分聚类，可得到层次聚类。

HAC算法（层次聚类：自底向上）：
给定一个相似度函数 $f$，初始时所有对象都是单独的一类。计算所有类间相似度，将最近的两个合并成一类，直到剩下下一类。这个合并过程就对应了一棵层次聚类树。$C_1, C_2$ 的类间相似度的三种度量：
Single Link: $\min\{f(x,y): x \in C_1, y \in C_2\}$
Complete Link: $\max\{f(x,y): x \in C_1, y \in C_2\}$
Group Average: $\frac{1}{|C_1||C_2| - I(C_1 \cup C_2)|C_1 \cup C_2| - 1)} \sum_{x,y \in C_1 \cup C_2: x \ne y} f(x,y)$
上述算法每轮聚类时计算 $C_1, C_2$ 的相似度的复杂度都是 $O(n^2)$，若算的是 group average 的余弦相似度，可通过维护类内向量和实现 $O(1)$。

分裂式聚类（层次聚类：自顶向下）：
预先指定目标簇数量 $k$，递归分割数据实现。依赖聚类评估函数（如轮廓系数、SSE等）量化聚类质量。枚举超参数 $k$，根据评估函数，用肘方法确定合适的 $k$。

k-mean (非层次聚类类)：
预先指定目标簇数量 $k$，随机选择 $k$ 个实例作为初始质心 (seeds)。根据距离度量（如欧氏距离）将每个实例分配到最近的质心所属簇，然后重新计算质心；直到迭代次数到达上限或者每个簇不再变化，停止。
这里的最近可以用不同的距离函数度量，如欧氏距离、曼哈顿距离、余弦相似度。
复杂度：
若每个向量是 $m$ 维的，重新分配聚类要算 $O(kn)$ 个距离，故 $O(knm)$。计算质心需 $O(nm)$。若迭代 $t$ 轮，则复杂度 $O(klnm)$。
这个算法的设计初衷是最小化每个点到其所属类的质心的距离之和。
算法缺陷：
初始质心敏感性强；需预先指定K值；小数据集输入顺序影响结果；均值计算易受离群值干扰导致质心偏移

k-means++：
k-means改进版。预先指定目标簇数量 $k$，先随机选一个点作为第一个类的聚类中心；计算每个点到最近的聚类中心的距离 $D$，然后将 $D$ 最大的那个点设为下一个类的聚类中心，直到找到 $K$ 个聚类中心；剩下的点，距离哪个中心近就是哪类

Buckshot算法(结合HAC和K-means)：
随机选 $\sqrt{n}$ 个样本，在这些样本上跑HAC，复杂度 $O(n)$；将HAC上的 $k$ 个聚类结果作为k-means的初始质心。
优点：复杂度只有 $O(n)$；减轻k-means对初始种子敏感的问题。

密度聚类 (DBSCAN)：
定义一些概念，核心点 (在$\epsilon$邻域内至少包含$N$个点的点)、p直接可达q ($p$是核心点且$q$在$p$的$\epsilon$邻域内)、q可达p (存在路径$q \to p_1 \to \dots \to p$，其中任何相邻的两个都是直接可达) 、若一个点无法到达任何一个核心点，则称噪声/不可达。
算法：
从未分类的核心点中选取种子，通过密度可达性递归扩展形成簇；重复选择新种子生成新簇，直至无未分类核心点剩余；非核心点若不在任何核心点的$\epsilon$邻域内则判定为噪声
复杂度：若有索引结构，则为$O(N \log N)$；暴力枚举为$O(N^2)$。
优势：无需预设聚类数量；自带噪声识别；对异常值鲁棒
缺点：对数据密度敏感；大数据集收敛慢；参数调优困难

软聚类：每个样本以一个概率属于某个类。

EM算法：期望最大化算法。
E步：计算期望。这等价于计算后验分布 $p(c_i|x)$
M步：期望最大化。更新模型参数 $\theta$。

EM算法无监督聚类：初始化一个分类器，将数据用分类器分类，用分完类的数据训练这个分类器 重复上述步骤直到分布收敛。
EM算法半监督聚类：和无监督的唯一差别是，那些已标注的样本不会被重新打标签。

LDA (Latent Dirichlet Allocation)

特点：无监督主题模型、三层贝叶斯概率结构（文档-主题-词）

生成式建模假设：文档由多个主题混合，主题是词的概率分布。文档生成先采样主题，再根据主题采样词。设有 $D$ 个文档，词典大小 $V$，每个文档至多 $N$ 个词，假设有 $K$ 个主题，那么每个文档的主题是一个 $K$ 维多项分布，其先验均为相同的Dirichlet分布；每个主题的词是 $V$ 维多项分布，其先验也为Dirichlet分布。

推断目标：根据词频，推断所属集合、主题集合、文档中主题比例。
参数估计：用gibbs采样或变分推断，估计出上面这些分布的参数。
# Lecture 12: Information Extraction
信息抽取
目标：把无结构信息变成有结构。
历史：1998年MUC会议提出模板关系（组织类）；2000年Snowball实现半监督模式生成；2003年开启基于句法树的核方法；2004年转向特征工程（MaxEnt/SVM）；2005年处理高阶关系图；2009年远程监督降低标注成本；2014年神经网络（CNN+词向量）突破；2016年Bi-LSTM联合建模NER与RE；2019年R-BERT刷新SOTA；2021年ERICA通过对比学习深化实体关系理解。

重要子任务：Named Entity Recognition (NER)
即找到名字并正确分类(可能是人名、公司、国名)。NER本质是序列标注任务。标注用的标签有不同的方案：
BIO：B（实体起始词）、I（实体内部词）、O（非实体词）。局限是无法直接处理相邻同类型实体的边界歧义（如“北京北京饭店”中的两个“北京”）。
IO：I（实体词）、O（非实体词）。淘汰。
BMES/BIES：B（起始）、M（中间）、E（结尾）、S（单字实体）、O（非实体）。代价是要更多数据。

NER的实现方法：
基于规则：依赖人工构建词典和语法模式，在专业领域可实现高准确率但召回率受限，典型系统如FASTUS等通过穷尽式规则匹配实现精准抽取，面临词典覆盖不全的固有缺陷。

基于特征：人工设计特征提取器，利用标注数据训练序列分类器（如MEMM/CRF），在测试时对文本进行逐词标注以识别实体。

序列标注特征：词特征（当前词及上下文词）、语言学特征（POS标签、短语块、词形简写）和标签上下文（前后预测标签依赖关系）

序列标注：全局上看是seq2seq的任务，局部上是进行特征提取后喂给分类器，通过优化器和正则化，输出一个标签。保存所有序列会组合爆炸，下面是三种寻找较优序列的方式：
贪婪推理：从左到右的局部最优标注策略。快，但因错误累积导致全局次优
束推理（Beam Inference）：保留最好的k个候选策略，平衡效率和效果
Viterbi推理：在Markov假设下全局最优，用动态规划，复杂度高。

典型的通用的命名实体识别类型：

| Type                 | Tag   | Sample Categories                           | Example sentences                                          |
|----------------------|-------|---------------------------------------------|------------------------------------------------------------|
| People               | PER   | people, characters                          | Turing is a giant of computer science.                 |
| Organization         | ORG   | companies, sports teams, government apartment | The IPCC warned about the cyclone.                     |
| Location             | LOC   | regions, mountains, seas                    | The Mt. Sanitas loop is in Sunshine Canyon.        |
| Geo-Political Entity | GPE   | countries, states, provinces                | Palo alto is raising the fees for parking.             |
| Facility             | FAC   | bridges, buildings, airports                | Consider the Golden Gate Bridge.                       |
| Vehicles             | VEH   | planes, trains, automobiles                 | It was a classic Ford Falcon.                          |

命名实体识别 (NER) 作为序列标注
指标 (精确匹配评估)
只有当命名实体的边界和类型都与真实标注完全匹配时，才被认为识别正确。
精确率 (Precision)、召回率 (Recall) 和 F1 分数 (F-score) 基于真正常例 (TP)、假正例 (FP) 和假反例 (FN) 的数量计算。
真正常例 (TP)：被命名实体识别系统识别且与真实标注匹配的实体。
假正例 (FP)：被命名实体识别系统识别但与真实标注不匹配的实体。
假反例 (FN)：真实标注中存在但未被命名实体识别系统识别的实体。
$$\text{Precision} = \frac{TP}{TP+FP} \quad \text{Recall} = \frac{TP}{TP+FN} \quad F\text{-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$


# Lecture 13: Nature Language Generation
自然语言生成（NLG）：指我们生成（即撰写）新文本的任何场景。

一旦训练好了（条件）语言模型，如何使用它来生成文本呢？  
解码算法是一种用于从语言模型生成文本的算法。  
两种广泛使用的解码算法：贪婪解码；束搜索  

贪婪解码：
在每一步，选取最有可能的单词（即求最大值的自变量）  
将其用作下一个单词，并在下一步将其作为输入  
持续进行，直到生成结果（或达到某个最大长度）

束搜索：
一种搜索算法，旨在通过同时追踪多个可能的序列来找到一个高概率序列（不过不一定是最优序列）
核心思想：在解码器的每一步，追踪 k 个最有可能的部分序列（我们称之为假设），k 是束宽
达到某个停止准则后，选择概率最高的序列（考虑对长度进行一些调整）
![[Pasted image 20250611163811.png|400]]
k 值较小时与贪心解码（k=1）存在类似的问题：不符合语法规则、不自然、无意义、不正确。  
k 值越大意味着考虑更多的假设：增大 k 值可以减少上述一些问题；k 值越大，计算成本越高；但是增大 k 值可能会引入其他问题：；对于神经机器翻译（NMT），k 值过大会降低 BLEU 分数，主要是因为大 k 值的束搜索会产生过短的译文（即使进行了分数归一化）；在诸如闲聊对话这类开放式任务中，较大的 k 值会使输出更泛泛。

纯采样 (Pure sampling)：
在每一步 $t$，从概率分布 $P_t$ 中随机采样以获得下一个词。
类似于贪婪解码，但使用采样而不是 argmax。

Top-n 采样：
在每一步 $t$，从 $P_t$ 中随机采样，仅限于前 n 个最可能的词。
类似于纯采样，但截断了概率分布。
n=1 是贪婪搜索，n=V 是纯采样。
增加 n 以获得更多样化/有风险的输出。
减少 n 以获得更通用/安全的输出。


Softmax 温度
回顾：在时间步 t，语言模型通过将 softmax 函数应用于得分向量 $s \in \mathbb{R}^{|V|}$ 来计算概率分布 $P_t$
$$P_t(w) = \frac{\exp(s_w)}{\sum_{w' \in V} \exp(s_{w'})}$$
可以将温度超参数 $\tau$ 应用于 softmax：
$$P_t(w) = \frac{\exp(s_w/\tau)}{\sum_{w' \in V} \exp(s_{w'}'/\tau)}$$
提高温度 $\tau$：$P_t$ 变得更均匀，因此输出更多样化（概率分散在整个词汇表中）
降低温度 $\tau$：$P_t$ 变得更尖锐，因此输出多样性更低（概率集中在顶部词汇上）
softmax 温度不是解码算法，它是一种在测试时可以与解码算法结合使用的技术。

ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (摘要评估的召回导向替补)
$$
\text{ROUGE-N} = \frac{\sum_{S \in \{\text{ReferenceSummaries}\}} \sum_{\text{gram}_n \in S} \text{Count}_{\text{match}}(\text{gram}_n)}{\sum_{S \in \{\text{ReferenceSummaries}\}} \sum_{\text{gram}_n \in S} \text{Count}(\text{gram}_n)}
$$
与 BLEU 类似，它基于 n-gram 重叠。区别：
ROUGE 没有简洁性惩罚。
ROUGE 基于召回率，而 BLEU 基于精确率。
有争议的是，对于机器翻译 (MT)，精确率可能更重要（然后添加简洁性惩罚以修复欠翻译），而对于摘要，召回率更重要（假设您有最大长度限制）。
然而，通常会报告 ROUGE 的 F1（精确率和召回率的组合）版本。

摘要生成：
抽取式摘要：选择原始文本的部分内容（通常是句子）来形成摘要。  
抽象式摘要：使用自然语言生成技术生成新的文本。
传统方法：基于词频统计、文档结构特征或图算法（如句子相似度聚类，选取图中中心度最高的句子）筛选关键词句，再排序重组，虽无法生成新内容但效率稳定。
基于NLP/IE的修辞结构分析法：构建文本的修辞关系树（核心-卫星结构）实现摘要生成：先识别核心论点与支持性内容间的修辞关系，通过聚类或树形指标选择最优结构树后，提取靠近根节点的K个核心节点组成摘要；
评估方面，ROUGE指标则侧重n-gram召回率（分ROUGE-1/2/L单独报告），与BLEU的精确率导向不同，通常用F1衡量摘要质量。
神经网络：引入复制机制，即以一概率复制原文，一概率生成新的token。为解决复制太多和总体把控不好的问题，引入两阶段生成式：标注源文本中需保留的词；在seq2seq+注意力阶段对无需保留施加掩码，限制注意力范围。

评论生成：通过注意力机制，输入（用户、产品、打分），输出产品评论。

对话生成：Seq2Seq对话系统存在通用回复、上下文脱节、重复输出等问题。
改进方法包括：用最大化互信息MMI($\ln\frac{p(\text{输入,回复})}{p(\text{输入})p(\text{回复})}$)增强相关性，检索-改写策略提升多样性，覆盖抑制机制重复，融入用户画像保持人格一致性。核心是通过潜变量分离语言生成与对话策略。

故事生成：通过图片或文本提示驱动模型（如RNN/CNN结合注意力机制）自动创作故事，目前内容逻辑性不足、长期连贯性差，需通过事件结构化建模（使用叙事模板等）和多模态对齐优化叙事质量。

诗歌生成：用有限状态自动机作为押韵的检测器。输入主题，然后获取主题词、筛选押韵的词，通过检测器，输出诗句。通过上调主题词的权值来实现自定义诗歌。

NLG 将结构化/非语言数据转换为自然语言：
评估核心问题：词重叠指标 (BLEU/ROUGE 等)，对开放任务 (如对话、故事生成) 效果差，因过度依赖表面匹配；缺乏全局自动指标，需结合任务特性设计；需要分项评估流畅性、多样性等，无法替代人工。
# Lecture 14: Question Answering
问答系统的范式：
基于信息检索 (IR) (如搜索引擎)：分3步：问题处理：检测问题与答案类型、关注点、关系等，将询问词给搜索引擎；文章检索：按匹配度给相关文章排序，将相关文章拆成更小的部分；答案处理：从中抽取候选答案并排序
基于知识 (Siri)：建立询问词的语义表示(时间日期、位置、实体等)；从该语义表示映射到对应的结构化数据上(如地理空间数据、科学数据库)
基于混合方法(IBM Waston)：通过结合浅层语义解析、信息检索生成候选答案，并利用本体论/半结构化数据增强，最后基于地理空间数据库、时序推理和分类学等知识源对候选答案进行评分

QA的评价指标：准确率；平均倒数排名(MRR)。1/Rank，Rank为正确答案在候选答案中的排序，然后取这些排名倒数的均值。

问题处理：通过答案类型检测、查询关键词生成、问题分类、焦点识别和关系抽取解析提问。

QA 的流程：
答案类型检测：通过分析问题词、词性标注、句法特征、命名实体和语义关联词等特征来确定问题答案类型的技术

标注框架：答案类型分类体系采用6大粗粒度类别（如地点、数字、实体等）和50个细粒度子类（如城市、货币、动物等）的层次化结构
方法：手工规则：如正则表达式，Who {is|was|are|were} PERSON 或基于问题首词的规则 What is the city flower of Shenzhen?；机器学习：定义问题类型分类体系、标注训练数据、基于丰富特征训练分类器

关键词提取：逐步筛选 (1) 非停用词 (2) 专有名词 (3) 复杂名词及其前缀 (4) 其他复杂名词 (5) 名词及其前缀 (6) 其他名词 (7) 动词 (8) 副词 (9) QFW词 (10) 其他
例 Who coined (7) the term (4) "cyberspace" (1) in his novel (4) "Neuromancer" (1) ?

文章检索：文章检索分文档、段落和排步三步。文章和询问都用稀疏的高维向量表示，计算询问向量和文章向量的余弦相似度，从而找到相关性高的文章。这些向量可通过命名实体匹配度、查询词覆盖率、N-gram重合度、关键词邻近度、最长连续匹配及文档权重等特征构成。

答案处理：假设我们已经找到到一段文字，答案就在里面。若答案类型是CITY，那么可以对这段文字进行标注，则标注为CITY的字段就是答案。但问题是可能这段文字存在多个候选答案，即多个城市，因此需要对其排序，选择最有可能的那个（指标：答案类型）

基于信息检索的事实型问答结构：
![[Pasted image 20250611170629.png|400]]
使用知识过滤答案：先抽取关系，然后去对应的关系型数据库找答案。
Whose granddaughter starred in E.T.? 
可拆成两个关系(acted-in ?x "E.T.")  (granddaughter-of ?x ?y)
一些已有的工作：
IBM Watson的候选答案评分系统通过50+多维特征（包括文本结构匹配、来源可信度、时空关系等）实现智能加权评估。
A passage retrieval system for question answering 通过重构段落排序并基于问题的n-gram结构匹配优化问答效果
A QA System based on Information Retrieval and Validation 结合高性能信息检索引擎与答案验证步骤双重过滤错误答案
Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval 采用段落内负采样策略增强同一段落中句子表征的多样性生成

沃森架构：
![[Pasted image 20250611170655.png|400]]


# Lecture 15: Large Language Model I

# Lecture 16: Large Language Model II